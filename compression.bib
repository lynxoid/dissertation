@inproceedings{PatroLCPRC,
author={Sichen,Zhong and Zhao, Lu and Liang, Yan and Zamani, Mohammadzaman and Patro, Rob and Chowdhury, Rezaul and Arkin, Esther M. and Mitchell, Joseph S. B. and Skiena, Steven},
title={Optimizing Read Reversal for Sequence Compression (Extended Abstract)},
year=2015,
pages={14},
booktitle={WABI'15}
}
@article{GATK,
title={{A framework for variation discovery and genotyping using next-generation DNA sequencing data}},
author={DePristo, M and Banks, E and Poplin, R and Garimella, K and Maguire, J and Hartl, C and Philippakis, A and del Angel, G and Rivas, MA and Hanna, M and McKenna, A and Fennell, T and Kernytsky, A and Sivachenko, A and Cibulskis, K and Gabriel, S and Altshuler, D and Daly, M},
year=2011,
journal={{Nature Genetics}},
volume=43,
pages={491-498}
}
@article{Assembly,
title={Assisted assembly: how to improve a de novo genome assembly by using related species},
author={Gnerre, Sante and Lander,  Eric S and Lindblad-Toh, Kerstin and Jaffe, David B},
year=2009,
journal={{Genome Research}},
volume=10,
pages={R88}
}
@article{JaninQuals,
author = {Janin, Lilian and Rosone, Giovanna and Cox, Anthony J.},
title = {Adaptive reference-free compression of sequence quality scores},
volume = {30},
number = {1},
pages = {24-30},
year = {2014},
journal = {Bioinformatics}
}
@inproceedings{KmerQuals,
author={Yu, YW and Yorukoglu, D and Berger, B},
year={2014},
title={{Traversing the k-mer landscape of NGS read datasets for quality score sparsification}},
booktitle={RECOMB'14},
pages={385--399}
}
@article{PathEncode,
author = {Kingsford, Carl and Patro, Rob},
title = {Reference-based compression of short-read sequences using path encoding},
year = {2015},
volume={31},
number={12},
pages={1920--8},
journal = {Bioinformatics}
}
@article{GSQZ,
author = {Tembe, Waibhav and Lowey, James and Suh, Edward},
title = {{G-SQZ: compact encoding of genomic sequence and quality data}},
volume = {26},
number = {17},
pages = {2192-2194},
year = {2010},
journal = {Bioinformatics}
}
@article{SlimGene,
author={Kozanitis, C and Saunders, C and Kruglyak, S and Bafna, V and Varghese, G},
title={{Compressing genomic sequence fragments using SlimGene}},
journal={{Journal of Computational Biology}},
year={2011},
volume={18},
number={3},
pages={401-413}
}
@article{Goby,
author = {Campagne, Fabien AND Dorff, Kevin C. AND Chambwe, Nyasha AND Robinson, James T. AND Mesirov, Jill P.},
journal = {PLoS ONE},
publisher = {Public Library of Science},
title = {Compression of Structured High-Throughput Sequencing Data},
year = {2013},
volume = {8},
pages = {e79871},
number = {11},
}
@article{IGV,
author={Robinson, James T and Thorvaldsd\'{o}ttir, Helga and Winckler, Wendy and Guttman, Mitchell and Lander,	Eric S and Getz, Gad and Mesirov, Jill P},
title={Integrative genomics viewer},
journal={{Nature Biotechnology}},
year={2011},
volume=29,
pages={24-26}
}
@article{D2Dist,
title={New Powerful Statistics for Alignment-free Sequence Comparison Under a Pattern Transfer Model},
author={Liu, Xuemei and Wan, Lin and Li, Jing and Reinert, Gesine and Waterman, Michael S. and Sun, Fengzhu},
year={2011},
volume={284},
number={1},
pages={106-116},
journal={{Journal of Theoretical Biology}}
}
@article{SeqSqueeze,
author = {Bonfield, James K. and Mahoney, Matthew V.},
journal = {{PLoS ONE}},
publisher = {Public Library of Science},
title = {{Compression of FASTQ and SAM format sequencing data}},
year = {2013},
volume = {8},
pages = {e59190},
number = {3}
}
@article{Rozov2014,
title={{Fast lossless compression via cascading Bloom filters}},
author={Rozov, Roye and Shamir, Ron and Halperin, Eran},
journal={BMC Bioinformatics},
year={2014},
pages={S7},
number={Suppl 9},
volume={15}
}
@article{CRAM,
author = {Fritz, Markus Hsi-Yang and Leinonen, Rasko and  Cochrane, Guy and  Birney, Ewan},
title = {{Efficient storage of high throughput DNA sequencing data using reference-based compression}},
journal={Genome Research},
volume={21},
pages={734-740},
year=2011
}
@article{Subread,
author = {Liao, Yang and Smyth, Gordon K. and Shi, Wei},
title = {The Subread aligner: fast, accurate and scalable read mapping by seed-and-vote},
volume = {41},
number = {10},
pages = {e108},
year = {2013},
journal = {Nucleic Acids Research}
}
@article{DobinSTAR,
author = {Dobin, Alexander and Davis, Carrie A. and Schlesinger, Felix and Drenkow, Jorg and Zaleski, Chris and Jha, Sonali and Batut, Philippe and Chaisson, Mark and Gingeras, Thomas R.},
title = {{STAR: ultrafast universal RNA-seq aligner}},
volume = {29},
number = {1},
pages = {15-21},
year = {2013},
journal = {Bioinformatics}
}
@article{RSEM,
author = {Li, Bo and Dewey, Colin N},
title={{RSEM: accurate transcript quantification from RNA-Seq data with or without a reference genome}},
journal={BMC Bioinformatics},
year=2011,
volume=12,
pages={323}
}
@article{Cufflinks,
author = {Trapnell, Cole and Williams,Brian A and Pertea, Geo and Mortazavi, Ali and Kwan, Gordon and van Baren, Marijke J and Salzberg, Steven L and Wold, Barbara J and Pachter, Lior},
title = {{Transcript assembly and quantification by RNA-Seq reveals unannotated transcripts and isoform switching during cell differentiation}},
volume = {28},
pages = {511-515},
year = {2010},
journal = {Nature Biotechnology}
}
@article{Sailfish,
title = {Sailfish enables alignment-free isoform quantification from RNA-seq reads using lightweight algorithms},
author = {Patro, Rob and Mount, Stephen M and Kingsford, Carl},
journal = {Nature Biotechnology},
year = 2014,
volume=32,
pages={462-464}
}
@article{Mince,
	author = {Patro, Rob and Kingsford, Carl},
	title = {Data-dependent Bucketing Improves Reference-free Compression of Sequencing Reads},
	journal = {Bioinformatics},
	year=2015
}
@article{SamTools,
author = {Li, Heng and Handsaker, Bob and Wysoker, Alec and Fennell, Tim and Ruan, Jue and Homer, Nils and Marth, Gabor and Abecasis, Goncalo and Durbin, Richard},
title = {{The Sequence Alignment/Map format and SAMtools}},
volume = {25},
number = {16},
pages = {2078-2079},
year = {2009},
journal = {Bioinformatics}
}
@article{Jones2012,
author = {Jones, Daniel C and Ruzzo, Walter L and Peng, Xinxia and Katze, Michael G.},
journal = {Bioinformatics},
volume=40,
number=22,
year=2012,
title={Compression of next-generation sequencing reads aided by highly efficient de novo assembly}
}
@article{Sahinalp2015,
author = {Hach, Faraz and Numanagic, Ibrahim and Sahinalp, S Cenk},
journal = {Nature Methods},
volume=11,
pages={1082-1084},
title = {{DeeZ: reference-based compression by local assembly}},
year = {2014}
}
@article{Sahinalp2012,
author = {Sahinalp, S Cenk and Hach, Faraz and Namanagik, Ibrahim and Alkan, Can},
journal = {Bioinformatics},
volume={28},
number={23},
pages={3051-3057},
title = {{SCALCE: boosting sequence compression algorithms using locally consistent encoding}},
year = {2012}
}
@article{Byarugaba2013,
abstract = {Influenza B viruses can cause morbidity and mortality in humans but due to the lack of an animal reservoir are not associated with pandemics. Because of this, there is relatively limited genetic sequences available for influenza B viruses, especially from developing countries. Complete genome analysis of one influenza B virus and several gene segments of other influenza B viruses isolated from Uganda from May 2009 through December 2010 was therefore undertaken in this study.},
author = {Byarugaba, Denis K and Erima, Bernard and Millard, Monica and Kibuuka, Hannah and L, Lukwago and Bwogi, Josephine and Mimbe, Derrick and Mworozi, Edison A and Sharp, Bridget and Krauss, Scott and Webby, Richard J and Webster, Robert G and Martin, Samuel K and Wabwire-Mangen, Fred and Ducatez, Mariette F},
issn = {1743-422X},
journal = {Virology journal},
keywords = {InfluenzaMotivation},
mendeley-tags = {InfluenzaMotivation},
pages = {11},
title = {{Genetic analysis of influenza B viruses isolated in Uganda during the 2009-2010 seasons.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3547786\&tool=pmcentrez\&rendertype=abstract},
volume = {10},
year = {2013}
}
@article{Deorowicz2013,
author = {Deorowicz, S. and Danek, A. and Grabowski, S.},
journal = {Bioinformatics},
volume=29,
number=20,
pages = {2572-2578},
title = {{Genome compression: A novel approach for large collections}},
year = {2013}
}
@article{Ferrada2013,
abstract = {Advances in DNA sequencing mean databases of thousands of human genomes will soon be commonplace. In this paper we introduce a simple technique for reducing the size of conventional indexes on such highly repetitive texts. Given upper bounds on pattern lengths and edit distances, we preprocess the text with LZ77 to obtain a filtered text, for which we store a conventional index. Later, given a query, we find all matches in the filtered text, then use their positions and the structure of the LZ77 parse to find all matches in the original text. Our experiments show this also significantly reduces query times.},
author = {Ferrada, H. and Gagie, T. and Hirvola, T. and Puglisi, S. J.},
title = {{Hybrid Indexes for Repetitive Datasets}},
url = {http://arxiv.org/abs/1306.4037},
year = {2013}
}
@article{Nordstrom2013,
author = {Nordstr\"{o}m, Karl J V and Albani, Maria C and James, Geo Velikkakam and Gutjahr, Caroline and Hartwig, Benjamin and Turck, Franziska and Paszkowski, Uta and Coupland, George and Schneeberger, Korbinian},
issn = {1087-0156},
journal = {Nature Biotechnology},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
shorttitle = {Nat Biotech},
title = {{Mutation identification by direct comparison of whole-genome sequencing data from mutant and wild-type individuals using k-mers}},
url = {http://dx.doi.org/10.1038/nbt.2515},
volume = {advance on},
year = {2013}
}
@article{Ochoa2013,
author = {Ochoa, Idoia and Asnani, Himanshu and Bharadia, Dinesh and Chowdhury, Mainak and Weissman, Tsachy and Yona, Golan},
issn = {1471-2105},
journal = {BMC Bioinformatics},
number = {1},
pages = {187},
title = {{QualComp: a new lossy compressor for quality scores based on rate distortion theory}},
url = {http://www.biomedcentral.com/1471-2105/14/187},
volume = {14},
year = {2013}
}
@article{Ruths2013,
abstract = {BACKGROUND:Forward-time population genetic simulations play a central role in deriving and testing evolutionary hypotheses. Such simulations may be data-intensive, depending on the settings to the various parameters controlling them. In particular, for certain settings, the data footprint may quickly exceed the memory of a single compute node.RESULTS:We develop a novel and general method for addressing the memory issue inherent in forward-time simulations by compressing and decompressing, in real-time, active and ancestral genotypes, while carefully accounting for the time overhead. We propose a general graph data structure for compressing the genotype space explored during a simulation run, along with efficient algorithms for constructing and updating compressed genotypes which support both mutation and recombination. We tested the performance of our method in very large-scale simulations. Results show that our method not only scales well, but that it also overcomes memory issues that would cripple existing tools.CONCLUSIONS:As evolutionary analyses are being increasingly performed on genomes, pathways, and networks, particularly in the era of systems biology, scaling population genetic simulators to handle large-scale simulations is crucial. We believe our method offers a significant step in that direction. Further, the techniques we provide are generic and can be integrated with existing population genetic simulators to boost their performance in terms of memory usage.},
author = {Ruths, Troy and Nakhleh, Luay},
issn = {1471-2105},
journal = {BMC Bioinformatics},
number = {1},
pages = {192},
title = {{Boosting forward-time population genetic simulators through genotype compression}},
url = {http://www.biomedcentral.com/1471-2105/14/192},
volume = {14},
year = {2013}
}
@article{Baker2012,
annote = {a Nature review on the state of affairs in genome assembly. has a couple of nice examples where traditional gen. assem. failed (missing conserved genes in chicken, human, sea squirt)},
author = {Baker, Monya},
issn = {1548-7091},
journal = {Nature Methods},
number = {4},
pages = {333--337},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
shorttitle = {Nat Meth},
title = {{De novo genome assembly: what every biologist should know}},
url = {http://dx.doi.org/10.1038/nmeth.1935},
volume = {9},
year = {2012}
}
@inproceedings{Bowe2012,
address = {Berlin, Heidelberg},
author = {Bowe, Alexander and Onodera, Taku and Sadakane, Kunihiko and Shibuya, Tetsuo},
editor = {Raphael, Ben and Tang, Jijun},
pages = {225--235},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Succinct de Bruijn graphs}},
url = {http://dl.acm.org/citation.cfm?id=2415255.2415273},
volume = {7534},
year = {2012}
}
@article{Chinnaiyan2012,
abstract = {BACKGROUND:Despite significant advancement in alignment algorithms, the exponential growth of nucleotide sequencing throughput threatens to outpace bioinformatic analysis. Computation may become the bottleneck of genome analysis if growing alignment costs are not mitigated by further improvement in algorithms. Much gain has been gleaned from indexing and compressing alignment databases, but many widely used alignment tools process input reads sequentially and are oblivious to any underlying redundancy in the reads themselves.RESULTS:Here we present Oculus, a software package that attaches to standard aligners and exploits read redundancy by performing streaming compression, alignment, and decompression of input sequences. This nearly lossless process (>0 99.9\%) led to alignment speedups of up to 270\% across a variety of data sets, while requiring a modest amount of memory. We expect that streaming read compressors such as Oculus could become a standard addition to existing RNA-Seq and ChIP-Seq alignment pipelines, and potentially other applications in the future as throughput increases.CONCLUSIONS:Oculus efficiently condenses redundant input reads and wraps existing aligners to provide nearly identical SAM output in a fraction of the aligner runtime. It includes a number of useful features, such as tunable performance and fidelity options, compatibility with FASTA or FASTQ files, and adherence to the SAM format. The platform-independent C++ source code is freely available online, at http://code.google.com/p/oculus-bio.},
author = {Chinnaiyan, Arul and Iyer, Matthew and Veeneman, Brendon},
issn = {1471-2105},
journal = {BMC Bioinformatics},
number = {1},
pages = {297},
title = {{Oculus: faster sequence alignment by streaming read compression}},
url = {http://www.biomedcentral.com/1471-2105/13/297},
volume = {13},
year = {2012}
}
@article{Conway2012,
abstract = {The de novo assembly of short read high-throughput sequencing data poses significant computational challenges. The volume of data is huge; the reads are tiny compared to the underlying sequence, and there are significant numbers of sequencing errors. There are numerous software packages that allow users to assemble short reads, but most are either limited to relatively small genomes (e.g. bacteria) or require large computing infrastructure or employ greedy algorithms and thus often do not yield high-quality results.},
author = {Conway, Thomas and Wazny, Jeremy and Bromage, Andrew and Zobel, Justin and Beresford-Smith, Bryan},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
number = {14},
pages = {1937--8},
title = {{Gossamer--a resource-efficient de novo assembler.}},
url = {http://bioinformatics.oxfordjournals.org/content/28/14/1937.full},
volume = {28},
year = {2012}
}
@article{Iqbal2012,
abstract = {Detecting genetic variants that are highly divergent from a reference sequence remains a major challenge in genome sequencing. We introduce de novo assembly algorithms using colored de Bruijn graphs for detecting and genotyping simple and complex genetic variants in an individual or population. We provide an efficient software implementation, Cortex, the first de novo assembler capable of assembling multiple eukaryotic genomes simultaneously. Four applications of Cortex are presented. First, we detect and validate both simple and complex structural variations in a high-coverage human genome. Second, we identify more than 3 Mb of sequence absent from the human reference genome, in pooled low-coverage population sequence data from the 1000 Genomes Project. Third, we show how population information from ten chimpanzees enables accurate variant calls without a reference sequence. Last, we estimate classical human leukocyte antigen (HLA) genotypes at HLA-B, the most variable gene in the human genome.},
author = {Iqbal, Zamin and Caccamo, Mario and Turner, Isaac and Flicek, Paul and McVean, Gil},
issn = {1546-1718},
journal = {Nature genetics},
keywords = {Algorithms,Animals,Base Sequence,Chromosome Mapping,Genome, Human,Genome, Human: genetics,Genotyping Techniques,HLA-B Antigens,HLA-B Antigens: genetics,Humans,Pan troglodytes,Pan troglodytes: genetics,Sequence Analysis, DNA,Software},
number = {2},
pages = {226--32},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
shorttitle = {Nat Genet},
title = {{De novo assembly and genotyping of variants using colored de Bruijn graphs.}},
url = {http://dx.doi.org/10.1038/ng.1028},
volume = {44},
year = {2012}
}
@article{Loh2012,
author = {Loh, Po-Ru and Baym, Michael and Berger, Bonnie},
issn = {1546-1696},
journal = {Nature biotechnology},
number = {7},
pages = {627--30},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
shorttitle = {Nat Biotech},
title = {{Compressive genomics.}},
url = {http://dx.doi.org/10.1038/nbt.2241},
volume = {30},
year = {2012}
}
@article{Pell2012,
abstract = {Deep sequencing has enabled the investigation of a wide range of environmental microbial ecosystems, but the high memory requirements for de novo assembly of short-read shotgun sequencing data from these complex populations are an increasingly large practical barrier. Here we introduce a memory-efficient graph representation with which we can analyze the k-mer connectivity of metagenomic samples. The graph representation is based on a probabilistic data structure, a Bloom filter, that allows us to efficiently store assembly graphs in as little as 4 bits per k-mer, albeit inexactly. We show that this data structure accurately represents DNA assembly graphs in low memory. We apply this data structure to the problem of partitioning assembly graphs into components as a prelude to assembly, and show that this reduces the overall memory requirements for de novo assembly of metagenomes. On one soil metagenome assembly, this approach achieves a nearly 40-fold decrease in the maximum memory requirements for assembly. This probabilistic graph representation is a significant theoretical advance in storing assembly graphs and also yields immediate leverage on metagenomic assembly.},
author = {Pell, Jason and Hintze, Arend and Canino-Koning, Rosangela and Howe, Adina and Tiedje, James M and Brown, C Titus},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Base Pairing,Base Pairing: genetics,Chromosomes, Bacterial,Chromosomes, Bacterial: genetics,Computational Biology,DNA, Circular,DNA, Circular: genetics,Escherichia coli,Escherichia coli: genetics,Genome, Bacterial,Genome, Bacterial: genetics,Information Theory,Metagenome,Metagenome: genetics,Nonlinear Dynamics,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Soil Microbiology},
number = {33},
pages = {13272--7},
title = {{Scaling metagenome sequence assembly with probabilistic de Bruijn graphs.}},
url = {http://www.pnas.org/content/109/33/13272.short},
volume = {109},
year = {2012}
}
@book{Raphael2012,
address = {Berlin, Heidelberg},
editor = {Raphael, Ben and Tang, Jijun},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Algorithms in Bioinformatics}},
url = {http://www.springerlink.com/index/10.1007/978-3-642-33122-0},
volume = {7534},
year = {2012}
}
@article{Roy2012,
abstract = {Data compression plays an important role to deal with high volumes of DNA sequences in the field of Bioinformatics. Again data compression techniques directly affect the alignment of DNA sequences. So the time needed to decompress a compressed sequence has to be given equal priorities as with compression ratio. This article contains first introduction then a brief review of different biological sequence compression after that my proposed work then our two improved Biological sequence compression algorithms after that result followed by conclusion and discussion, future scope and finally references. These algorithms gain a very good compression factor with higher saving percentage and less time for compression and decompression than the previous Biological Sequence compression algorithms. Keywords: Hash map table, Tandem repeats, compression factor, compression time, saving percentage, compression, decompression process.},
author = {Roy, Subhankar and Khatua, Sunirmal and Roy, Sudipta and Bandyopadhyay, Samir K.},
pages = {9},
title = {{An Efficient Biological Sequence Compression Technique Using LUT And Repeat In The Sequence}},
url = {http://arxiv.org/abs/1209.5905},
year = {2012}
}
@article{Selman2012,
abstract = {Little is known about the processes that enable influenza A viruses to jump into new host species. Here we show that the non-structural protein1 nucleotide substitution, A374G, encoding the D125G(GATGGT) mutation, which evolved during the adaptation of a human virus within a mouse host, activates a novel donor splice site in the non-structural gene, hence producing a novel influenza A viral protein, NS3. Using synonymous 125G mutations that do not activate the novel donor splice site, NS3 was shown to provide replicative gain-of-function. The protein sequence of NS3 is similar to NS1 protein but with an internal deletion of a motif comprised of three antiparallel $\beta$-strands spanning codons 126 to 168 in NS1. The NS1-125G(GGT) codon was also found in 33 natural influenza A viruses that were strongly associated with switching from avian to mammalian hosts, including human, swine and canine populations. In addition to the experimental human to mouse switch, the NS1-125G(GGT) codon was selected on avian to human transmission of the 1997 H5N1 and 1999 H9N2 lineages, as well as the avian to swine jump of 1979 H1N1 Eurasian swine influenza viruses, linking the NS1 125G(GGT) codon with host adaptation and switching among multiple species.},
author = {Selman, Mohammed and Dankar, Samar K and Forbes, Nicole E and Jia, Jian-Jun and Brown, Earl G},
issn = {2222-1751},
journal = {Emerging Microbes \& Infections},
keywords = {InfluenzaMotivation},
mendeley-tags = {InfluenzaMotivation},
number = {11},
pages = {e42},
publisher = {Shanghai Shangyixun Cultural Communication Co., Ltd},
shorttitle = {Emerg Microbes Infect},
title = {{Adaptive mutation in influenza A virus non-structural gene is linked to host switching and induces a novel protein by alternative splicing}},
url = {http://dx.doi.org/10.1038/emi.2012.38},
volume = {1},
year = {2012}
}
@article{Simpson2012,
abstract = {De novo genome sequence assembly is important both to generate new sequence assemblies for previously uncharacterized genomes and to identify the genome sequence of individuals in a reference-unbiased way. We present memory efficient data structures and algorithms for assembly using the FM-index derived from the compressed Burrows-Wheeler transform, and a new assembler based on these called SGA (String Graph Assembler). We describe algorithms to error-correct, assemble, and scaffold large sets of sequence data. SGA uses the overlap-based string graph model of assembly, unlike most de novo assemblers that rely on de Bruijn graphs, and is simply parallelizable. We demonstrate the error correction and assembly performance of SGA on 1.2 billion sequence reads from a human genome, which we are able to assemble using 54 GB of memory. The resulting contigs are highly accurate and contiguous, while covering 95\% of the reference genome (excluding contigs <200 bp in length). Because of the low memory requirements and parallelization without requiring inter-process communication, SGA provides the first practical assembler to our knowledge for a mammalian-sized genome on a low-end computing cluster.},
author = {Simpson, Jared T and Durbin, Richard},
issn = {1549-5469},
journal = {Genome research},
keywords = {Algorithms,Animals,Computational Biology,Computational Biology: methods,Data Compression,Genomics,Genomics: methods,Humans,Internet,Reproducibility of Results,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Software},
number = {3},
pages = {549--56},
title = {{Efficient de novo assembly of large genomes using compressed data structures.}},
url = {http://genome.cshlp.org/content/22/3/549.abstract},
volume = {22},
year = {2012}
}
@article{Wandelt2012,
abstract = {Modern high-throughput sequencing technologies are able to generate DNA sequences at an ever increasing rate. In parallel to the decreasing experimental time and cost necessary to produce DNA sequences, computational requirements for analysis and storage of the sequences are steeply increasing. Compression is a key technology to deal with this challenge. Recently, referential compression schemes, storing only the differences between a to-be-compressed input and a known reference sequence, gained a lot of interest in this field. However, memory requirements of the current algorithms are high and run times often are slow. In this paper, we propose an adaptive, parallel and highly efficient referential sequence compression method which allows fine-tuning of the trade-off between required memory and compression speed. When using 12 MB of memory, our method is for human genomes on-par with the best previous algorithms in terms of compression ratio (400:1) and compression speed. In contrast, it compresses a complete human genome in just 11 seconds when provided with 9 GB of main memory, which is almost three times faster than the best competitor while using less main memory.},
author = {Wandelt, Sebastian and Leser, Ulf},
issn = {1748-7188},
journal = {Algorithms for Molecular Biology},
number = {1},
pages = {30},
title = {{Adaptive efficient compression of genomes}},
url = {http://www.almob.org/content/7/1/30},
volume = {7},
year = {2012}
}
@article{Wu2012,
abstract = {One of the difficulties in metagenomic assembly is that homologous genes from evolutionarily closely related species may behave like repeats and confuse assemblers. As a result, small contigs, each representing a short gene fragment, instead of complete genes, may be reported by an assembler. This further complicates annotation of metagenomic datasets, as annotation tools (such as gene predictors or similarity search tools) typically perform poorly on configs encoding short gene fragments.},
author = {Wu, Yu-Wei and Rho, Mina and Doak, Thomas G and Ye, Yuzhen},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
number = {18},
pages = {i363--i369},
title = {{Stitching gene fragments with a network matching algorithm improves gene assembly for metagenomics.}},
url = {http://bioinformatics.oxfordjournals.org/content/28/18/i363.abstract},
volume = {28},
year = {2012}
}
@misc{Bhola2011,
abstract = {In this paper, we propose a system to compress Next Generation Sequencing (NGS) information stored in a FASTQ file. A FASTQ file contains text, DNA read and quality information for millions or billions of reads. The proposed system first parses the FASTQ file into its component fields. In a partial first pass it gathers statistics which are then used to choose a representation for each field that can give the best compression. Text data is further parsed into repeating and variable components and entropy coding is used to compress the latter. Similarly, Markov encoding and repeat finding based methods are used for DNA read compression. Finally, we propose several run length based methods to encode quality data choosing the method that gives the best performance for a given set of quality values. The compression system provides features for loss less and nearly loss less compression as well as compressing only read and read + quality data. We compare its performance to bzip2 text compression utility and an existing benchmark algorithm. We observe that the performance of the proposed system is superior to that of both the systems.},
author = {Bhola, Vishal and Bopardikar, Ajit S and Narayanan, Rangavittal and Lee, Kyusang and Ahn, TaeJin},
booktitle = {2011 IEEE International Conference on Bioinformatics and Biomedicine},
keywords = {fastq,genomic data compression,next generation sequencing},
pages = {147--150},
publisher = {IEEE},
title = {{No-Reference Compression of Genomic Data Stored in FASTQ Format}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6120426},
year = {2011}
}
@article{Claude2011,
address = {New York, New York, USA},
author = {Claude, Francisco and Fari\~{n}a, Antonio and Mart\'{\i}nez-Prieto, Miguel a. and Navarro, Gonzalo},
doi = {10.1145/2063576.2063646},
isbn = {9781450307178},
journal = {Proceedings of the 20th ACM international conference on Information and knowledge management - CIKM '11},
pages = {463},
publisher = {ACM Press},
title = {{Indexes for highly repetitive document collections}},
url = {http://dl.acm.org/citation.cfm?doid=2063576.2063646},
year = {2011}
}
@article{Conway2011,
abstract = {MOTIVATION: Second-generation sequencing technology makes it feasible for many researches to obtain enough sequence reads to attempt the de novo assembly of higher eukaryotes (including mammals). De novo assembly not only provides a tool for understanding wide scale biological variation, but within human biomedicine, it offers a direct way of observing both large-scale structural variation and fine-scale sequence variation. Unfortunately, improvements in the computational feasibility for de novo assembly have not matched the improvements in the gathering of sequence data. This is for two reasons: the inherent computational complexity of the problem and the in-practice memory requirements of tools. RESULTS: In this article, we use entropy compressed or succinct data structures to create a practical representation of the de Bruijn assembly graph, which requires at least a factor of 10 less storage than the kinds of structures used by deployed methods. Moreover, because our representation is entropy compressed, in the presence of sequencing errors it has better scaling behaviour asymptotically than conventional approaches. We present results of a proof-of-concept assembly of a human genome performed on a modest commodity server.},
author = {Conway, Thomas C and Bromage, Andrew J},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Computational Biology,Computational Biology: methods,Genome, Human,Genomics,Genomics: methods,Humans,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Software},
number = {4},
pages = {479--86},
title = {{Succinct data structures for assembling large genomes.}},
url = {http://bioinformatics.oxfordjournals.org/content/27/4/479.abstract?ijkey=3011d8482e5eafcf19872d001edeabf6680c5790\&keytype2=tf\_ipsecsha},
volume = {27},
year = {2011}
}
@article{Deorowicz2011,
abstract = {Modern sequencing instruments are able to generate at least hundreds of millions short reads of genomic data. Those huge volumes of data require effective means to store them, provide quick access to any record and enable fast decompression.},
author = {Deorowicz, Sebastian and Grabowski, Szymon},
institution = {Institute of Informatics, Silesian University of Technology, Akademicka 16, 44-100 Gliwice, Poland. sebastian.deorowicz@polsl.pl},
journal = {Bioinformatics},
keywords = {algorithms,base sequence,computational biology,computational biology methods,data compression,data compression methods,dna,genomics,internet,sequence analysis},
number = {6},
pages = {860--862},
title = {{Compression of DNA sequence reads in FASTQ format.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21252073},
volume = {27},
year = {2011}
}
@article{Jez2011,
abstract = {In this paper, a fully compressed pattern matching problem is studied. The compression is represented by straight-line programs (SLPs), i.e. a context-free grammars generating exactly one string; the term fully means that both the pattern and the text are given in the compressed form. The problem is approached using a recently developed technique of local recompression: the SLPs are refactored, so that substrings of the pattern and text are encoded in both SLPs in the same way. To this end, the SLPs are locally decompressed and then recompressed in a uniform way.   This technique yields an O((n+m)log M) algorithm for compressed pattern matching, assuming that M fits in O(1) machine words, where n (m) is the size of the compressed representation of the text (pattern, respectively), while M is the size of the decompressed pattern. If only m+n fits in O(1) machine words, the running time increases to O((n+m)log M log(n+m)). The previous best algorithm due to Lifshits had O(n\^{}2m) running time.},
author = {Jeż, Artur},
title = {{Faster fully compressed pattern matching by recompression}},
url = {http://arxiv.org/abs/1111.3244},
year = {2011}
}
@article{Pinho2011,
abstract = {Research in the genomic sciences is confronted with the volume of sequencing and resequencing data increasing at a higher pace than that of data storage and communication resources, shifting a significant part of research budgets from the sequencing component of a project to the computational one. Hence, being able to efficiently store sequencing and resequencing data is a problem of paramount importance. In this article, we describe GReEn (Genome Resequencing Encoding), a tool for compressing genome resequencing data using a reference genome sequence. It overcomes some drawbacks of the recently proposed tool GRS, namely, the possibility of compressing sequences that cannot be handled by GRS, faster running times and compression gains of over 100-fold for some sequences. This tool is freely available for non-commercial use at ftp://ftp.ieeta.pt/ap/codecs/GReEn1.tar.gz.},
author = {Pinho, Armando J and Pratas, Diogo and Garcia, Sara P},
file = {::},
issn = {13624962},
journal = {Nucleic Acids Research},
number = {4},
pages = {1--8},
publisher = {Oxford Univ Press},
title = {{GReEn: a tool for efficient compression of genome resequencing data.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3287168\&tool=pmcentrez\&rendertype=abstract},
volume = {40},
year = {2011}
}
@article{Rajarajeswari2011,
abstract = {Data compression is concerned with how information is organized in data. Efficient storage means removal of redundancy from the data being stored in the DNA molecule. Data compression algorithms remove redundancy and are used to understand biologically important molecules. We present a compression algorithm, DNABIT Compress for DNA sequences based on a novel algorithm of assigning binary bits for smaller segments of DNA bases to compress both repetitive and non repetitive DNA sequence. Our proposed algorithm achieves the best compression ratio for DNA sequences for larger genome. Significantly better compression results show that DNABIT Compress algorithm is the best among the remaining compression algorithms. While achieving the best compression ratios for DNA sequences (Genomes),our new DNABIT Compress algorithm significantly improves the running time of all previous DNA compression programs. Assigning binary bits (Unique BIT CODE) for (Exact Repeats, Reverse Repeats) fragments of DNA sequence is also a unique concept introduced in this algorithm for the first time in DNA compression. This proposed new algorithm could achieve the best compression ratio as much as 1.58 bits/bases where the existing best methods could not achieve a ratio less than 1.72 bits/bases.},
author = {Rajarajeswari, Pothuraju and Apparao, Allam},
file = {::},
journal = {Bioinformation},
keywords = {arithmetic coding,biocompress,dna compress,gencompress,ziv lempel},
number = {8},
pages = {350--360},
publisher = {Biomedical Informatics},
title = {{DNABIT Compress – Genome compression algorithm}},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3046040/},
volume = {5},
year = {2011}
}
@article{Wang2011,
abstract = {With the advent of DNA sequencing technologies, more and more reference genome sequences are available for many organisms. Analyzing sequence variation and understanding its biological importance are becoming a major research aim. However, how to store and process the huge amount of eukaryotic genome data, such as those of the human, mouse and rice, has become a challenge to biologists. Currently available bioinformatics tools used to compress genome sequence data have some limitations, such as the requirement of the reference single nucleotide polymorphisms (SNPs) map and information on deletions and insertions. Here, we present a novel compression tool for storing and analyzing Genome ReSequencing data, named GRS. GRS is able to process the genome sequence data without the use of the reference SNPs and other sequence variation information and automatically rebuild the individual genome sequence data using the reference genome sequence. When its performance was tested on the first Korean personal genome sequence data set, GRS was able to achieve 159-fold compression, reducing the size of the data from 2986.8 to 18.8 MB. While being tested against the sequencing data from rice and Arabidopsis thaliana, GRS compressed the 361.0 MB rice genome data to 4.4 MB, and the A. thaliana genome data from 115.1MB to 6.5KB. This de novo compression tool is available at http://gmdd.shgmo.org/Computational-Biology/GRS.},
author = {Wang, Congmao and Zhang, Dabing},
editor = {Popper, Zo\"{e} A},
file = {::},
institution = {School of Life Sciences and Biotechnology, Key Laboratory of Genetics \& Development and Neuropsychiatric Diseases, Ministry of Education, Shanghai Jiao Tong University, Shanghai 200240, China.},
journal = {Nucleic Acids Research},
number = {7},
pages = {e45},
publisher = {Oxford University Press},
series = {Methods in Molecular Biology},
title = {{A novel compression tool for efficient storage of genome resequencing data}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3074166\&tool=pmcentrez\&rendertype=abstract},
volume = {39},
year = {2011}
}
@article{Gagie2010,
abstract = {Wavelet trees are widely used in the representation of sequences, permutations, text collections, binary relations, discrete points, and other succinct data structures. We show, however, that this still falls short of exploiting all of the virtues of this versatile data structure. In particular we show how to use wavelet trees to solve fundamental algorithmic problems such as em range quantile queries, em range next value queries, and em range intersection queries. We explore several applications of these queries in Information Retrieval, in particular em document retrieval in hierarchical and temporal documents, and in the representation of em inverted lists.},
author = {Gagie, Travis and Navarro, Gonzalo and Puglisi, Simon J},
file = {::},
journal = {Theoretical Computer Science},
pages = {25--41},
title = {{New Algorithms on Wavelet Trees and Applications to Information Retrieval}},
url = {http://arxiv.org/abs/1011.4532},
year = {2010}
}
@misc{Kaipa2010,
abstract = {For DNA sequence Compression, it has been observed that methods based on Markov modeling and repeats give best results. However, these methods tend to use uniform distribution assumption of mismatches for approximate repeats. We show that these replacements are not uniformly distributed and we can improve compression efficiency by using non uniform distribution for mismatches. We also propose a hash table based method to predict repeat location which works well for block based genomic sequence compression algorithms. The proposed methods give good compression gains. The method can be incorporated into any algorithm that uses approximate repeats to realize similar gains.},
author = {Kaipa, Kalyan Kumar and Bopardikar, Ajit S and Abhilash, Srikantha and Venkataraman, Parthasarathy and Lee, Kyusang and Ahn, Taejin and Narayanan, Rangavittal},
booktitle = {Bioinformatics and Biomedicine Workshops BIBMW 2010 IEEE International Conference on},
institution = {SAIT India Lab, Samsung, Bangalore, India},
pages = {851--852},
publisher = {IEEE},
title = {{Algorithm for DNA sequence compression based on prediction of mismatch bases and repeat location}},
year = {2010}
}
@article{Brandon2009,
abstract = {Motivation: The continuing exponential accumulation of full genome data, including full diploid human genomes, creates new challenges not only for understanding genomic structure, function and evolution, but also for the storage, navigation and privacy of genomic data. Here, we develop data structures and algorithms for the efficient storage of genomic and other sequence data that may also facilitate querying and protecting the data. Results: The general idea is to encode only the differences between a genome sequence and a reference sequence, using absolute or relative coordinates for the location of the differences. These locations and the corresponding differential variants can be encoded into binary strings using various entropy coding methods, from fixed codes such as Golomb and Elias codes, to variables codes, such as Huffman codes. We demonstrate the approach and various tradeoffs using highly variables human mitochondrial genome sequences as a testbed. With only a partial level of optimization, 3615 genome sequences occupying 56 MB in GenBank are compressed down to only 167 KB, achieving a 345-fold compression rate, using the revised Cambridge Reference Sequence as the reference sequence. Using the consensus sequence as the reference sequence, the data can be stored using only 133 KB, corresponding to a 433-fold level of compression, roughly a 23\% improvement. Extensions to nuclear genomes and high-throughput sequencing data are discussed. Availability: Data are publicly available from GenBank, the HapMap web site, and the MITOMAP database. Supplementary materials with additional results, statistics, and software implementations are available from http://mammag.web.uci.edu/bin/view/Mitowiki/ProjectDNACompression. Contact: pfbaldiics.uci.edu},
author = {Brandon, Marty C and Wallace, Douglas C and Baldi, Pierre},
institution = {Department of Computer Science, UCI, Irvine, CA 92697, USA.},
journal = {Bioinformatics},
keywords = {algorithms,data compression,data compression methods,dna,dna methods,genome,genomics,genomics methods,humans,sequence analysis},
number = {14},
pages = {1731--1738},
publisher = {Oxford University Press},
title = {{Data structures and compression algorithms for genomic sequence data}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2705231\&tool=pmcentrez\&rendertype=abstract},
volume = {25},
year = {2009}
}
@article{Christley2009,
abstract = {SUMMARY: The amount of genomic sequence data being generated and made available through public databases continues to increase at an ever-expanding rate. Downloading, copying, sharing and manipulating these large datasets are becoming difficult and time consuming for researchers. We need to consider using advanced compression techniques as part of a standard data format for genomic data. The inherent structure of genome data allows for more efficient lossless compression than can be obtained through the use of generic compression programs. We apply a series of techniques to James Watson's genome that in combination reduce it to a mere 4MB, small enough to be sent as an email attachment.},
author = {Christley, Scott and Lu, Yiming and Li, Chen and Xie, Xiaohui},
issn = {13674811},
journal = {Bioinformatics},
number = {2},
pages = {274--5},
publisher = {Oxford Univ Press},
title = {{Human genomes as email attachments.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18996942},
volume = {25},
year = {2009}
}
@article{Ferragina2009,
abstract = {A compressed full- text self- index represents a text in a compressed form and still answers queries efficiently. This represents a significant advancement over the (full-) text indexing techniques of the previous decade, whose indexes required several times the size of the text . ...},
author = {Ferragina, P and Gonz\'{a}lez, R and Navarro, G and Venturini, R},
issn = {10846654},
journal = {Journal of Experimental Algorithmics JEA},
number = {December},
pages = {1--12},
publisher = {ACM},
title = {{Compressed text indexes: From theory to practice}},
url = {http://portal.acm.org/citation.cfm?id=1412228.1455268},
volume = {13},
year = {2009}
}
@article{Liu2009,
abstract = {Matrix protein 1 (M1), the major structural protein of the avian influenza virus, plays a critical role in regulation of viral RNA transcription via interaction with RNA and transportation of RNP cores. Mutations in M1 have been frequently observed in the highly virulent avian influenza H5N1 virus, which might be crucial to the pathogenic function. Here we report the characterization of mutated peptides in M1 purified from highly pathogenic avian influenza virus H5N1 by nanoelectrospray MS and MS/MS analyses on a quadrupole-time-of-flight mass spectrometer (Q-TOFMS). The specificity of tandem mass spectrometry allowed the identification of six amino acid (AA) substitutions in M1, including R95K, A166V, I168T, N207S, N224S, and R230K. Two commonly observed modifications such as oxidation and deamidation were accurately assigned in the protein. Bioinformatics analysis suggested some relationship between the amino acid substitution and structural property of M1 protein. Discussions on de novo sequencing of MS/MS spectra, especially in dealing with the AA substitutions, were provided.},
author = {Liu, Ning and Song, Wenjun and Lee, Kim-Chung and Wang, Pui and Chen, Honglin and Cai, Zongwei},
issn = {1044-0305},
journal = {Journal of the American Society for Mass Spectrometry},
keywords = {Amino Acid Sequence,Amino Acid Substitution,Animals,Chick Embryo,Influenza A Virus, H5N1 Subtype,Influenza A Virus, H5N1 Subtype: chemistry,InfluenzaMotivation,Molecular Sequence Data,Nanotechnology,Nanotechnology: methods,Spectrometry, Mass, Electrospray Ionization,Spectrometry, Mass, Electrospray Ionization: metho,Tandem Mass Spectrometry,Tandem Mass Spectrometry: methods,Viral Matrix Proteins,Viral Matrix Proteins: chemistry,Viral Matrix Proteins: genetics},
mendeley-tags = {InfluenzaMotivation},
number = {2},
pages = {312--20},
title = {{Identification of amino acid substitutions in avian influenza virus (H5N1) matrix protein 1 by using nanoelectrospray MS and MS/MS.}},
url = {http://dx.doi.org/10.1016/j.jasms.2008.10.010},
volume = {20},
year = {2009}
}
@inproceedings{Jung2008,
abstract = {After identifying the function of a protein, biologists produce new useful proteins by substituting some residues of the identified protein. These new proteins have high sequence homology (similarity). We define a sequence cluster as a cluster that is constituted of similar sequences. As another example of a sequence cluster, we consider a SNP (single nucleotide polymorphism) cluster. A SNP is a DNA sequence variation occurring when a single nucleotide in the genome (or other shared sequence) differs between members of a species (or between paired chromosomes in an individual). We suggest a new compressing technique for these sequence clusters using a sequence alignment method. We select a representative sequence which has a minimum sequence distance in the cluster by scanning distances of all sequences. The distances are obtained by calculating a sequence alignment score. The result of this sequence alignment is utilized to author conversion information called an edit-script between the two sequences. We only stored representative sequences and edit-scripts of each cluster into a database. Member sequences of each cluster can then be easily created using representative sequences and edit-scripts.},
author = {Jung, Kwang Su and Yu, Nam Hee and Shin, Seung Jung and Ryu, Keun Ho},
booktitle = {2008 8th IEEE International Conference on Computer and Information Technology},
pages = {520--525},
publisher = {IEEE},
title = {{A compressing method for genome sequence cluster using sequence alignment}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4594729},
year = {2008}
}
@article{Lam2008,
abstract = {MOTIVATION: Recent experimental studies on compressed indexes (BWT, CSA, FM-index) have confirmed their practicality for indexing very long strings such as the human genome in the main memory. For example, a BWT index for the human genome (with about 3 billion characters) occupies just around 1 G bytes. However, these indexes are designed for exact pattern matching, which is too stringent for biological applications. The demand is often on finding local alignments (pairs of similar substrings with gaps allowed). Without indexing, one can use dynamic programming to find all the local alignments between a text T and a pattern P in O(TP) time, but this would be too slow when the text is of genome scale (e.g. aligning a gene with the human genome would take tens to hundreds of hours). In practice, biologists use heuristic-based software such as BLAST, which is very efficient but does not guarantee to find all local alignments. RESULTS: In this article, we show how to build a software called BWT-SW that exploits a BWT index of a text T to speed up the dynamic programming for finding all local alignments. Experiments reveal that BWT-SW is very efficient (e.g. aligning a pattern of length 3 000 with the human genome takes less than a minute). We have also analyzed BWT-SW mathematically for a simpler similarity model (with gaps disallowed), and we show that the expected running time is O(/T/(0.628)/P/) for random strings. As far as we know, BWT-SW is the first practical tool that can find all local alignments. Yet BWT-SW is not meant to be a replacement of BLAST, as BLAST is still several times faster than BWT-SW for long patterns and BLAST is indeed accurate enough in most cases (we have used BWT-SW to check against the accuracy of BLAST and found that only rarely BLAST would miss some significant alignments). AVAILABILITY: www.cs.hku.hk/\~{}ckwong3/bwtsw CONTACT: twlamcs.hku.hk.},
author = {Lam, T W and Sung, W K and Tam, S L and Wong, C K and Yiu, S M},
institution = {Department of Computer Science, University of Hong Kong, Hong Kong, China. twlam@cs.hku.hk},
journal = {Bioinformatics},
keywords = {algorithms,base sequence,chromosome mapping,chromosome mapping methods,data compression,data compression methods,dna,dna genetics,dna methods,genome,human,human genetics,humans,molecular sequence data,sequence alignment,sequence alignment methods,sequence analysis},
number = {6},
pages = {791--797},
publisher = {Oxford University Press},
title = {{Compressed indexing and local alignment of DNA.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18227115},
volume = {24},
year = {2008}
}
@article{Navarro2007,
abstract = {Full-text indexes provide fast substring search over large text collections. A serious problem of these indexes has traditionally been their space consumption. A recent trend is to develop indexes that exploit the compressibility of the text, so that their size is a function of the compressed text length. This concept has evolved into self-indexes, which in addition contain enough information to reproduce any text portion, so they replace the text. The exciting possibility of an index that takes space close to that of the compressed text, replaces it, and in addition provides fast search over it, has triggered a wealth of activity and produced surprising results in a very short time, which radically changed the status of this area in less than 5 years. The most successful indexes nowadays are able to obtain almost optimal space and search time simultaneously. In this article we present the main concepts underlying (compressed) self-indexes. We explain the relationship between text entropy and regularities that show up in index structures and permit compressing them. Then we cover the most relevant self-indexes, focusing on how they exploit text compressibility to achieve compact structures that can efficiently solve various search problems. Our aim is to give the background to understand and follow the developments in this area.},
author = {Navarro, Gonzalo and M\"{a}kinen, Veli},
file = {::},
issn = {03600300},
journal = {ACM Computing Surveys},
number = {1},
pages = {2--es},
publisher = {ACM},
title = {{Compressed full-text indexes}},
url = {http://portal.acm.org/citation.cfm?doid=1216370.1216372},
volume = {39},
year = {2007}
}
@article{Shih2007,
abstract = {The HA1 domain of HA, the major antigenic protein of influenza A viruses, contains all of the antigenic sites of HA and is under continual immune-driven selection. To resolve controversies on whether only a few or many residue sites of HA1 have undergone positive selection, whether positive selection at HA1 is continual or punctuated, and whether antigenic change is punctuated, we introduce an approach to analyze 2,248 HA1 sequences collected from 1968 to 2005. We identify 95 substitutions at 63 sites from 1968 to 2005 and show that each substitution occurred very rapidly. The rapid substitution and the fact that 57 of the 63 sites are antigenic sites indicate that hitchhiking plays a minor role and that most of these sites, many more than previously found, have undergone positive selection. Strikingly, 88 of the 95 substitutions occurred in groups, and multiple mutations at antigenic sites sped up the fixation process. Our results suggest that positive selection has been ongoing most of the time, not sporadic, and that multiple mutations at antigenic sites cumulatively enhance antigenic drift, indicating that antigenic change is less punctuated than recently proposed.},
author = {Shih, Arthur Chun-Chieh and Hsiao, Tzu-Chang and Ho, Mei-Shang and Li, Wen-Hsiung},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Amino Acid Substitution,Amino Acid Substitution: genetics,Antigenic Variation,Antigenic Variation: genetics,Computational Biology,Evolution,Genetic,Hemagglutinins,Hemagglutinins: genetics,Influenza A virus,Influenza A virus: genetics,InfluenzaMotivation,Molecular,Selection},
mendeley-tags = {InfluenzaMotivation},
number = {15},
pages = {6283--8},
title = {{Simultaneous amino acid substitutions at antigenic sites drive influenza A hemagglutinin evolution.}},
url = {http://www.pnas.org/content/104/15/6283.full},
volume = {104},
year = {2007}
}
@article{Anwar2006,
abstract = {The importance of influenza viruses as worldwide infectious agents is well recognized. Specific mutations and evolution in influenza viruses is difficult to predict. We studied specific mutations in matrix protein 1 (M1) of H5N1 influenza A virus together with properties associated with it using prediction tools developed in Bioinformatics. Changes in hydrophobicity, polarity and secondary structure at the site of mutation were noticed and documented to gain insight towards its infection.},
author = {Anwar, Tamanna and Lal, Sunil K and Khan, Asad U},
issn = {0973-2063},
journal = {Bioinformation},
keywords = {InfluenzaMotivation},
mendeley-tags = {InfluenzaMotivation},
number = {7},
pages = {253--6},
title = {{Matrix protein 1: A comparative in silico study on different strains of influenza A H5N1 Virus.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1891697\&tool=pmcentrez\&rendertype=abstract},
volume = {1},
year = {2006}
}
@article{Solorzano2005,
abstract = {It has been shown previously that the nonstructural protein NS1 of influenza virus is an alpha/beta interferon (IFN-alpha/beta) antagonist, both in vitro and in experimental animal model systems. However, evidence of this function in a natural host has not yet been obtained. Here we investigated the role of the NS1 protein in the virulence of a swine influenza virus (SIV) isolate in pigs by using reverse genetics. The virulent wild-type A/Swine/Texas/4199-2/98 (TX/98) virus and various mutants encoding carboxy-truncated NS1 proteins were rescued. Growth properties of TX/98 viruses with mutated NS1, induction of IFN in tissue culture, and virulence-attenuation in pigs were analyzed and compared to those of the recombinant wild-type TX/98 virus. Our results indicate that deletions in the NS1 protein decrease the ability of the TX/98 virus to prevent IFN-alpha/beta synthesis in pig cells. Moreover, all NS1 mutant viruses were attenuated in pigs, and this correlated with the amount of IFN-alpha/beta induced in vitro. These data suggest that the NS1 protein of SIV is a virulence factor. Due to their attenuation, NS1-mutated swine influenza viruses might have a great potential as live attenuated vaccine candidates against SIV infections of pigs.},
author = {Sol\'{o}rzano, Alicia and Webby, Richard J and Lager, Kelly M and Janke, Bruce H and Garc\'{\i}a-Sastre, Adolfo and Richt, J\"{u}rgen A},
issn = {0022-538X},
journal = {Journal of virology},
keywords = {Animals,Cell Line,Chick Embryo,Dogs,Humans,Influenza A virus,Influenza A virus: genetics,Influenza A virus: pathogenicity,Influenza A virus: physiology,Influenza, Human,Influenza, Human: immunology,Influenza, Human: veterinary,Influenza, Human: virology,InfluenzaMotivation,Interferon-alpha,Interferon-alpha: antagonists \& inhibitors,Interferon-alpha: biosynthesis,Interferon-beta,Interferon-beta: antagonists \& inhibitors,Interferon-beta: biosynthesis,Mutation,Swine,Swine Diseases,Swine Diseases: immunology,Swine Diseases: virology,Viral Nonstructural Proteins,Viral Nonstructural Proteins: genetics,Virulence,Virus Replication},
mendeley-tags = {InfluenzaMotivation},
number = {12},
pages = {7535--43},
title = {{Mutations in the NS1 protein of swine influenza virus impair anti-interferon activity and confer attenuation in pigs.}},
url = {http://jvi.asm.org/content/79/12/7535.short},
volume = {79},
year = {2005}
}
@misc{Chen2004,
abstract = {We propose derivative Boyer-Moore (d-BM), a new compressed pattern matching algorithm in DNA sequences. This algorithm is based on the Boyer-Moore method, which is one of the most popular string matching algorithms. In this approach, we compress both DNA sequences and patterns by using two bits to represent each A, T, C, G character. Experiments indicate that this compressed pattern matching algorithm searches long DNA patterns (length > 50) more than 10 times faster than the exact match routine of the software package Agrep, which is known as the fastest pattern matching tool. Moreover, compression of DNA sequences by this method gives a guaranteed space saving of 75\%. In part the enhanced speed of the algorithm is due to the increased efficiency of the Boyer-Moore method resulting from an increase in alphabet size from 4 to 256.},
author = {Chen, Lei Chen Lei and Lu, Shiyong Lu Shiyong and Ram, J},
booktitle = {Computer Science and Information Technology ICCSIT 2010 3rd IEEE International Conference on},
institution = {Wayne State University, USA. ak3230@wayne.edu},
number = {Csb},
pages = {62--68},
publisher = {IEEE Computer Society},
title = {{Compressed pattern matching in DNA sequences}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16448000},
volume = {9},
year = {2004}
}
@article{Adjeroh2002,
abstract = {We investigate off-line dictionary oriented approaches to DNA sequence compression, based on the Burrows-Wheeler Transform (BWT). The preponderance of short repeating patterns is an important phenomenon in biological sequences. Here, we propose off-line methods to compress DNA sequences that exploit the different repetition structures inherent in such sequences. Repetition analysis is performed based on the relationship between the BWT and important pattern matching data structures, such as the suffix tree and suffix array. We discuss how the proposed approach can be incorporated in the BWT compression pipeline.},
author = {Adjeroh, Don and Zhang, Yong and Mukherjee, Amar and Powell, Matt and Bell, Tim},
institution = {Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV 26506-6109, USA.},
journal = {Proceedings IEEE Computer Society Bioinformatics Conference},
number = {January},
pages = {303--313},
publisher = {IEEE \{C\}omputer \{S\}ociety},
title = {{DNA sequence compression using the Burrows-Wheeler Transform}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15838146},
volume = {1},
year = {2002}
}
@article{Chen2001,
abstract = {While searching for alternative reading-frame peptides encoded by influenza A virus that are recognized by CD8+ T cells, we found an abundant immunogenic peptide encoded by the +1 reading frame of PB1. This peptide derives from a novel conserved 87-residue protein, PB1-F2, which has several unusual features compared with other influenza gene products in addition to its mode of translation. These include its absence from some animal (particularly swine) influenza virus isolates, variable expression in individual infected cells, rapid proteasome-dependent degradation and mitochondrial localization. Exposure of cells to a synthetic version of PB1-F2 induces apoptosis, and influenza viruses with targeted mutations that interfere with PB1-F2 expression induce less extensive apoptosis in human monocytic cells than those with intact PB1-F2. We propose that PB1-F2 functions to kill host immune cells responding to influenza virus infection.},
annote = {This may be a compelling candidate},
author = {Chen, W and Calvo, P A and Malide, D and Gibbs, J and Schubert, U and Bacik, I and Basta, S and O'Neill, R and Schickli, J and Palese, P and Henklein, P and Bennink, J R and Yewdell, J W},
issn = {1078-8956},
journal = {Nature medicine},
keywords = {Amino Acid Sequence,Animals,Apoptosis,Base Sequence,Conserved Sequence,Cysteine Endopeptidases,Cysteine Endopeptidases: metabolism,Half-Life,HeLa Cells,Humans,Influenza A virus,Influenza A virus: pathogenicity,InfluenzaMotivation,Mitochondrial Proteins,Mitochondrial Proteins: genetics,Mitochondrial Proteins: metabolism,Molecular Sequence Data,Multienzyme Complexes,Multienzyme Complexes: metabolism,Oligopeptides,Oligopeptides: genetics,Oligopeptides: pharmacology,Open Reading Frames,Peptide Fragments,Peptide Fragments: genetics,Peptide Fragments: pharmacology,Proteasome Endopeptidase Complex,Protein Biosynthesis,Protein Transport,Species Specificity,Viral Proteins,Viral Proteins: genetics,Viral Proteins: metabolism},
mendeley-tags = {InfluenzaMotivation},
number = {12},
pages = {1306--12},
shorttitle = {Nat Med},
title = {{A novel influenza A virus mitochondrial protein that induces cell death.}},
url = {http://dx.doi.org/10.1038/nm1201-1306},
volume = {7},
year = {2001}
}
@article{DeMoura2000,
abstract = {We present a fast compression technique for natural language texts. The novelties are that (1) decompression of arbitrary portions of the text can be done very efficiently, (2) exact search for words and phrases can be done on the compressed text directly, using any known sequential pattern-matching algorithm, and (3) word-based approximate and extended search can also be done efficiently without any decoding. The compression scheme uses a semistatic word-based model and a Huffman code where the coding alphabet is byte-oriented rather than bit-oriented. We compress typical English texts to about 30\% of their original size, against 40\% and 35\% for em Compress/ and em Gzip, respectively. Compression time is close to that of em Compress/ and approximately half of the time of em Gzip, and decompression time is lower than that of em Gzip/ and one third of that of em Compress. We present three algorithms to search the compressed text. They allow a large number of variations over the basic word and phrase search capability, such as sets of characters, arbitrary regular expressions, and approximate matching. Separators and stopwords can be discarded at search time without significantly increasing the cost. When searching for simple words, the experiments show that running our algorithms on a compressed text is twice as fast as running the best existing software on the uncompressed version of the same text. When searching complex or approximate patterns, our algorithms are up to 8 times faster than the search on uncompressed text. We also discuss the impact of our technique in inverted files pointing to logical blocks and argue for the possibility of keeping the text compressed all the time, decompressing only for displaying purposes.},
author = {{De Moura}, Edleno Silva and Navarro, Gonzalo and Ziviani, Nivio and Baeza-Yates, Ricardo A},
file = {::},
issn = {10468188},
journal = {ACM Transactions on Information Systems},
number = {2},
pages = {113--139},
publisher = {ACM Press},
title = {{Fast and flexible word searching on compressed text}},
url = {http://portal.acm.org/citation.cfm?doid=348751.348754},
volume = {18},
year = {2000}
}
