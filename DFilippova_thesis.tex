%for a more compact document, add the option openany to avoid
%starting all chapters on odd numbered pages
\documentclass[12pt]{cmuthesis}

% This is a template for a CMU thesis.  It is 18 pages without any content :-)
% The source for this is pulled from a variety of sources and people.
% Here's a partial list of people who may or may have not contributed:
%
%        bnoble   = Brian Noble
%        caruana  = Rich Caruana
%        colohan  = Chris Colohan
%        jab      = Justin Boyan
%        josullvn = Joseph O'Sullivan
%        jrs      = Jonathan Shewchuk
%        kosak    = Corey Kosak
%        mjz      = Matt Zekauskas (mattz@cs)
%        pdinda   = Peter Dinda
%        pfr      = Patrick Riley
%        dkoes = David Koes (me)

% My main contribution is putting everything into a single class files and small
% template since I prefer this to some complicated sprawling directory tree with
% makefiles.

% some useful packages
%\usepackage{times}
%\usepackage[defaultsans]{droidsans}
%\renewcommand*\familydefault{\sfdefault} %% Only if the base font of the document is to be typewriter style
%\usepackage[T1]{fontenc}

%\usepackage[sfdefault,light]{roboto}  %% Option 'sfdefault' only if the base font of the document is to be sans serif
\usepackage{libertine}
\usepackage[T1]{fontenc}

\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
% \usepackage{mathtools}
\usepackage[numbers,sort]{natbib}
\usepackage[backref,pageanchor=true,plainpages=false, pdfpagelabels, bookmarks,bookmarksnumbered,
%pdfborder=0 0 0,  %removes outlines around hyper links in online display
]{hyperref}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{xspace}
\usepackage{hyperref}

% \usepackage[all]{nowidow}

% Approximately 1" margins, more space on binding side
%\usepackage[letterpaper,twoside,vscale=.8,hscale=.75,nomarginpar]{geometry}
%for general printing (not binding)
\usepackage[letterpaper,twoside,vscale=.8,hscale=.75,nomarginpar,hmarginratio=1:1]{geometry}

\widowpenalty10000

% Provides a draft mark at the top of the document.
\draftstamp{\today}{DRAFT}

% special definitions
\newcommand\Coral{Coral\xspace}
\newcommand{\Athal}{\textit{A. thaliana}\xspace}
\newcommand{\ie}{i.e.\@}
\newcommand{\eg}{e.g.\@}
\newcommand{\etal}{et al.\@\xspace}
\newcommand{\refer}{\textsc{Referee}\xspace}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin {document}
\frontmatter

%initialize page style, so contents come out right (see bot) -mjz
\pagestyle{empty}

\title{ %% {\it \huge Thesis Proposal}\\
{\bf Visualization and Algorithms for Large Structured Biological Data}}
\author{Darya Filippova}
\date{July 2015}
\Year{2015}
\trnumber{}

\committee{
Carl Kingsford, Chair \\
Takis Benos \\
Russel Schwartz \\
Liz Marai, University of Illinois, Chicago
}

\support{This research was supported in part by \ldots}
\disclaimer{}

% copyright notice generated automatically from Year and author.
% permission added if \permission{} given.

\keywords{Computational biology, visualization, algorithms, topological domains, sequence compression}

\maketitle

% TODO: enable dedication
%\begin{dedication}
%For my dog Coco and her uncle Shiloh
%\end{dedication}

\pagestyle{plain} % for toc, was empty

%% Obviously, it's probably a good idea to break the various sections of your thesis
%% into different files and input them into this file...

\begin{abstract}
% A short summary -- one page.

% The problem.
Advances in biological technology over the last decade allow us to capture biologically meaningful data on a scale not possible before. Carefully designed experimental protocols now allow scientists to capture protein-protein interactions, dependencies between genes, their products, and metabolites for tens of thousands of targets at a time. At the same time, rapid evolution of sequencing technologies made whole genome and accurate transcriptome analyses possible. However, large amounts of data generated in these high-throughput biological experiments make data storage, analysis, and visualization difficult.
% Significant advances and contribute to our understanding and knowledge.
This dissertation discusses several analysis and visualization challenges in detail, and presents efficient and flexible solutions. %advancing our understanding of the underlying data and adding to existing body of knowledge.
First part of the dissertation identifies problems of presenting large structured data  and discusses the interfaces and processes designed to address them.
% Designed interfaces and explored their usability.
Designed algorithms and analysis techniques.

Conclusions and contributions that came out of this work.

\end{abstract}


% dedication
\begin{dedication}
To my Dad who I know would be proud of me.
\end{dedication}

\begin{acknowledgments}
CK. Ben Shneiderman. Rob Patro and Geet Duggal. All members of CK lab.

Michael Pack.

My collaborators.

People who taught me resilience.

My family for support, especially my mom.
\end{acknowledgments}



\tableofcontents
\listoffigures
\listoftables

\mainmatter

%% Double space document for easy review:
%\renewcommand{\baselinestretch}{1.66}\normalsize

% The other requirements Catherine has:
%
%  - avoid large margins.  She wants the thesis to use fewer pages,
%    especially if it requires colour printing.
%
%  - The thesis should be formatted for double-sided printing.  This
%    means that all chapters, acknowledgements, table of contents, etc.
%    should start on odd numbered (right facing) pages.
%
%  - You need to use the department standard tech report title page.  I
%    have tried to ensure that the title page here conforms to this
%    standard.
%
%  - Use a nice serif font, such as Times Roman.  Sans serif looks bad.
%
% Other than that, just make it look good...



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%
%%
%% Introductions -- why vis and why anything else
%%
%%
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}

Since the first sequencing and assembly of the human genome~\cite{FirstHumanGenome} (first genome citation), this confluence of chemistry, general and molecular biology, genetics, and medicine bore a new field of computational biology --- and it continues to grow and present new challenges some of which stem from the sheer volume of the data and the complexity of the hypotheses it generates. Sequencing technologies have vastly improved since then sharply dropping the cost of sequencing and helping it become a commonplace tool in many labs and even hospitals. Advances in sequencing were followed by rapid development of other high-throughput techniques that aimed to capture interactions between proteins~\cite{ppiNetworks}, dependencies between metabolites and genes~\cite{gene2geneNetworks}, rates of gene transcription~\cite{RNA-seqTechnology}, and protein translation~\cite{riboseq}.

These new data provided a first glimpse into how organisms operate on a molecular level on a scale of thousands of genes and proteins, but often the insights were hidden under layers of experimental noise and natural variation, hindered by large data volumes. Novel methods were developed for filtering, aggregating, and identifying structure in data: methods to automatically detect plausible protein complexes~\cite{ppiClustering} and groups of genes that act in concert~\cite{geneInteractionGroups}, to automatically infer function and annotate based on existing ontologies. Every method came with its own set of parameters and ways of accounting for noise. Solutions produced by these algorithms were not always stable or optimal and often other, alternative optimal or near-optimal solutions could be just as valid and informative as the single original solution.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\linewidth]{anscombes_quartet}
  \caption{\textbf{Anscombe's quartet.} Statistician F. Anscombe constructed these four data series to stress the importance of plotting the data before computing its summary statistics. These four sets of data points are indistringuishable when considering their $x$ and $y$ mean, variance, correlation, and linear regression, yet are strickingly different when visualized.}
  \label{fig:intro:anscombe}
\end{figure}

Visualization is an important first step when working with a new set of data as it provides the means to assess the overall shape of the data, its distribution, and to detect outliers. Anscombe's quartet~\cite{anscombe} succinctly demonstrates the importance of plotting the data before analyzing it: four datasets have the same $x$ and $y$ mean and variance, Pearson's correlation and fit the same regression line, yet are strickingly different (Figure~\ref{fig:intro:anscombe}). Simiarly, complex high-dimensional biological data requires novel visualization approaches that provide an overview of the data along with more detailed and focused displays.

Large data volumes resulting from high-throughput experiments present yet another challenge. To maintain interactivity and to provide useful insights, the software must offer ways to aggregate data and generate an overview without oversimplifying or obscuring the underlying patterns. Since manual inspection is infeasible for large data, the software should offser ways to extract meaningful higher-order motifs. Aggregate structure may speed up  computation as in functional compression XXX


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Aims/Challenges}

% % Also -- challenges
% Gain insights, gneerate hypotheses -- visualizations (coral, search in SRA, map of jazz)

% From visualizations -- ways to identify prominent structure and validate its significance. domains

% use structure to develop new representations for the data and apply functional compression -- Referee.

% XXX

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Major contributions}

\begin{itemize}
  \item \textbf{Summary and detailed visualizations for alternative classifications}

  Coral XXX

  % \item \textbf{Augmented search in Sequence Read Archive (SRA)}
  % SHARQ

  \item \textbf{Focused exploration in dynamic networks}

  Map of Jazz XXX

  \item \textbf{Finding structure in chromatin conformation data}

  Armatus XXX

  \item \textbf{Compression methods for structured data}

  Referee XXX

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%
%%
%% Part 1 -- visualizaiton for biological data
%%
%%
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Visualization approaches for large structured data}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Coral --
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Interfaces for evaluation and comparison of classification results}

  Clustering has become a standard analysis step for many types of biological data, \eg~interaction networks, gene expression, and metagenomic abundance. In practice, it is possible to obtain a large number of contradictory clusterings by varying which clustering algorithm is used, which data attributes are considered, how algorithmic parameters are set, and  which near-optimal clusterings are chosen. It is a difficult task to sift though such a large collection of varied clusterings to determine which clustering features are affected by parameter settings or are artifacts of particular algorithms and which represent meaningful patterns. Knowing which items are often clustered together helps to improve our understanding of the underlying data and to increase our confidence about generated modules.

  In this chapter, we discuss design decisions and implementation of \Coral, an application for interactive exploration of ensembles of clusterings. We discuss how each visual component in \Coral tackles a specific question related to clustering comparison and provide examples of their use. We also show how \Coral could be used to visually and quantitatively compare clusterings with a ground truth clustering. 

  We showcase how to use \Coral on a collection of clusterings for a protein interaction network of \textit{Arabidopsis thaliana}. We find that the clusterings vary significantly and that few proteins are consistently co-clustered in all clusterings, and use \Coral's core identification to identify these consistent protein subgroups. Our case study shows that several clusterings should typically be considered when evaluating clusters of genes, proteins, or sequences, and Coral can be used to perform a comprehensive analysis of these clustering ensembles.

  The work presented in this chapter has appeared as a publication in BMC Bioinformatics~\cite{Filippova2012} and was presented at ISMB XXX (year) as a poster. The software is available as a standalone Java application and can be downloaded from \url{https://github.com/lynxoid/coral}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background and related work}




  Collections of protein interactions, gene expression vectors, metagenomic samples, and gene sequences containing thousands to hundreds-of-thousands of elements are now being analyzed routinely. 
  XXX more detail on the size of the problem. 
  Clustering is often used to condense such large datasets into an understandable form: it has been successfully used on protein-protein interaction (PPI) networks to discover protein complexes and predict protein function, \eg~\cite{Sharan2007}; on gene expression data to find patterns in gene regulation and essential cell processes, \eg~\cite{Ulitsky2010}; and on metagenomic samples to identify new species, compare them to existing clades, evaluate the diversity of a population, and suggest interdependencies between them~\cite{Chatterji2007, White2010}. In other words, clustering has become a ubiquitous part of analysis for large biological datasets.

  % Introduce the problem.

  There are many clustering algorithms available for numerical and network data, \eg~\cite{VanDongen2000, Bader2003, Clauset2004, Adamcsek2006, Blondel2008, Ahn2010, Jiang2010, Rhrissorrakrai2011}. Each algorithm, and choice of its parameters, results in different clusterings. Sometimes, clustering algorithms must resolve ties when generating modules or may be randomized. Consequently, a single clustering algorithm may produce diverse partitions on the same data~\cite{Navlakha10}. Clusterings may also change when the underlying data becomes increasingly noisy or displays variation under different conditions (such as varying gene expression levels). In addition, considering many optimal and near-optimal partitions has been shown to improve the understanding of module dynamics and the strength of relationships between individual items~\cite{Duggal2010, Lewis2010, Langfelder2008, Hopcroft2004}. Such clusterings may offer drastically different perspectives on the data, where assessing the commonalities and differences is of great interest.

  % existing solutions

  There are several ways in which the problem of diverse clusterings has been addressed. Some tools rely on a single clustering only and focus on module quality assessment, \eg~\cite{Yu2007a, Hibbs2005}. Comparing two or more clusterings at a time is usually done by computing a single metric, such as the Jaccard or Rand index~\cite{Thalamuthu2006}, to compare clusterings side-by-side~\cite{Seo2007a} or in a dendrogram~\cite{Laderas2007}. These approaches can easily compare a pair of clusterings, but are not extendable to greater number of clusterings. Another approach is to aggregate multiple partitions into a consensus clustering~\cite{strehl02, Monti2003a} without delving into the differences between individual clusterings and, thus, disregarding possibly important information about the clusterings. Finally, some approaches have made steps towards visual examination of multiple clusterings: King and Grimmer~\cite{Grimmer2011} compare clusterings pairwise and project the space of clusterings onto a plane to visualize a clustering landscape, and Langfelder et al.~\cite{Langfelder2011} investigate ways to compare individual modules across multiple conditions. However, none of these approaches offer a platform for a multi-level analysis of ensembles of diverse clusterings.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Interface design}

% overview -- shows Coral layout
  \begin{figure}[htb!]
    \centering
    \includegraphics[width=\linewidth]{coral_overview.png}
    \caption{\textbf{Coral overview.} Coral views in a clockwise direction: co-cluster matrix (1), pairs table (2), parallel partitions plot (3), module-to-module table (4), ladder (5), overview statistics (6). Users may rearrange the individual views or close them to focus on fewer visualizations at a time. Data: \Athal clusterings.}
    \label{fig:coral:overview}
  \end{figure}


  In \Coral's design, we followed the visualization mantra coined by
  Shneiderman~\cite{Shneiderman1996}: overview, zoom-and-filter,
  details-on-demand. At the overview level, \Coral displays dataset statistics and highlights the most similar and dissimilar clusterings; at the mid-level, ``zoomed-in,'' analysis explains similarities between clusterings through module comparison; the low-level analysis compares co-clustering patterns at the level of individual data items: the genes, proteins, or sequences. The displays are coordinated~\cite{North2000} so selecting data in one of the views highlights the corresponding items in the other views (see Figure~\ref{fig:coral:overview}).


  Coral works with modules --- groups of items closely related to one another according to some metric or property. For example, modules can constitute a collection of genes that get co-expressed together or proteins forming a complex. A clustering is a collection of modules and usually is an output of a clustering algorithm. Users may also choose to group data according to attributes that come with the data such as cellular component or molecular function GO terms and use that partition as a clustering. Users may combine data from different experiemnts and across species so long as the data items that the user treats as homologous have the same IDs across the dataset.



  \Coral takes as an input the module files where each file represents a single clustering, and each line in the file contains a list of data items (proteins, genes, or sequence ids) from a single module, \eg~as produced by MCL, the clustering algorithm by van Dongen~\cite{VanDongen2000}. \Coral aggregates and visualizes these data through several connected displays, each of which can be used to answer specific questions about the clusterings. Below, we examine a few such questions and describe how \Coral's visualizations help to answer them.

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%
  %% Subsection
  %%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsection{Summary statistics}



\begin{figure}[h]
    \centering
    \includegraphics[width=0.60\linewidth]{coral_ladder}
    \caption{\textbf{All-to-all clustering comparison in a ladder widget.} The ladder represents  a lower triangle of an all-to-all matrix where each cell $(i, j)$ holds a score for similarity between clusterings $K_{i}$ and $K_{j}$. Users can choose between several comparison metrics by toggling a dropdown above the ladder. Every cell is color-coded, with darker colors indicating more similarity between the pair} 
    \label{fig:coral:ladder}
  \end{figure}



  %
  To gain a quick overview of their collection of clusterings, \Coral users may start the analysis by reviewing basic statistics about their data: number of modules per clustering, average module size, module coverage, clustering entropy~\cite{Meila2003}:
  %
  \[
  H(K) = - \sum_{m_i \in K} p_i \log_2 p_i,
  \]
  %
  where $K$ is a clustering and $m_i$ is a module in $K$, $p_i = |m_i| / |K|$, and percentage of data items that ended up in the overlapping modules. Questions such as ``Do clusterings have the same number of modules?''\@ and ``Are module sizes evenly distributed?''\@ can be easily answered through these statistics. Each statistic is shown as a bar chart, and every clustering is associated with a unique color that is used consistently throughout \Coral to identify the clustering anywhere in the system. If a clustering contains overlapping modules, the corresponding bar in the chart is striped as opposed to a solid bar for the clusterings containing no overlapping modules (see Figure~\ref{fig:coral:overview}).

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%
  %% Subsection
  %%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Clusterings similarity}
  %
  % introduce ladder 
  \Coral computes similarity scores between all clusterings and visualizes the lower triangle of the similarity matrix in a ladder widget (Figure~\ref{fig:coral:ladder}). The ladder compactly displays similarity scores for every pair of clusterings in the ensemble allowing for precise comparisons. The ladder is color-coded as a heatmap with more intense blue cells corresponding to higher similarity scores and paler cells corresponding to low scores. Clicking a cell updates the contents of a module-to-module comparison widget (see next subsection).


  \Coral offers several choices of similarity measures to compare partitions. Four of the measures are based on the pair counting, two measures use information theory to quantify diversity of the clusterings, and the remaining measures rely on set matching to produce a score. Pair counting measures use the following four quantities to compute a score:

  % pair counting quantities
  \begin{itemize}
  \item $a$ --- pairs of items placed in a module together in clustering $K_{i}$ and $K_{j}$,
  \item  $b$ --- pairs of items placed in a module together in $K_{i}$, but not in $K_{j}$,
  \item $c$ --- pairs of items placed in separate modules in $K_{i}$, but in the same module in $K_{j}$,
  \item $d$ --- pairs of items placed in separate modules in both $K_{i}$ and $K_{j}$.
  \end{itemize}

  $K_i$ and $K_j$ are two clusterings, the measures of similarity between them are as follows:
  %
  \begin{enumerate}
    \item Jaccard coefficient for sets of sets:
    %
    \[
    J(K_i, K_j) = a / (a + b + c);
    \]
    %
    Jaccard coefficient is symmetric and ignores quantity $d$ instead focusing on the pairs of items that were in the same module in either $K_i$ or $K_j$.

    \item Mirkin coefficient:
    %
    \[
    M(K_i, K_j) = 2(b + c)
    \]
    %
    XXX -- is symmetric? Unbounded.
    % Mirkin coefficient is symmetric, but is not a metric.

    \item Rand index:
    \[
    R(K_i, K_j) = (a + d) / (a + b + c + d)
    \]
    is not symmetric.

    \item Fowlkes-Mallows index:
    %
    \[
    FM(K_i, K_j) = \sqrt{a / (a + b) a / (a + c) },
    \]
    %
    where higher values correspond to greater similarity between the clusterings.

    \item Mutual information (MI):
    %
    \[
    I(K_i,K_j) = \sum_{m_k \in K_i} \sum_{m_l \in K_j} p_{kl} 
        \log \frac{ p_{kl} }{ p_k p_l }
    \]
    %
    where $m_k$ and $m_l$ are modules in their corresponding clusterings and probabilities are calculated as follows: $p_k = |m_k| / |K_i|$, $p_l = |m_l| / |K_j|$, $p_{kl} = |m_k \cup m_l| / (|K_i| + |K_j|)$. Mutual information is symmetric, but is not a metric, although its version, variation of information, can be combined with joint entropy $H(K_i, K_j)$ to become one.

    \item Variation of information (VI):
    %
    \[
    VI(K_i,K_j) = H(K_i, K_j) - I(K_i, K_j)
    \]
    %
    where $H(K_i, K_j)$ is a joint entropy. VI is a metric.

    \item Purity index:
    %
    \[
    P(K_i,K_j) = \frac{ \sum_{m_k \in K_i} \argmax_{m_l \in K_j} |m_k \cap m_l| }{N},
    \]
    %
    where $N$ is the number of clustered data items in $K_i$. For every module in $K_i$, purity index finds the moduling in $K_j$ that overlaps with $m_k$ the most therefore capturing the maximum amount of overlap between modules in $K_i$ and $K_j$. The index is not symmetric.

    \item Inverse purity index:
    %
    \[
    IP(K_i,K_j) = \frac{ \sum_{m_l \in K_j} \argmax_{m_k \in K_i} |m_k \cap m_l| }{N},
    \]
    %
    is similar to purity, but captures the maxmium amount of overlap as viewed from $K_j$'s perspective. The index is not symmetric.

    \item F-measure~\cite{Meila2003}:
    %
    \[
    F(K_i,K_j) = \frac{ \sum_{m_k \in K_i} |m_k| \argmax_{m_l \in K_j} f(m_k, m_l) }{N/2},
    \]
    %
    where $N$ is the number of data items in $K_i$. Precision between two modules $m_k$ and $m_l$ is defined as $p(m_k, m_l) = |m_k \cap m_l| / |m_k|$ and is an asymmetric measure with recall define as its opposite: $r(m_k, m_l) = p(m_l, m_k)$. The f-score between two modules is $f(m_k,m_l) = r(m_k, m_l) p(m_k, m_l) / ( r(m_k, m_l) + p(m_k, m_l) )$.

  \end{enumerate}

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%
  %% Subsection
  %%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Module similarity across clusterings}
  \label{sec:modules}

  As a follow-up to finding a highly similar pair of clusterings, users can review the similarities between their individual modules. Is a group of interacting genes preserved between the two stages in the cell life cycle? Is there a match for a given protein complex in the PPI network of another species? Module-to-module comparison table helps to answer these questions to explains in detail what contributed to the clustering similarity at a  module level.

  For a given pair $K_1, K_2$ of clusterings, \Coral calculates the Jaccard similarity $J = |m^{1}_{i} \bigcap m^{2}_{j}| / |m^{1}_{i} \bigcup m^{2}_{j} |$ between every module $m^{1}_{i} \in K_1$ and $m^{2}_{j} \in K_2$ thus capturing the amount of overlap between the two modules. For every such module pair, \Coral displays the pair's Jaccard score and items in the union, intersection, left and right set differences. All module pairs are organized in a sortable table (see Figure~\ref{fig:coral:modules}). The slider above the table allows the user to filter out module pairs for which the Jaccard score is outside the slider's range allowing users to focus on highly similar (or dissimilar) modules. Although module-to-module analysis is possible with the parallel partitions plot (discussed below), the table offers a sortable and filterable view of the same data while supplying additional information (\eg~Jaccard index). The module-to-module table shows only the module pairs with some overlap and easily scales to hundreds of modules, thereby offering a more compact and easily navigable alternative to a confusion matrix (\eg\@ as used in~\cite{Langfelder2011}).

  % module to module comparison table
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{coral_modules}
    \caption{\textbf{Module-to-module comparison for two clusterings.} When users decide to focus on a pair of clusterings, they may explore all pairs
    of modules in a sortable table. Each module pair is shown against its Jaccard
    score, and lists of items in the module intersection, left and right
    differences. Users can filter the table rows by Jaccard score to only show rows
    within a given similarity range by adjusting the slider above the table. Cells
    holding the Jaccard scores are color-coded to indicate similarity.}
    \label{fig:coral:modules}
  \end{figure}

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%
  %% Subsection
  %%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsection{Module persistence across clusterings}

  The ability to track individual items and whole modules across multiple clusterings provides a high level of abstraction in clustering analysis: modules may split and merge as users switch from clustering to clustering. To afford an exploration at the module level, we have developed a parallel partitions plot --- an extension of a parallel sets plot used in the analysis of categorical multivariate data~\cite{Kosara2006}. The parallel partitions plot represents each clustering as a horizontal band. The blocks comprising each bands represent modules, with the width of a block proportional to the number of items in that module. Semi-transparent parallelograms between clusterings connect data items with the same name. That is, each item in a clustering will be connected to its copy in the band immediately below it (see Figure~\ref{fig:coral:parsets}).

  % parallel partitions figure
  \begin{figure}[htb!]
    \centering
    \includegraphics[width=\linewidth]{coral_parsets}
    \caption{\textbf{Parallel partitions maps modules between clusterings.} Horizontal bands represent partitions, and modules are separated by blank spaces. Semi-transparent bands connect the same items from different
   clusterings. Red traces highlight selected items across all partitions and show how modules split and merge. Here, the user has selected a group of 215 proteins that belong to the largest core in the ensemble of clusterings for \Athal PPI network. The \texttt{Louvaine}, \texttt{Clauset}, \texttt{MCL}, and \texttt{MCODE.F} clusterings assign all of the selected proteins to a single
   module. For other clusterings, most of the selected proteins are placed in a
   grab bag region of items that were not contained in any module (shown in gray).}
    \label{fig:coral:parsets}
  \end{figure}

  The parallel partitions plot allows users to track individual items and whole modules across all partitions. To see whether a module splits or merges in other clusterings, users can select a module with a mouse while holding a shift key to highlight its members in every clustering in the plot (see red traces in Figure~\ref{fig:coral:parsets}). Similarly, users may select individual items and trace them through every clustering band. The selections made in the parallel partitions plot propagate to other views making it easy to track the same group of items throughout the application. The plot is zoomable --- users may zoom in to focus on a few items at a time or zoom out to see global trends across the ensemble. When the zoom level permits it, the plot displays the item labels.

  % co-occurrence matrixes
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{coral_matrices}
    \caption{\textbf{Co-cluster matrices.} (a) A co-cluster matrix of three identical decompositions forms blocks on
    the diagonal with only red cells indicating that all three clsuterings agreed
    (synthetic data). (b) Big modules were broken up into smaller modules to form
    new clusterings (four \texttt{Hybrid} decompositions from the Langfelder
    study~\cite{Langfelder2008}).}
    \label{fig:coral:matrices}
  \end{figure}

  The order of items in the clustering bands matches the order of items in the
  co-cluster matrix (discussed below) as closely as possible, while at the same
  time attempting to minimize the amount of crossover between the parallelograms
  connecting items in the consecutive clusterings. However, the items in the bands
  must be placed inside their respective modules. We
  discuss an algorithm that finds a good ordering of items in the
  clustering bands in the Methods section.

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%
  %% Subsection
  %%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  % section's math
  \newcommand{\Mplus}{$A^{+}$~}

  %\subsection*{co-cluster matrix}
  \subsection{What other items are in the same module as a given item $u$?}

  A single clustering assigns a data item $v$ to a module defining its \textit{cohort} --- a set of items in the same module as $v$. Knowing the item's module helps in assigning function to unknown proteins~\cite{Bader2003} and novel genes~\cite{Ulitsky2010}; knowing that the item's cohort is consistent across many clusterings increases the confidence of such predictions.

  % introduce co-cluster matrix

  In \Coral, pairwise co-cluster memberships are aggregated in a \textit{co-cluster matrix}
  ~\cite{Monti2003a}. Given a single clustering $K$, $n = |K|$, we define an
  $n \times n$ matrix $A^{K}$ to be $K$'s co-cluster matrix where its entries
  $a_{ij}^{K}$ are:
  %
  \[
   a_{ij}^{K} =
    \begin{cases}
  0 & \text{$v_{i}$ and $v_{j}$ are in different modules in $K$}\\
  1 & \text{$v_{i}$ and $v_{j}$ are in the same module in $K$}.\\
    \end{cases}
  \]

  For some item pairs, co-clustering may be an artifact of a tie-breaking policy or a choice of an algorithm parameter: such item pairs may only co-cluster in few clusterings. On the other hand, we would like to know whether there were item pairs that co-clustered across most partitions in the ensemble. These cohort dynamics stand out if we sum up the co-cluster matrices to form a single matrix:
  %
  \[
   A^{+} = \sum_{t=1}^{k} A^{K_{t}},
  \]
  %
  where $A^{K_{t}}$ is a co-cluster matrix for a clustering $K_{t}$ and $k$ is the
  number of clusterings. Here, the $a_{ij}^{+}$ entries equal $k$ (the number of
  clusterings) for item pairs $(v_{i}, v_{j})$ that have co-clustered in all
  partitions suggesting a strong relationship between the items, and the low
  $a_{ij}^{+}$ values correspond to pairs that co-clustered in only a few
  clusterings and are more likely to have been assigned to the same module by
  chance. The cells are colored according to their values and vary from white
  (low values) to red (high values). Users may zoom in and out on the matrix to
  focus on areas of interest.

  % reordering, core extraction

  The co-cluster matrix is hard to read unless similar rows and columns are placed near each other. Reordering the rows and columns of \Mplus brings non-zero entries closer to the diagonal and exposes any modular structure. When clusterings are highly similar, the reordered matrix will consist of blocks along the diagonal with high $a_{ij}^{+}$ values (Figure~\ref{fig:coral:matrices}). Clusterings that are very dissimilar produce reorderings similar to Figure~\ref{fig:coral:big_matrix} --- the diagonal blocks mostly contain low $a_{ij}^{+}$ values (colored white or light pink) with many entries away from the diagonal.

  % big reordered matrix w/ corresponding network vis
  \begin{figure}[htb!]
    \centering
    \includegraphics[width=0.6\linewidth]{coral_vidal_matrix}
    \caption{\textbf{Reordered co-cluster matrix reveals co-clustering patterns.} (a) Values in the co-cluster matrix range from 1 (light pink) to 9 (red) for nine clusterings of the \Athal PPI network with 2402 proteins from~\cite{Vidal2011}. Pink regions represent item pairs that were placed in the same module by very few clusterings while regions of more saturated red represent proteins that co-clustered in most clusterings. Black indicates that the items never co-clustered. On the inset matrix, the matrix items under the green square formed a core. A large blue square overlay suggests that the core was tightly integrated into the rest of the network. (b) Left: nodes that formed a core in (a) are colored green, the edges between the nodes within the core are colored blue. The inset to the right shows an isolated view of the core nodes (green), edges between core nodes (blue), and nodes one hop away from the core nodes (gray). Green nodes share many edges with nodes outside of the core which resulted in the core's low cohesion.}
    \label{fig:coral:big_matrix}
  \end{figure}

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%
  %% Subsection
  %%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %\subsection*{Cores and periphery}
  \subsection{Subset persistence across clusterings}
  \label{sec:cores}

  Groups of items that end up in the same module across many clusterings are of a particular interest because they represent the robust subclusters in the data. Such commonly co-clustered sets are called \textit{cores}. Items in cores form the most trustworthy modules and indicate particularly strong ties between data items, increasing, for example, the confidence in protein complex identification~\cite{Luo2009} and gene annotation~\cite{Saha}.

  In a co-cluster matrix, cores translate to contiguous blocks of high-value entries. \Coral finds the cores using a fast dynamic programming algorithm and highlights them within the co-cluster matrix (inset, Figure~\ref{fig:coral:big_matrix}a). When users provide clusterings derived from a network, \Coral can augment cores with an overlay showing each core's cohesion --- the ratio $E_\textrm{in} / E_\textrm{out}$ where $E_\textrm{in}$ is the number of edges within the core and $E_\textrm{out}$ is the number of edges that have one endpoint inside the core and another endpoint outside of it~\cite{Bailey1982}. When a core's cohesion is low, the blue overlay is smaller indicating that the core shares many connections with the rest of the network (Figure~\ref{fig:coral:big_matrix}b). Cores for which cohesion is high are more isolated from the rest of the network --- these cores are distinguishable by the blue overlays that almost cover the core.

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%
  %% Subsection: Base clustering
  %%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsection{Persistence of ground-truth modules across clusterings}

  When validating new protein complexes or co-expressed gene modules, users may want to see how well their results match ground-truth clusterings such as protein complexes from MIPS~\cite{Mewes2011}, or sets of co-regulated genes from RegulonDB~\cite{Gama-Castro2011}. In \Coral, users may designate a single clustering as a \textit{base} --- a set of trusted modules with which other clusterings are expected to agree. When in this mode, \Coral will only highlight those cells in the co-cluster matrix that are within the modules of the base and gray out all other non-zero matrix cells to bring users' attention to the clustering in question. Figure~\ref{fig:coral:base_clust} shows an example of a co-cluster matrix with the base set to be the \Athal modules reported in~\cite{Vidal2011}.

  % showcasing base clustering feature
  \begin{figure}[!htb]
    \centering
    \includegraphics[width=0.7\linewidth]{coral_base_matrix}
    \caption{\textbf{Base clustering mode for the co-cluster matrix.} The base clustering highlights only the item pairs that co-clustered within the selected clustering graying out the rest of the matrix. Base clustering helps users focus on comparisons with the selected clustering. In this figure, the colored areas represent the original 26 \Athal modules; their mostly pink hue indicates that their item pairs co-clustered in few clusterings. Large areas of gray indicate that many novel modules found by other clusterings were not found by the link clustering algorithm~\cite{Ahn2010}.}
    \label{fig:coral:base_clust}
  \end{figure}

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%
  %% Subsection
  %%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %\subsection*{Pairs table}
  \subsection{In which clusterings do particular items co-cluster?}
  \label{sec:pairs_table}

  The co-cluster matrix displays the total number of times any two items were co-clustered, and the tooltips that appear after hovering over a matrix cell show a list of clusterings in which a given pair has been co-clustered. To facilitate sorting and search for particular item pairs, \Coral provides a companion table where each row represents a pair of data items and displays the number of times the items co-clustered along with the pair's \textit{signature}. The signature is a $k$-long vector where the $t^{th}$ element is 1 when both data items, say, proteins, have been placed in the same module in clustering $K_{t}$. If the pair's items were not in the same module in $K_{t}$, the $t^{th}$ element is set to 0.

  Visually, the signature's elements that are 1 are drawn as tall solid columns and zeros are represented by the short stumps using the same color for each clustering as used in the overview statistics and in the parallel partitions plot. Figure~\ref{fig:coral:signature} shows an example of two such pairs that have different co-cluster signatures suggesting that the relationship between the last two \Athal proteins is stronger than that of the first pair. Users can sort the rows by either the item name, the number of shared clusterings, or by the co-clustering signature. Users can also filter by the signatures to display only the rows matching a user's pattern.

  %co-clustering signatures
  \begin{figure}[!htb]
    \centering
    \includegraphics[width=0.4\linewidth]{coral_item_pair}
    \caption{\textbf{Co-cluster signatures help track where two items have co-clustered.} Two rows from the pairs table for the \Athal dataset: each row starts with the two item IDs (here: \Athal proteins), followed by the number of times these two proteins were co-clustered, followed by a co-cluster signature that tells in which clusterings the two proteins were co-clustered. Clusterings order for this example: \Athal, \texttt{Clauset}, \texttt{CFinder}, \texttt{Louvain}, \texttt{MCL}, \texttt{MCODE.F}, \texttt{MCODE}, \texttt{MINE}, \texttt{SPICi}. Proteins AT1G04100 and AT1G51950 co-clustered in 8 clusterings. The two share many specific GO annotations: both are involved in auxin stimulus, localize to the nucleus, and participate in protein binding and sequence-specific DNA binding transcription factor activity. AT1G04100 and AT1G50900 were in the same module just once and shared no GO annotations suggesting that the relationship between these two proteins was of a more tenuous nature.}
    \label{fig:coral:signature}
  \end{figure}

\section{Algorithmic challenges}

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%
  %% Subsection
  %%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Reordering the co-cluster matrix}
  \label{sec:reordering}

  The order of rows and columns in the co-cluster matrix is critical to extracting meaningful information from it. Finding an optimal matrix reordering is NP-complete for almost any interesting objective. Algorithms for the optimal linear arrangement~\cite{Mueller} and bandwidth minimization~\cite{Lai1982} problems have been used to reorder matrices with considerable success; however, both approaches perform poorly for matrices that have many off-diagonal elements. After comparing several reordering algorithms using the bandwidth and envelope metrics, we have chosen the SPIN~\cite{Tsafrir2005} approach that consistently produced better results on a wide range of matrices.

  This approach works as follows: given a matrix $A^{+}$, we solve a linear assignment problem (LAP) by mapping $A^{+}$'s rows to their optimal positions in the matrix. In other words, given a bipartite graph $G = (R, I, E)$ with $R$ being the set of $A$'s rows, $I$ a set of indices to which the rows will be assigned, and $E$ all possible edges between nodes in $R$ and $I$, we seek a matching between $R$ and $I$ with in a minimum cost. The edges connecting the row nodes to index nodes are weighted according to how well a row fits a particular index according to a metric that rewards rows that have non-zero entries close to diagonal and penalizes those rows that have weight away from diagonal:
  %
  \[
  w(i, \ell) = \sum_{j=1}^{n} a^{+}_{ij} |j-i|,
  \]
  %
  where $w(i, \ell)$ is the weight of assigning $i^{\textrm{th}}$ row to $\ell^{\textrm{th}}$ position, and the $a^{+}_{ij}$ values are the co-cluster matrix entries. After permuting $A^{+}$'s rows, the columns of $A^{+}$ must be permuted to match the row order, thus changing the weights $w(i,\ell)$ and making the row assignments found previously no longer optimal, so this process is repeated. In \Coral, we use two different solvers for the LAP problem: a fast, greedy solver and the Hungarian algorithm. The greedy solver matches rows to indexes by iteratively selecting the best row-index pair; it quickly finds a starter reordering that can later be improved by the Hungarian algorithm. The Hungarian algorithm solves the linear assignment problem optimally, but because a permutation of rows invalidates the ordering of the columns, the algorithm has to be re-run for several iterations to improve the reordering. We continue iterating LAP until we get no improvement in assignment cost, observe a previously considered permutation, or exceed the maximum number of iterations.

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%
  %% Identifying cores
  %%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Identifying cores}
  \label{sec:dense_sub}

  Given a reordered co-cluster matrix $A$, we want to find contiguous areas containing high co-cluster values (\textit{cores}). We rely on the notion of region density:
  %
  \begin{equation}\label{eq:dens}
    d(p, q) = \frac{ \sum_{i=p}^{q-1} \sum_{j=i+1}^{q} a^{+}_{ij}}{ |q-p|} =
    \frac{s(p,q)}{ |q-p|},
  \end{equation}
  %
  where a region is a square block on the matrix diagonal between rows $p$ and $q$, and its density is the sum $s(p, q)$ of all matrix entries within the area divided by the area's width $|q-p|$. Alternatively, we can think of the co-cluster matrix $A^{+}$ as a weighted adjacency matrix of some graph $G(A^{+})$, then $d(p,q)$ is the density of a subgraph $S$ induced by the vertices $p, \ldots, q$: $d(p,q) = |E(S)| / |V(S)|$, where $|E(S)|$ is the sum of edge weights in $S$ and $V(S)$ is a set vertices in $S$~\cite{Saha}.

  %
  % define the DP for cores, discuss runtime
  %
  To find cores, we want to find areas on the diagonal such that the sum of their densities is highest. We do not allow the identified cores to overlap (thus we require disjoint subgraphs). We formulate the problem of finding maximally dense arrangement of areas as a dynamic program with the recurrence:
  %
  \[
    D_{\textrm{\scriptsize opt}}(j) = \max_{1 \leq i < j}
            \{D_{\textrm{\scriptsize opt}}(i - 1) + d(i,j)\}.
  \]
  %
  where $D_{\textrm{opt}}(j)$ is the optimal area arrangement between $0^{th}$ and $j^{th}$ item, and $D_{\textrm{opt}}(n)$ gives the optimal partition of a matrix $A^{+}$ into cores. Assuming that densities $d(p, q)$ are precomputed and require only a constant time to look up, the dynamic program above takes $O(n^2)$ time (for each $i$, we solve at most $n$ subproblems, and $i$ ranges from 1 to $n$). However, a brute force algorithm for computing the block sums $s(p,q)$ (and, hence, the densities) in equation~\ref{eq:dens} must iterate through every pair $1 \leq p < n$, $p < q \leq n$, each time computing a sum of ${|q-p+1| \choose 2}$ entries, resulting in  a runtime of $O(n^4)$. This can be improved because the sums are related. We have:
  %
  \[
   s(p, q+1) = s(p, q) + \sum_{i = p}^{q} a_{i, q+1},
   \]
  %
  making it possible to compute all $s(p,q)$ in $O(n^{2})$ time. This reduces the total runtime to find cores to $O(n^2 + n^2) = O(n^2)$.

  % Filtering out low-probability cores

  The algorithm finds a series of areas of varying size and density. Some areas are of no interest and were included in the series only because every block contributes a non-zero score to the total sum. To focus on the meaningful regions only, we filter out the cores with density less than the average density. To calculate the average density for a region $p, \ldots, q$, we first compute an average cell value for $A^{+}$:
  \[
  w_{\textrm{avg}} = \frac{ s(1, n) } { \bar{z} },
  \]
  %
  where $\bar{z}$ is the number of non-zero cells in $A^{+}$. We then define a probability of an edge existing in a graph induced by $A^{+}$: 
  %
  \[
  P(e) = \frac{\bar{z}}{ {n-1 \choose 2} }.
  \]
  %
  Then, for a given ordering of the matrix $A^{+}$, let $S$ be a subgraph induced by vertices $p, \ldots, q$. Then $h_{pq} = |q-p+1|$ is the number of vertices in $S$ and $h_{pq} \choose 2$ is the maximum number of edges $S$ can possibly have. For this block, the expected block density would be:
  %
  %\[ %% to align the formular in column mode: &= blah blah \\ &= blah blah
  \begin{align*}
  d_{\textrm{\scriptsize avg}}(p, q) =
    \frac{w_{\textrm{avg}} P(e) {h_{pq} \choose 2} }{h_{pq}} =
    \frac{ s(p,q) } { \bar{z} }
          \frac{ \bar{z} }{ {n-1 \choose 2} }
          \frac{ {h_{pq} \choose 2} }{ h_{pq}}
    = \frac{ {h_{pq} \choose 2} s(p,q)}{ h_{pq} {n-1 \choose 2} }.
  \end{align*}
  %\]
  %
  The areas that have density higher than their $d_{\textrm{avg}}(p,q)$ represent groups of data items that have co-clustered together more often than is expected by chance. Hence, \Coral displays only these cores.

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%
  %% Subsection
  %%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Ordering in parallel partitions}

  When ordering clustering bands in the parallel partitions plot, we would like to put similar clusterings next to each other and avoid putting two dissimilar clusterings vertically adjacent. The intuition for such a constraint is that if the two clusterings $K_{i}$ and $K_{i+1}$ share many similarities, the bands connecting individual items between the clusterings will only cross a few times making it easier to track module changes. We also need to order items within the bands in a way that puts items from the same module next to each other and does not allow items from other modules to interleave.

  % ordering bands

  To find a vertical order for the clustering bands, we apply a greedy algorithm that uses clustering similarity scores. First, we compute the similarity for every pair of clusterings $\textrm{sim}(K_{i}, K_{j})$ using Jaccard. Next, we find the two most similar clusterings $K_{1}, K_{2}$, add them to a list, and look for a clustering most similar to either $K_{1}$ or $K_{2}$ (whichever is greater). We proceed by repeatedly picking the clustering that is most similar to the last clustering added to the list. The order in which clusterings were added to the list determines the order of the clustering bands.

  % ordering items in the bands

  We pursue two objectives when ordering items and modules within a single clustering band: items that belong to the same module must be placed next to each other, and the ordering has to be similar to the column ordering in the co-cluster matrix (so as to maintain the user's mental mapping between the two visualizations). To preserve the matrix ordering in clustering bands, each module is best placed in a position where most of its items are close to the matrix columns corresponding to those items. However, the order of the columns in the matrix may be such that two items $u$ and $v$ from the same module are far apart in $A^{+}$. We propose a heuristic to solve this ordering problem: given an ordering of the columns in the matrix $A^{+}$, for each module $m_{i}$ in clustering $K = \{m_{1}, \ldots, m_{k_{i}}\}$ we compute its rank based on how ``displaced'' items in the module are relative to the positions of the module's items in the matrix: 
  %
  \[
   d(m_{j}) = \sum_{u \in m_{j}} i(u),
  \]
  %
  where $i(u)$ is the index of a column in $A^{+}$ corresponding to the data item $u$. Modules that should be placed first in the clustering band would have the lowest rank, so we sort the modules in order of increasing $d$, and the module's position in the sorted array determines module's position in the clustering band.

\section{Results and discussion}

  \subsection{Multiple classifications data}
  \label{sec:data}

  \textit{Arabidopsis thaliana} is a model organism widely used in plant science, but out of its 35,000 predicted proteins one third still lack an assigned function~\cite{Kerrien2011}. A recent publication reports a new protein interaction network for \Athal that covers a part of the plant's proteome not studied previously~\cite{Vidal2011}. We have selected several clustering algorithms that are often used on PPI networks (Table 1) and, for each of the algorithms, we have generated a clustering of the largest connected component of the \Athal's network. To test the resulting modules for robustness, we compare this ensemble of clusterings to the modules reported by the authors of~\cite{Vidal2011} who used a link-clustering method by Ahn, Bagrow, and Lehman~\cite{Ahn2010}. Prior to comparison, we filtered the newly generated modules using the same criteria as~\cite{Vidal2011} by removing modules of size smaller than 6 and with partition density $< 0$. The new modules were tested for GO enrichment with FuncAssoc~\cite{Berriz2009} (see Table 1 for details).

  %== Table 1 ==
  \begin{table}[ht]
    \centering
    \begin{tabular*}\textwidth{@{\extracolsep\fill}l r r r@{\extracolsep\fill}}
    \toprule
    Algorithm & Proteins & Modules & Of them enriched for GO \\
    \midrule
    \texttt{Louvain}~\cite{Blondel2008}   & 2369 & 23     & 21\\
    \texttt{CFinder}~\cite{Adamcsek2006}  & 508    & 666   &   180 \\
    \texttt{Clauset}~\cite{Clauset2004}   & 2313  & 20    & 18\\
    \texttt{MCL}~\cite{VanDongen2000}     & 844 & 46  & 33    \\
    \texttt{MCODE}~\cite{Bader2003}       & 268 & 20 & 16\\
    \texttt{MCODE.F}~\cite{Bader2003}     & 1314 & 20     & 19 \\
    \texttt{MINE}~\cite{Rhrissorrakrai2011}   & 206 & 57  & 29    \\
    \texttt{SPICi}~\cite{Jiang2010}           & 259 & 46 & 27 \\
    \bottomrule
    \end{tabular*}
    \caption{\textbf{Clustering algorithms used on \textit{A. thaliana} network.} Algorithms were run with default parameters on the largest connected component of the \Athal PPI
    network. \texttt{MCODE}
    was run without ``haircut'' and no ``fluff,'' \texttt{MCODE.F} included
    ``fluff.'' The table reports the number of proteins that were assigned to at
    least one module, the number of modules after filtering according to procedure
    used in Vidal et al.~\cite{Vidal2011}, and the number of modules
    FuncAssoc~\cite{Berriz2009} reported as enriched for at least one GO annotation.}
    \label{table:clusterings_2vid}
  \end{table}

  % describe clustering algorithms

  For our comparison, we have focused on the graph clustering algorithms
  for which the implementations were available (see
  Table~\ref{table:clusterings_2vid}). \texttt{Louvain}~\cite{Blondel2008} and
  \texttt{Clauset}~\cite{Clauset2004} are two algorithms that search for a
  network partition with highest modularity~\cite{newman06}. Both tend to find
  large clusters and usually cover most of the nodes in a network.
  \texttt{CFinder}~\cite{Adamcsek2006} is a clique percolation method that
  identifies overlapping communities by continuously rolling cliques of an
  increasing size. Resulting clusterings usually contain many small modules with
  a high amount of overlap and cover only a part of the network ignoring
  graph structures like bridges and stars. \texttt{MCL}~\cite{VanDongen2000} is a
  fast, flow-based clustering algorithm that uses random walks to separate
  high-flow and low-flow parts of the graph. Its modules tend to be small and
  usually cover most of the input network. \texttt{MCODE}~\cite{Bader2003}
  algorithm finds modules in biological networks by expanding communities around
  vertices with high clustering coefficient. ``Fluff'' and ``haircut'' options for
  \texttt{MCODE} allow to add singleton nodes connected to the module
  by just one edge and to remove nodes weakly connected to the module
  correspondingly. \texttt{MINE}~\cite{Rhrissorrakrai2011} is closely related to
  \texttt{MCODE}, but uses a modified weighting scheme for vertices which results
  in denser, possibly overlapping modules.
  \texttt{SPICi}~\cite{Jiang2010} grows modules around vertices with high
  weighted degree by greedily adding vertices that increase module's density. The
  partitions contain many dense modules, but usually cover only a part of the
  network.

  \subsection{Applying \Coral to \Athal clusterings}

  % overview stats

  To get an overview of the data, we review various statistics on
  the generated clusterings. For the majority of the clusterings, modules
  that remained after filtering covered only a portion of the network. The two
  clusterings produced by the modularity-based methods, \texttt{Louvain} and
  \texttt{Clauset}, were the only clusterings that included more than 95\% of all
  proteins into their modules. The number of modules per clustering varied
  significantly from 20 to 666 (Table 1).
  The average module size was highest for \texttt{Clauset} (115.65),
  \texttt{Louvaine} (103.00), and the \texttt{MCODE.F} (82.05) clusterings
  significantly exceeding the average module size among all other clusterings
  (3.02-26.31 items per module). For the original 26 \Athal
  modules~\cite{Vidal2011}, 3\% of the proteins were assigned to more than one
  module; in the \texttt{CFinder} clustering over half of the clustered proteins
  (59\%) participated in multiple modules.

  % all-to-all clustering comparisons

  The nine \Athal clusterings are highly dissimilar: most cells in the ladder widget (Figure~\ref{fig:coral:ladder}) are white or pale blue, and the majority of pairwise Jaccard similarity scores are below 0.07. \texttt{MCL} yielded the partition most similar to \Athal modules reported in~\cite{Vidal2011} (\texttt{A.Thal original}) with Jaccard similarity of 0.60. Surprisingly, the 26 modules generated by link clustering~\cite{Vidal2011} shared very little similarity with \texttt{CFinder}, the only other algorithm in the ensemble designed to produce overlapping modules.

  % module-2-module table

  Low pairwise similarity scores between so many pairs of clusterings is easily explained using the module-to-module table: clusterings with Jaccard similarity below 0.07 overlap by a few small modules or no modules at all. The similarity of 0.60 between \texttt{MCL} and \texttt{A.Thal} (Figure~\ref{fig:coral:modules}) may be attributed to the two big modules that are largely replicated between the two clusterings: the module \texttt{m9} from \texttt{MCL} and the module \texttt{m0} from \Athal (highlighted row) overlap by 288 proteins with Jaccard similarity 0.8. Several smaller modules (shown at the top of the table) are duplicated exactly between the two clusterings.

  % co-cluster matrix

  The co-clustering matrix for \Athal clusterings contains several large regions of co-clustered proteins along the diagonal (Figure~\ref{fig:coral:big_matrix}), however, most cells are pale indicating that they were co-clustered by only a few clustering algorithms; very few matrix cells are close to the saturated red. Indeed, 65.25\% of all co-clustered pairs of \Athal proteins have co-clustered just once across all of the nine clusterings used in the analysis and only 6.34\% of protein pairs were co-clustered in 5 or more partitions. This low number of protein pairs that were assigned to the same cluster means that the clusterings in the ensemble mostly disagreed.

  % cores for A. thals

  The dynamic program for identifying cores found 249 areas in the \Athal network in which proteins co-clustered more often than could be expected by chance, with the largest core containing 215 proteins and with the average number of proteins per core of 10.38 proteins. Most cores, including the largest core, had low cohesion values indicating that the proteins forming the cores had many connections to proteins outside of the cores (see Figure~\ref{fig:coral:big_matrix}). This finding is correlated with the fact that the clusterings did not agree in general and only small sets of proteins were consistently clustered together across the ensemble.

  % base clustering

  Finally, setting \texttt{A.thal original} to be the base clustering shows that the modules found by~\cite{Vidal2011} covered only a fraction of modules found by other methods, although they included the largest core. The majority of \texttt{A.thal original} modules were colored pale pink  (Figure~\ref{fig:coral:base_clust}) indicating that modules found by the link clustering were found by no more than 3 other clustering methods. We trace the largest core in the parallel partitions plot (Figure~\ref{fig:coral:parsets}): the proteins in the core are co-clustered by \texttt{A.thal original}, \texttt{Clauset}, \texttt{Louvaine}, \texttt{MCL}, and \texttt{MCODE.F} while \texttt{SPICi}, \texttt{MINE}, and \texttt{MCODE} ignored the majority of core's proteins completely. \texttt{CFinder}, with its many overlapping modules of size 3, 4, and 5, modules some of the core's proteins and puts a large part of the core in the grab bag group representing unclustered proteins.

\section{Discussion and Conclusions}

  Clustering algorithms may generate wildly varying clusterings for the same data: algorithms optimize for different objectives, may use different tie breaking techniques, or only cluster part of the data items. A popular technique for optimizing modularity has been shown to suffer from a resolution limit~\cite{Fortunato2006} and multiple decompositions may have the same modularity value~\cite{Duggal2010}. When a true decomposition is available, the clustering quality can be quantified using the similarity score and the true positive and true negative rates. However, when there is no true clustering, it is hard to decide which clustering is better than the others. We propose that researchers generate several clusterings by either using different clustering algorithms or varying algorithm parameters. \Coral can help compare such clusterings and identify cores in which the data items co-cluster across multiple clusterings.

  Most views and statistics in \Coral work for both non-overlapping and overlapping clusterings. All overview statistics extrapolate well for overlapping modules except for entropy which assumes that no two modules overlap and therefore may overestimate the actual entropy. The co-cluster matrix naturally adapts to overlapping modules by allowing their corresponding blocks to overlap. Currently, if a pair of data items co-occur in more than one module within a single clustering, their co-cluster value is set to 1 and is not weighted higher relative to other pairs. The parallel partitions plot assumes that the modules in individual clusterings do not overlap. However, if there are overlapping modules, parallel partitions will still lay out the modules in a line by duplicating the co-occurring element in every module in which it occurs.

  Although the examples we use in this paper are based on network clustering, \Coral does not require its input data to be a network partition and can be used with equal success on gene expression or classification data. In particular, if users would like to compare several classification results, they can do so in the same manner as we have demonstrated for the \Athal clusterings. The similarity measures of purity, inverse purity, and the F-measure implemented in \Coral are helpful in comparing classifications to the available truth. The module-to-module table is a more flexible alternative to the confusion matrix that is often used to evaluate classification results.

  \Coral has been used to analyze clusterings of up to 4115 items. The runtime varies with the number of clusterings, number of modules and data items per clustering, and the size of the modules. The startup operations --- parsing the input clusterings, computing dataset statistics and all-to-all clustering similarities, as well as rendering the views --- take from under a second to 11 seconds for clusterings from 33 to 4115 data items. Matrix reordering is the single biggest performance bottleneck for \Coral. Reordering the co-cluster matrix for 2376 \Athal proteins took, on average, 29 seconds when to reorder using the greedy heuristic and 70 seconds when to reorder using the Hungarian algorithm. However, both the greedy heuristic and the Hungarian algorithm find good orderings after very few iterations and the reordering only needs to be computed once before analysis. Solutions for LAP computed with the Hungarian algorithm improve with every iteration and usually converge on a good reordering fast.

  \Coral offers a comprehensive array of visualizations that allow users to investigate modules from various viewpoints inlcuding several novel views. \Coral guides users from overview statistics implemented as familiar bar charts to detailed cluster-to-cluster comparison in a table. The ladder widget, a lower triangle of the comparison matrix, helps users pick the most similar (or dissimilar) pair of clusterings and to judge how similar clusterings in the dataset are overall. A color-coded co-cluster matrix shows how often any pair of items in the dataset have been placed in a module together. A novel adaptation of parallel coordinates, parallel partitions plot, makes tracking a group of items across clusterings easy with intuitive selection techniques. These views combined create a powerful tool for a comprehensive exploration of an ensemble of clusterings. \Coral can help users generate questions and hypotheses about the data that could be later definitively answered with the help of additional experiments.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% SHARQ -- search for SRA metadata
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Augmenting search for high-throughput sequencing metadata}

% XXX merge into Coral chapter

SHARQ is available at http://sharq.compbio.cs.cmu.edu/.

SHARQ~\cite{SHARQwebsite}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}

The Sequence Read Archive (SRA)~\cite{kodama2012sequence} is a central resource for high-throughput sequencing data produced by a variety of sequencing technologies. As part of an international effort to make data generated using public funds accessible to researchers and the rest of the world, SRA's goal is to maintain the data and make is easily searchable to aid in reproducibility efforts and allow for new discoveries. The database contains nearly 2000 terabases of open access data of which recent human RNA-seq expression amounts to about 10\%. RNA-seq is a way to gauge the expression of genes and individual isoforms and holds a lot of promise XXX as a fast and inexpensive genetic diagnostic tool XXX any citation?. Sample comparisons between and across tissue and cell types are essential for establishing the common baselines, however, the manual annotations of tissue and cell types for experiments submitted to the SRA are often inconsistent or completely missing. The variability of submissions to SRA and the lack of any consistent vocabulary for cell and tissue types results in searches through the existing tools (through web interface or database) to miss relevant experiments and to return false positives.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Annotation for tissue and cell types}

XXX

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Interface}

\begin{figure}[ht]
  \includegraphics[width=\linewidth]{sharq-ui.png}
  \caption{\textbf{Overview of SHARQ interface.} The interface offers a variety of controls to search by multiple tissues, cell types, read length, or date of submission. Text search (not shown) scans through the PubMed abstracts and other text fields describing each sequencing run.}
  \label{fig:sharq:interface}
\end{figure}

SHARQ is implemented as an lightweight web-based interactive tool using open source Javascript packages \texttt{crossfilter}~\cite{crossfilter} and \texttt{dc.js}~\cite{dcjs} (Figure~\ref{fig:sharq-ui}). It offers a way to quickly assess metadata for thousands of records and use visual cues to narrow the search down to a set of sequencing runs of interest. Users can start by selecting several tissue types, then refine the search by specifying  specific cell types or by restricting their search to runs that have a particular read length. Additionally, users can provide a date range that dictates the date of initial submission to SRA. As a special case, the user may choose to only focus on data associated with cancers or data associated with non-differentiated cells, since these samples may originate from any tissue. In addition to selecting parameters of interest, users may search across all text fields, such as study summaries, including tissue and cell type annotations. For example, searching for `blood' will bring select all sequencing runs that were annotated as coming from blood as all other runs that have a mention of `blood' in their annotations, title, abstracts, and summaries.\\

Once users are satisfied with their query, they can download all matching sequencing run IDs in a single file for further processing --- these could be used, for example, for a batch download through SRA Toolkit \cite{leinonen2010sequence}. Alternatively, users can download full records for the matched runs that include FTP links for direct download.\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion and conclusions}

XXX





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%
%% Map of jazz -- dynamic network vis
%%
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Visualizing dynamic networks with context}

Relationships between people, cooperation between genes in a cell, co-dependence of species within an ecosystem are just a few examples of interactions that are often modeled as a network, however, the fact that these networks are dynamic is often ignored. Dynamic networks may change in several ways: a gene that was not expressed in the previous time step may turn on; two proteins may form a complex under certain conditions (or two people become friends); the amount of a gene product may increase as a result of an activated pathway; the strength of a relationship between two people may grow over time. These changes are equivalent to node and edge gain and loss and directly affect network topology; the changes in the amount of a gene product or strength of a relationship are equivalent to property changes on the nodes and edges, but do not affect the graph structure itself.

The amount of information that changes between any two consequitive timepoints is often too great to comprehend even for small networks making visualization of such networks a challenging problem. We explore a setting in which the user focuses on a single node (e.g., a gene of interest) and its immediate neighborhood over time (ego-network). The main challenge is to maintain user's mental model of the ego-network between differnt time steps while allowing changes to propagate through the viusalization. We model the amount of ``influence'' shared between the main node and its peripheral connections at any given time and develop an algorithm that assigns unique positions to the peripheral nodes according to the amount of shared influence. We suggest novel widgets to inform the user of general patterns of interaction between the main node and its neighbors.

In this capter, we discuss a new system for exploring, in an intuitive and interactive way, a large compendium of collaboration data between jazz musicians. The system consists of an easy-to-use web application that marries an ego-network view of collaborations with an interactive timeline.  We develop a new measure of collaboration strength that is used to highlight strong and weak collaborations in the network view. The ego-network is arranged using a novel algorithm for ordering nodes that avoids occlusion even when the network is frequently changing. We build the system around a large, unique, hand-curated data set of recorded jazz collaborations.

The work discussed in this chapter was presented at SocialCom 2012~\cite{MapOfJazz}. The system can be accessed at \url{http://mapofjazz.com/socialcom}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}

  Social networks such as those between collaborators or friends are an object of intense study. Often, such connections are assumed to be immutable and the networks are considered static. However, there are many social interactions that violate this assumption: work colleagues, neighbors, acquaintances, friends, and family may all change over the course of a person's lifetime. This is particularly true of jazz collaborations where band members frequently come together for only a single recording session, and where some musicians have played with over a thousand other artists over their decades-long careers. Understanding changes in connectivity on a scale of the whole network can provide an insight about the global change within the network, but has proven to be a difficult task for both algorithmic~\cite{Hopcroft2004, Palla2005c, Tantipathananandh2007, TangLiuZhna08} and visualization~\cite{BenderDeMoll2006, Rosvall2010,Yi2010} approaches. Changes may affect the profile of an individual node in a significant way, but these observations are lost in the sea of data when analyzing the network as a whole. The dynamism of jazz collaborations requires new approaches to visualize frequently changing networks.

  Apart from the topological changes, there may be other, more subtle variations in the characteristics of relationships over time.  Node or edge attributes may change, e.g.\@ the relationship between $A$ and $B$ may gradually change from an acquaintance to a close collaboration, or the ``importance'' of $A$'s immediate collaborators may grow, thus indirectly increasing the importance of $A$ itself.  Collectively, such changes may affect the profile of an individual in a significant way, but these observations are lost in the sea of data when analyzing the network as a whole. This, along with a traditional focus on the lives of individual musicians, leads to the desire to have a visualization that can be focused on subregions of the entire space of collaborations. It also leads to the need for techniques to quantify the strength of the relationships encoded in an evolving network and to show this information effectively through a visualization.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Jazz collaboration networks}

  The evolving community of jazz musicians is an example of a social network where personal connections are essential~\cite{Pinheiro2009}. In jazz, a highly collaborative art form, one person's individual style is shaped by constant experimentation and exchange of techniques and ideas with fellow musicians~\cite{Berliner1994}.  Every musician is part of a dense network of collaborators with many transient connections: a composer may arrange music for multiple bands simultaneously, band leaders may recruit new members to their bands and lose them to competition, or musicians' skills may improve to the point where they are featured as soloists and have a prominent place in the band. Study of recorded jazz collaborations can help identify influences on style, explain career success, and lead to a richer understanding of the progression of the jazz art form.

  The traditional means by which these collaborations are explored is via the compilation and study of discographies presented as lists and tables in either hard-copy books or in computerized databases~\cite{Timner2007,Albin}. These discographies list recording sessions, the roles each musician played in them, often songs and albums that were produced as a result, along with other information. They provide an extremely rich source of information from which to trace the collaborations of musicians. However, such a static and textual presentation is difficult to comb through and does not easily allow the user to comprehend the dynamics of changing collaborations. While changes in band membership are easily traceable, it is hard to assess the overall contribution of a band member over time unless the historians are intimately familiar with the band's history. A system for exploring jazz collaborations that makes large discography data more approachable is needed.

  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Dynamic network visualization}

  There are two main approaches to visualizing time-varying networks: to show animations by constantly recomputing layouts at every time step~\cite{Yee2001} and to compare static snapshots of a network at several distinct time points. For either approach, the objective is to highlight the differences between the network views at different time steps.

  Brandes and Corman~\cite{Brandes2003a} stack network snapshots on top of each other in 3D where the nodes with the same labels are connected by vertical columns. The graph is laid out using a spring-embedded algorithm, and each slice shows only the nodes and edges present at that time point. Yim, Shaw, and Bartram~\cite{Yim2009} show a snapshot of a network in its early state and indicate changes in node positions with red arrows leading to complicated and cluttered displays. Diehl and G\"{o}rg~\cite{Diehl2002} place the two network snapshots side by side and propose three algorithms that minimize dissimilarities between the two representations. A recent study by Khurana et al.~\cite{Khurana2011} presents an extension to NodeXL~\cite{Bonsignore2009} that aggregates the snapshots of a network at two different time points into a single view. The edges are colored based on the time interval at which they existed. Additionally, NodeXL plots the values of several common graph properties such as node and edge counts as they change over time between the two time points. An approach by Yi et al.~\cite{Yi2010} combines a small multiples display (a histogram showing node degree over time) and a matrix representation of a network into a single view.

  Graph layout for the animations may be fixed or constantly updating at each time step. Moody et al.~\cite{Moody2005} keep the node positions fixed and allow the edges to appear and disappear as the time progresses in their \emph{flip-book} animations. Yee and colleagues~\cite{Yee2001} allow the nodes to move between the concentric circles along a smooth tangential trajectory.

  While these approaches are able to highlight the topological changes, they all make an assumption that node and edge attributes (such as edge length) are either absent or remain static over time.

  % related work: circular layouts

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Circular and ego-network layouts}

  As early as 1990, circular layouts were used for organizing trees by placing the root in the center and assigning the child nodes to concentric circles with increasing radii~\cite{North1997}, with nodes at the same level of a tree assigned to the same circle. Six et al.~\cite{Six1999} extended the technique to work for more general graphs by connecting multiple circular structures. Yee and colleagues~\cite{Yee2001} have developed the technique further to support nodes of different sizes (where node size is proportional to some node attribute). They adapted their layout to handle dynamic graphs by showing animations of nodes traveling on a smooth trajectory from old locations to the new ones.

  Wang, Shi, and Wen~\cite{Wang2011a} experiment with a dynamic ego-network design where the central node and all of its connections are shown at the same time. The main node has several copies with each one representing the node at a specific point in time and linked only to those nodes with which it was associated during that period. This approach is not feasible if the central node has many collaborators over his or her career. Gansner and Koren~\cite{Gansner2006} develop heuristics that order nodes on a circle's periphery in a way that minimizes the edge crossovers and reroutes some of the links to go outside the circle's circumference. These authors suggest edge bundling for the links inside the circle to reduce clutter further. Their work does not consider dynamically changing links.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % related work: collaboration networks
  \subsection{Artist collaboration networks}

  Artist collaboration networks have received their fair share of attention in social network analysis. The data on interactions among misicians is available in some databases~\cite{Gleiser2003,Pattuelli2012}, has been collected through surveys~\cite{Heckathorn2001a}, or assembled manually by processing the tapes of interviews with the artists~\cite{Pattuelli2011}. Due to difficulties in data collection, these sources cover only a few artists and lack temporal information about their collaborations. For example, Gleiser and colleagues~\cite{Gleiser2003} base their analysis on 198 bands that were active in 1912-1940. Heckathorn and Jeffri's survey reached out to 110 musicians in New Orleans, 264 in New York, and 300 in San Franscisco~\cite{Heckathorn2001a}, a small fraction of the estimated 33,000 jazz musicians living in New York.

  Examples of applications supporting exploration of such networks are few. An online Classical Music Navigator~\cite{Smith1999} helps users expand their musical interests by suggesting composers who influenced or were influenced by a composer the users initially searched for. The Navigator offers a simple text and link interface. A visualization of Last.fm data~\cite{Bieh-Zimmert2011} allows one to compare two artists and their musical associations, but does not provide any intuition about collaborations between the two artists.

  In this paper, we propose a way to quantify the strength of collaboration between two actors in the network based on the frequency and timing of the events in which they have both participated. We provide a visualization system that displays much of the data available in large discographies. In this network, nodes represent artists and edges connect artists who participated in the same recording session at some time. We visualize creative partnerships between musicians using an interactive egocentric network view that allows users to focus on an individual and observe large and small scale changes in collaborations (Fig.~\ref{fig:moj:overview}). We couple this network view with an interactive timeline that allows the user to see how the strength of ties changed over time. We also introduce a novel algorithm that arranges collaborator nodes around the central musician in a way that minimizes node occlusion and variation in node positions as the network view changes over time. This helps the users to maintain their mental map of evolving collaborations. We demonstrate the utility of this approach on an extensive hand-curated collection of jazz collaborations spanning almost a hundred years.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dynamic network data}

  % Describe the process of entering, cleaning, improving, expanding,
  % cross-referencing.

  The Map of Jazz uses data that have been collected over the course of more than twenty years of discographical research, with additional content subsequently added in targeted batches. The data come from a myriad of sources: from general and artist discographies to specialist journals, magazines, and newsletters to biographical and historical monographic literature and many more. In almost every case, a single entry is a collection of data from multiple sources because no one source covers every aspect of the recording session. The sessions cover a period of time from the early 1920s to the present day.

  While there have been other attempts at digitizing discographical information, these used closed, proprietary systems that lacked both the ability to export and import or edit the information, and ultimately were unable to satisfy the information needs of serious researchers. The data used by the Map of Jazz were collected and stored using the open-source discographical software BRIAN~\cite{Albin}, named so for the English discographer Brian Rust who perfected the session-based format for print discographies~\cite{Rust1980,Rust2002}. BRIAN's support for data export and import allowed for multiple users to contribute to the project and amounted to a large number of cataloged recording sessions. The Map of Jazz is the first project to use large amounts of BRIAN data outside of the application itself.

  At the heart of BRIAN is the conceptual idea that the session is the primary entity, unlike many other databases designed to store sound recordings information. Sessions are events that have defined locations, both chronologically and geographically. Out of the many layers of details available in BRIAN, the Map of Jazz uses the top-level data on the sessions and musicians who performed during them, thus shifting the focus to the interpersonal relationships between the artists. The attribute for the main musical instrument helps to distinguish Bill Evans the saxophonist from Bill Evans the pianist; it also records what instrument each performer played.
  %
  % Top 5 guys by the number of collaborators
  %
  \begin{table}
    \centering
    \begin{tabular}{l  r}
    \toprule
    Name & Degree \\
    \midrule
    Slide Hampton & 1230\\
    Kenny Barron & 1090\\
    Ron Carter & 785\\
    Michael P. Mossman & 692\\
    Freddie Hubbard & 691\\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Top 5 musicians with the highest number of collaborations.} Slide
    Hampton was a prolific composer who has provided arrangements for multiple
    bands, hence his interaction surpasses that of many famous band leaders.}
    \label{tab:high_degree}
  \end{table}
  %

  % Data statistics.

  \begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{full_network_cropped.png}
    \caption{\textbf{Map of Jazz network of collaborations} visualized with Cytoscape~\cite{Cytoscape}. Individual nodes represent jazz musicians, node size corresponds to node degree, and the opacity of the edges between nodes corresponds to the frequency of collaboration over time.}
    \label{fig:moj:fullnetwork}
  \end{figure}



  At the time of publication, the database contained information on 11824 musicians and a total of 13873 recording sessions (see Figure~\ref{fig:moj:fullnetowrk} for an overview of the collaboration graph). The average number of people per recording session was 7.39 with the smallest sessions having just one performer, and the largest session having 72 people (not including the members of symphony orchestras). Network diameter (the longest among all shortest paths between all pairs of vertices) was equal to 8, and the average (\emph{characteristic}) shortest path was equal to 3.24. As with many social networks, very few musicians have a high number of connections, with an overall average node degree of 33.68 (see Fig.~\ref{tab:high_degree} for the top 5 artists by degree). However, the network does not pass the test for scale-freeness according to the test developed by Clauset et al.~\cite{Clauset2009b} suggesting that the network has evolved by processes other than the well-studied ``rich get richer"mechanism.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Interface design}


  The Map of Jazz focuses on the dynamics within the collaboration networks of individual musicians. An egocentric network layout (Fig.~\ref{fig:moj:overview}) is coupled with an interactive timeline that allows users to navigate through various time periods and observe gradual changes in the person's collaboration network. The timeline is augmented with aggregate statistics, such as the number of people per sessions, to aid in navigation through time. In the network view, nodes representing collaborators are arranged around the central node in a manner that preserves nodes' relative positions across time and shows the relative strength of a tie between the main musician and his or her collaborator. Nodes and edges used in the traditional node-link network representation are extended to display the change in attribute values over time. Finally, the interactions between the currently displayed main musician's collaborators can be explored on demand by highlighting a node of interest which reveals its connections to other nodes in the neighborhood. The central artist can be selected by double-clicking on a performer's node or by entering a musician's name in a search box. To help users pick a starting point, the Map of Jazz offers a dropdown with 21 hand-picked sessions that stand out in jazz history.


  \begin{figure}[tb!]
    \centering
    \includegraphics[width=\linewidth]{moj_overview.png}
    \caption{\textbf{Overview of the Map of Jazz web application.} XXX more details}
    \label{fig:moj:overview}
  \end{figure}


  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %
  % Visual: timeline
  %
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsection{Timeline}

  Users may navigate the extensive Map of Jazz timeline by dragging it with a
  mouse. They may also zoom in on a particular period of the musician's life or
  zoom out to get an overview of the artist's career. Every time the users
  interact with the timeline, the ego-network is updated to present an accurate
  snapshot for the selected time period.

  \textbf{Sessions.} The triangles on the timeline represent recording sessions (Figure~\ref{fig:moj:timeline}). When users hover over the triangle with a mouse, the session and the collaborators who recorded for that session are highlighted in yellow. Users may click on a session to select all the participating collaborators and the session itself, and vice versa. A tooltip that appears above the triangle icon summarizes the information about the session: the date, location, and a list of participating musicians along with their primary skill (an instrument they played, e.g.~alto saxophone, or a role they took, e.g.~band leader, during a session).

  When the date of birth and/or death are available, the span of time from birth to death (or to the present day) is colored in a lighter shade of gray to indicate the span of the musician's life.


  %
  \begin{figure}[ht]
    \includegraphics[width=\linewidth]{timeline}
    \caption{\textbf{A timeline augmented with a session similarity graph.} Triangles represent the individual recording sessions. Most sessions on the timeline have a high pairwise similarity indicating that session memberships changed only slightly.}
    \label{fig:moj:timeline}
  \end{figure}
  %

  
  \textbf{Augmented timeline.} To aid users in focusing on a specific time period of interest, the Map of Jazz offers several basic metrics to be overlaid on top of the timeline. These include: 
  %
  \begin{itemize}
    \item the number of collaborators per session,
    \item the number of unique musicians the person collaborated with up to this date,
    \item the Jaccard similarity between the members of the current session and the previous session.
  \end{itemize}
  %
  The number of collaborators and the speed at which a person attracts new collaborators have been shown to be significant in scientific~\cite{Petersen2012} and jazz~\cite{Pinheiro2009} collaborations, while the session size may explain the mechanism of acquiring new collaborators (i.e. switching between bands versus playing with the same band).


  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %
  % Implementation: influence function
  %
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsection{Measure of collaboration strength}

  The notion of mutual influence between any two musicians lies at the heart of the Map of Jazz. To quantify the strength of the relationship between any pair of musicians, we make an assumption that the recording sessions are not spontaneous events, but rather a result of previous undocumented collaboration (e.g. concerts and rehearsals). Consequently, the professional relationship between the musicians is not likely to start right before a session nor to stop immediately after recording it, but rather to grow before the session and wane gradually over time. The collaboration is strongest around the time of a recording session and is increased even more if the musicians record several sessions over a short period of time. With these assumptions in mind, we define a function of \emph{collaboration strength} that takes into account the frequency and proximity in time of the collaborations between the two musicians:
  %
  \begin{equation}\label{eqn:influence}
  %
  g(t; S, \sigma) = \alpha_\sigma \sum_{s \in S} e^{-\beta_\sigma(t_s-t)^2 },
  %
  \end{equation}
  %
  where $S$ is the set of all sessions the two musicians shared, $t$ is a point in time for which we want to evaluate the collaboration strength (i.e.\@ the center of the timeline), and $t_s$ is the date for a specific session $s$. To make the decay of the collaboration strength smooth, we model it as a normal distribution with $\alpha_\sigma = 4/(\sigma^2\sqrt{2\pi})$ and $\beta_\sigma = 1/(2\sigma^2)$. %
  The function~\eqref{eqn:influence} is similar to a kernel density estimator for normally distributed values, where choosing the bandwidth for the kernel is a known hard problem. To make the function smooth, we take an affine combination of two functions:
  %
  \begin{equation}\label{eqn:influence2}
  %
  f(t; S) = \delta g(t; S, \sigma_1) + (1-\delta)g(t; S, \sigma_2),
  %
  \end{equation}
  %
  with $\sigma_1 = 1$, $\sigma_2 = 3$, and $\delta=0.9$. The resulting function assigns more impact to the interactions that were closer to $t$ in time rather than assign an equal weight to all interactions no matter how long ago, or how far in the future, they occurred (Figure~\ref{fig:moj:infl_func}).

  %TODO: Unlike the function used in~\cite{Palla2005c}), we take into account both
  %the past and the future collaboration events.

  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{influence_plot.png}
    \caption{\textbf{The collaboration strength function} (red) is a combination of two functions: one for which  the value grows fast as $t$ gets closer to the session (blue) and another one for which the change in value is much smoother (green). Here, two hypothetical musicians played together in 1925, 1930, and 1932 (blue dots). Their relationship is strongest when several sessions occur within a short period of time such as in 1930--1932. The collaboration strength drops slightly between 1925 and 1930 and declines rapidly after the last session in 1932.}
    \label{fig:moj:infl_func}
  \end{figure}


  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %
  % Visual design: egonetwork
  %
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Egocentric network view}

  An egocentric view allows the user to track the fluctuating interactions between a given musician and his or her peers over time. Using the timeline, users may select a time range of interest and focus on the collaborations between the main musician and his or her collaborators within that period. Only the collaborators that were involved in sessions that occurred during that time range will be shown. For every collaborator $v$, we compute the collaboration strength between the center musician $u$ and $v$ where $S$, the set of sessions considered in the collaboration strength function, equals the sessions that are currently visible on the timeline. The length of an edge connecting $u$ and $v$ is inversely proportional to the strength of the tie shared by the two artists, as measured by the collaboration strength function~\eqref{eqn:influence}. This causes the collaborators who record with the main musician $u$ often or have recorded with him or her recently to be placed closer to the center, while musicians who record with $u$ rarely or have only recorded with $u$ a long time ago are placed on the periphery (Fig.~\ref{fig:moj:overview}).

  As users drag the timeline, the collaborators' nodes may move closer to the center or may drift away to the periphery as the set of sessions that is displayed changes. Nodes disappear from view when they are no longer involved in any session that is visible on the timeline, and new nodes appear when sessions containing them come into view on the timeline. To minimize the cognitive load on the users, the Map of Jazz preserves the angular positions for every collaborator node within the ego-network formed by a particular main musician. For every musician $u$, we assign a unique angular sector to every collaborator $v$ who has ever recorded with $u$ (Fig.~\ref{fig:moj:ordering}). Their trajectory, a straight line connected to the center, can be easily traced as collaborator nodes move to or from the center.

  If care is not taken with the assignments of nodes to angular sectors, collaborators may crowd near the center if the main musician consistently played with the same set of people (i.e.\@ their band). To avoid this, we propose an ordering algorithm in Section~\ref{sec:moj:ordering} to arrange the collaborator nodes in a way that assigns dissimilar angles to collaborators with which the center node has had similar patterns of collaboration.

  %TODO: highlight nodes that just showed up OR have a session with the main guy
  %withing $\epsilon$ distance of $t_{center}$

  %[XXX (move to ego-net) Users control the animation between two different states
  %by zooming in and out of the timeline and by dragging the timeline left or
  %right. These actions change the $t_c$ time thus causing layout to update.]

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %
  % Implementation: ordering collaborators
  %
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsection{Ordering collaborators in a circle}
  \label{sec:moj:ordering}

  % TODO: Test how heterogeneous/homogeneous interaction are over time for most
  %people? High degree ppl? Low degree ppl?

  The Map of Jazz maintains the same angular node positions across every time
  period, allowing easier comparison of the network at different time points. As
  users navigate the timeline, new nodes may show up on the map, but they will
  never change the angular position of the nodes already on screen, thus
  preserving the users' mental map of the ego-network.

  The permanent angular node positions also help alleviate another problem common
  to graph drawing: node occlusion. More often than not, jazz musicians have a
  band, or several bands, with which they play and record on a regular basis. In
  this case, every person in the band would share a strong tie with the musician
  in question. The Map's circular layout would try to place all such frequent
  collaborators near the center, causing the nodes and labels to occlude each
  other.

  To minimize such occlusions, we identify groups of people that are likely to be
  at the same distance from the central node at the same time. To find the groups,
  we first sample the collaboration strength function values between the main
  musician $u$ and each one of its collaborators $v$:
  %
  \begin{equation}\label{eqn:sample}
  %
    x_{u, v} = (f(t_1;S), f(t_2;S), \ldots, f(t_{1000};S)),
  %
  \end{equation}
  %
  where $t_i$ are equally spaced time points in the range $[t_\textrm{min}, t_\textrm{max}]$ with $t_\textrm{min}$ equal to the date of $u$'s earliest session and $t_\textrm{max}$ equal to the date of $u$'s last recording session. In~\eqref{eqn:sample}, $S$ is the set of all the sessions in which the central node $u$ participated. Next, we construct a pairwise similarity matrix for all $u$'s, $M_u = \left(m^u_{i,j}\right)$, where $m^u_{i, j}$ is the cosine similarity between vectors $x_{u,v_i}$ and $x_{u,v_j}$. To make it easier for the clustering algorithm to find groups in this matrix, we set every $m^u_{i,j} < 0.9$ to $0$. The resulting sparse matrix $M'_u$ is taken as a weighted adjacency matrix of a graph $G_u$. We run the Louvaine clustering algorithm~\cite{Blondel2008}, which attempts to find clusters maximizing modularity~\cite{newman06}, on $G_u$ to identify groups of musicians for whom the $x_{u, v_i}$ vectors were similar.

  Performers in the same cluster interact with the main performer in a similar fashion. To spread them out, we assign them sectors of the circle that are far from each other.
  %
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{ordering}
    \caption{\textbf{Assigning angular positions for the nodes.} Starting with the largest cluster, performers from the same cluster get assigned sectors of the circle that are far from each other. Here, \emph{a} is assigned to the $1^\textrm{st}$ sector, \emph{b} is $\left\lfloor \frac{12}{5} \right\rfloor = 2$ sectors away, and so on.}
    \label{fig:moj:ordering}
  \end{figure}
  %
  To assign all $N$ collaborators to their angular positions, we iterate through the clusters in order of decreasing size.  For each cluster $C$, we assign a node in $C$ to the first empty sector and continue assigning $v \in C$ to empty sectors evenly around the circle at intervals of $\left\lfloor\frac{N}{|C|}\right\rfloor$ sectors.  If at any time the target sector is not empty, we search linearly clockwise for the next available sector and continue assigning the remaining nodes in $C$ starting from that sector (Fig.~\ref{fig:moj:ordering}). This heuristic --- similar to linear reprobing in a hash table --- attempts to ensure that the nodes belonging to the same cluster are spread out around the circle at equal intervals.

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %
  % Visual: Node glyphs
  %
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsection{Node and edge glyphs}

  A static snapshot of a collaboration ego-network may be misleading due to the fact
  that the past and future collaborations between the central and peripheral nodes
  are not visible. A collaboration that flourished previously may be represented
  by a single recording session in the selected time period and could be
  indistinguishable from many sporadic one-time collaborations. Likewise, an
  ego-network does not provide enough detail about the level of activity for the
  nodes other than the central node based on the edge length alone. Questions such
  as: did the musicians represented by these nodes record often? How many sessions
  did they record overall within this time period? Will they continue recording
  actively? To answer these kinds of questions, we replace the standard graph
  nodes and links with more information-dense glyphs.

  We use node glyphs to represent the collaborator's activity overall
  (i.e. sessions recorded, not necessarily with the main musician), and reserve
  the edge glyphs to represent collaborations between the main musician and the
  collaborator. The node glyphs, therefore, encode the musician's overall artistic
  output while the edges connecting it to the main musician quantify the strength
  of the tie between them.

  \begin{figure}[ht]
    \centering
    \includegraphics[height=100px]{node-glyph}
    \caption{\textbf{Node glyph.} The inner-most circle of the node glyph encodes the number of times the musician played in \emph{some} session in the past. The middle circle (blue) represents the number of sessions the musician played with the main musician within the current time frame. The radius of the outer largest circle encodes the total number of sessions the musician played throughout his/her career with or without the main musician.}
    \label{fig:moj:node_glyphs}
  \end{figure}

  Node glyphs (Fig.~\ref{fig:node_glyphs}) consist of three concentric circles.
  The inner circle's radius is proportional to the square root of the count of
  sessions this musician played in the past, i.e. before the sessions currently
  visible on the timeline. The second circle's radius represents the square root
  of  the number of sessions from both the past and currently visible on the
  timeline in which that collaborator recorded. The radius of the outer circle is
  proportional to the square root of the total number of sessions the musician has
  recorded throughout their life. As users navigate the timeline in the direction
  of the future, more sessions would transfer to the ``past'' circle, increasing
  the radius of the inner-most circle. The radius of the circle representing the
  ``past+present'' may fluctuate depending on the number of sessions currently
  visible on the timeline. The radius of the outer circle, i.e. the square root of
  a total number of sessions recorded, does not change.

  Edge glyphs encode information about the overall count of recording sessions in
  which both the main musician and the collaborator participated
  (Fig.~\ref{fig:edge_glyphs} and~\ref{fig:duke-ell}). Edges are colored in varying shades of gray with
  the inner edge representing collaborations in the ``past'' and its thickness
  proportional to the square root of the number of sessions he or she shared with
  the main musician in the past. The thickness of the middle part is proportional
  to the root of the number of sessions for that collaborator currently visible on
  the timeline. Finally, the total edge thickness represents the number of
  sessions the two musicians played together overall.
   As users interact with the timeline, the thickness of the
  inner edges may change depending on the number of shared sessions in the past or
  currently visible on the timeline; however, the total thickness of an edge would
  not change.

  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{edge-glyph}
    \caption{\textbf{Edge glyph.} The thickness of the inner-most inset in the edge glyph
  is proportional to the number of sessions the main musician and their
  collaborator played in the past, and the thickness of the "present" inset
  corresponds to the count of sessions in the past and present. The total edge
  thickness represents the number of sessions the two musicians played together
  overall.}
    \label{fig:moj:edge_glyphs}
  \end{figure}

  Various combinations of node and edge thicknesses should alert the users to
  different modes of collaboration. A combination of a large peripheral node and a
  narrow edge connecting it to the center indicates that while the collaborator
  has played many sessions overall, they played very few with the main musician
  (for example, Don Redman in Fig.~\ref{fig:moj:duke-ell}).
  Further, if the node's inner ``past'' circle is large, such combination then
  indicates that the collaborator is an established musician who is sharing their
  skills with the up and coming musician at the center of the ego-network. On the
  contrary, if the inner ``past'' circle is small and the majority of sessions are
  in the ``future'', such behavior may indicate that the collaborator has joined
  the main musician briefly and later went on to form an active career of their
  own. A case where both the node and the edge are thick indicates that the two
  musicians have recorded many sessions together and, therefore, share a strong
  tie.

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %
  % Visual: hops
  %
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \subsection{Exploring collaborators' connectivity}

  An ego-network allows users to focus on the dynamics of a few collaborations at
  the expense of hiding all other topological information. Knowing which, if any,
  peripheral nodes collaborate with each other helps to understand the communities
  that form in the immediate neighborhood of the main musician. The Map of Jazz
  provides such details on demand: when users hover over a collaborator's node $v$,
  the Map renders additional edges that connect $v$ to the collaborators it shares
  with the central node (Fig.~\ref{fig:young}). The thickness of the edges
  corresponds to the number of
  sessions the two musicians played in the past (including those they played
  with the main musician). When the edges connecting the main node and the
  performers on the periphery are thin indicating few collaborations, it would be
  noteworthy to see thick edges between those performers. Such a situation would
  imply that those artists form a strong community, or a band, outside of the main
  musician's neighborhood.

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %
  % User study
  %
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  %\section{Evaluation}
\section{Example Analysis with the Map of Jazz}

  \textbf{Duke Ellington}'s career spanned more than half a century from the early twenties to his death in 1974. The first record on the Map's timeline dates back to July of 1923 with Ellington playing piano and Elmer Snowden as the band leader --- Ellington was yet to form his own band. Ellington's egonet for the period of 1923--1928 has several prominent performer nodes: Harry Carney, Otto Hardwick, Sonny Greer, Fred Guy,  Barney Bigard, Joe Nanton, and Wellman Braud (see Fig.~\ref{fig:duke-ell}). The size of Harry Carney's node, for example, indicates that he participated in a significant number of recording sessions throughout his career (1487 sessions), and the thickness of the edge connecting him to Duke Ellington reveals that most of those sessions were recorded with Ellington (1480 sessions). The same holds for others in the list above: their careers were tightly knit with Ellington's.


  \begin{figure}[tb!]
    \centering
    \includegraphics[width=0.7\linewidth]{duke-ellington-1925-1928}
    \caption{\textbf{Ego network.} Duke Ellington's ego-network during 1925--1928. His prominent collaborators, Otto Hardwick, Fred Guy, Sonny Greer, Johnny Hodges, and Harry Carney, can be easily identified by the thick edges.}
    \label{fig:moj:duke-ell}
  \end{figure}

  % overview of timeline

  From the details window, it is clear that Duke Ellington had a very productive career: over the course of his life he participated in more than 1740 sessions with 600 musicians. If the user zooms out on the timeline, it becomes evident how densely packed Ellington's recording sessions were, with the last right before his death. The pairwise session similarities that are visible on the timeline and the average pairwise similarity of 0.60 suggest that Ellington played with a core group of close collaborators who would replace one another over the years, but the band membership never changed all at once. Switching to the graph of session sizes, users can see that the average number of people per recording session was 14.19 (a size characteristic of big band ensembles) with the largest session at 29 performers recorded in January 1968.

  
  % different bands

  Among Ellington's closest collaborators, several stopped collaborating with him either permanently or for a significant period of time. Ellington and Greer recorded 588 sessions together from 1923 to 1951, but there are no sessions past that: their collaboration ended after Greer's propensity for drinking forced Ellington to hire a second drummer to replace Greer. Apart from Greer, Johnny Hodges and Lawrence Brown, two of his most prominent collaborators, have a gap in recording sessions starting in 1951 which correlates with both musicians leaving the band to pursue their ambitions elsewhere. The recording sessions that included Hodges restart 5 years later and span all the way until his death in 1970; Brown rejoined the band later in 1960 to record 432 more sessions with Ellington.  The closely coupled timeline and ego-network displays allow such events to be found with relative ease.

  \textbf{Slide Hampton} has the most collaborators (1230) among all musicians represented in the Map of Jazz. He has composed and arranged music for many prominent musicians such as Kenny Barron, Chick Corea, Tommy Flanagan, Dizzy Gillespie, Clark Terry --- their large nodes stand out in Hampton's egonet --- as well as hundreds of lesser-known performers. Dragging the timeline across the length of his career shows that while at any given time period Hampton is connected to many artists, he rarely collaborated with them for prolonged periods of time: the collaborators' nodes do not move close to Hampton's central node, but rather stay at the periphery. The difference in collaboration style between Ellington and Hampton is especially pronounced when one compares their average sessions similarity (visible on the timeline): Hampton's average session similarity is at 0.19 compared to 0.60 for Duke Ellington. The average session size for Hampton is 12.09 --- combined with the low session similarity, we can conclude that Hampton accumulated the highest number of total collaborations by continuously seeking out new collaborators.

  \textbf{Count Basie}'s sessions account for 159 sessions available in the Map of Jazz, and his number of collaborators (253) seems modest when compared to Slide Hampton or Duke Ellington (see Figure~\ref{fig:moj:overview}). The first session available in the Map of Jazz dates back to 1936, when Basie acted as a band leader and pianist and \textbf{Lester Young} was on tenor sax (the roles of each musician in each session are available by mousing over the session in the timeline). The large size of Young's node in the ego-network foretells his successful career --- indeed, later he became one of the most influential saxophone players.

  By double-clicking on Young's node, we can start navigating through his personal timeline. Young recorded with Basie often in the 1930s and 1940s. The records are sparse for the period of 1941--1945: first due to an American Federation of Musicians' recording ban that stopped all commercial recording in 1942--1944, then due to Young's being drafted into the army and serving a year-long jail term after being dishonorably discharged from service. Again, this gap in jazz productivity is clear from the display of sessions in the timeline.  Among his later collaborators are Billie Holiday, Charlie Parker, Buck Clayton, and Coleman Hawkins with whom Young recorded several times in 1946. The ``hops'' edges reveal that Parker, Clayton, and Hawkins had numerous sessions that did not include Young (see Figure~\ref{fig:moj:young}).

  \begin{figure}[htb!]
    \centering
    \includegraphics[width=0.8\linewidth]{lester-young}
    \caption{\textbf{Hops reveal ego network connectivity.} Additional "hops" edges show past collaborations between periphery nodes that did not include the main musician. Here, Buck Clayton had recorded with Rodney Richardson and Al Killian numerous times prior to working with Lester Young.}
    \label{fig:moj:young}
  \end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion and Conclusions}

  The Map of Jazz approaches the problem of dynamic graph visualization from a new angle: instead of tackling the hard problem of visualizing large graphs and tracking temporal changes on a global scale, the Map focuses on individual nodes and local changes that would have an immediate and personal effect. The Map of Jazz arranges related nodes into an ego-network with a single individual in the center surrounded by close neighbors who are placed according to the strength of their connection with the central node. Users can explore the dynamic properties of the network by dragging the time slider or zooming in on a particular era of interest. The ego-network adjusts the node positions according to the varying strength of their connection to the central node during that time. The appearance of nodes and edges updates as well to reflect the change in their attributes.

  We tailor our visualization to assist the exploration of a novel jazz collaboration network. In social interactions, the frequency and recency of interactions determine the strength of the collaboration between individuals. We propose and implement a collaboration strength function that takes into account both past and future interactions and helps quantify the strength of the relationship between the two musicians at any point in time.

  The concepts developed for the Map of Jazz can be applied to other social networks that record multiple interactions between individuals and to dynamic networks in general, especially those where the numerical attributes on nodes and edges change over time. One such example is the gene co-expression network where genes control the expression of other genes in the cell. The amount of one gene product may change over the natural cycle of a cell (cell division, growth, death) and affect the behavior of related genes. Applications to other collaboration networks such as co-authorship data are straightforward.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%
%%
%% Part 2 -- high-level structure and compression
%%
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Inferring structure and its use in data compression}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Domains -- inferring high-level structure from DNA conformation data
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% algorithms for managing biological data
\chapter{Identifying high-level structure in DNA conformation data}


Recent chromatin conformation capture (3C) experiments provide indirect measurement of spatial proximity between genomically disjoint parts of folded nuclear DNA. A prominent feature of 3C experimental results are genomically contiguous regions of dense 3C interactions --- the so-called topological domains --- that affect accessibility of certain regions of DNA and are associated with long-range gene regulation. These megabase-sized topological domains are similar across cell types and are conserved across species. Topological domains are strongly correlated with a number of chromatin markers and have been included in a number of analyses since their discovery. However, visual analyses suggests that functionally-relevant domains may exist at multiple length scales. We adapt the core finding algorithm discussed in the ealier chapter (Coral) to identify novel domain structures.


% XXX tie visualization to structure -- example of the matrix and cores and what lead to domains?

% necessity of validation -- random shuffling of the domains

% XXX significance of domains -- ``pioneering'', used in deconvolution


% We introduce a new and efficient algorithm that is able to capture persistent domains across various resolutions by adjusting a single scale parameter. The identified novel domains are substantially different from domains reported previously and are highly enriched for insulating factor CTCF binding and histone modfications at the boundaries.

The work described in this chapter has been presented at Workshop for Algorithms in Bioinformatics 2013 (WABI)~\cite{Filippova2013} and was extended to extract alternative optimal and near-optimal solutions. Further, this ensemble of solutions was used to quantify the strength of hierarchical organisation between domains at different resolutions~\cite{ArmatusAMB}. The algorithms are implemented in software Armatus which could be downloaded from \url{https://github.com/kingsfordgroup/armatus}.

\section{Background and related work}

  \subsection{Chromosome conformation data}

  Chromatin interactions obtained from a variety of recent experimental techniques in chromosome conformation capture (3C)~\cite{DeWit2012} have resulted in significant advances in our understanding of the geometry of chromatin structure~\cite{Gibcus2013}, its relation to the regulation of gene expression, nuclear organization, cancer translocations~\cite{Cavalli2013}, and copy number alterations in cancer~\cite{Fudenberg2011}.  Of these advances, the recent discovery of dense, contiguous regions of chromatin termed \emph{topological domains}~\cite{Dixon2012} has resulted in the incorporation of domains into many subsequent analyses~\cite{Hou2012,Kolbl2012,Lin2012} due to the fact that they are persistent across cell types, conserved across species, and serve as a skeleton for the placement of many functional elements of the genome~\cite{Bickmore2013a,Tanay2013}.


  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % heatmap figure with domains
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{figure}[t!]
    \begin{center}
      \includegraphics[width=3in]{HeatMapFig}
    \end{center}
    \caption{\textbf{Interaction matrix for a portion of human chromosome 1} from a recent Hi-C experiment by Dixon et al.~\cite{Dixon2012}. Each axis represents a location on the chromosome (40kbp bins).  Densely interacting domains identified by the method of Dixon et al. (red boxes).  Alternative domains are shown as dotted black lines on the upper triangular portion of the matrix.  Visual inspection of the lower triangular portion suggests domains could be completely nested within another and highly overlapping when compared to Dixon et al.'s domains. This motivates the problem of identifying alternative domains across length scales.}
    \label{armatus:heatmap}
  \end{figure}


  3C experiments result in matrices of counts that represent the frequency of cross-linking between restriction fragments of DNA that are spatially near one another (see Figure~\ref{armatus:heatmap} for an example matrix). The original identification of domains in Dixon et al.~\cite{Dixon2012} employed a Hidden Markov Model (HMM) on these interaction matrices to identify regions initiated by significant downstream chromatin interactions and terminated by a sequence of significant upstream interactions.  A defining characteristic of the domains resulting from their analysis is that higher frequency 3C interactions tend to occur within domains as opposed to across domain. This aspect of domains is also reflected in the block-diagonal structure of 3C interaction matrices as shown in Fig.~\ref{heatmap}. In this sense, domains can be interpreted as contiguous genomic regions that self-interact frequently and are more spatially compact than their surrounding regions.

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Topological domains and their significance}

  However, the single collection of megabase-sized domains may not be the only topologically and functionally relevant collection of domains. On closer inspection of the block-diagonal matrix structure in Fig.~\ref{armatus:heatmap}, it becomes clear that there are alternative contiguous regions of the chromosome that self-interact frequently and are likely more spatially compact than their surrounding regions (dotted lines).  Some of these regions appear to be completely nested within others, suggesting a hierarchy of compact regions along the chromosome, while others appear to overlap each other. These observations suggest that functionally-relevant chromosomal domains may exist at multiple scales.

  We introduce a new algorithm to efficiently identify topological domains in 3C interaction matrices for a given domain-length scaling factor $\gamma$. Our results suggest that there exist a handful of characteristic resolutions across which domains are similar. Based on this finding, we identify a consensus set of domains that persist across various resolutions. We find that domains discovered by our algorithm are dense and cover interactions of higher frequency than inter-domain interactions. Additionally, we show that inter-domain regions within the consensus domain set are highly enriched with insulator factor CTCF and histone modification marks. We argue that our straightforward approach retains the essence of the more complex multi-parameter HMM introduced in~\cite{Dixon2012} while allowing for the flexibility to identify biologically relevant domains at various scales.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Problem definition
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem formulation}


  \theoremstyle{definition}
  \newtheorem{prob}{Problem}

  Given the resolution of the 3C experiment (say, 40kb), the chromosome is broken into $n$ evenly sized fragments. 3C contact maps record interactions between different sections of the chromosome in the form of a weighted adjacency matrix $\mathbf{A}$ where two fragments $i$ and $j$ interact with frequency $\mathbf{A}_{ij}$.

  \begin{prob}[Resolution-specific domains] \label{domainprob} Given a $n \times n$ weighted adjacency matrix $\mathbf{A}$ and a resolution parameter $\gamma \geq 0$, we wish to identify a set of domains $D_{\gamma}$ where each domain is represented as an interval $d_i=[a_i, b_i]$, $1 \leq a_i < b_i \leq n$ such that no two $d_i$ and $d_j$ overlap for any $i \ne j$. Additionally, each domain should have a larger interaction frequency within domain than to its surrounding regions.

  Here, the parameter $\gamma$ is inversely related to the average domain size in $D_{\gamma}$: lower $\gamma$ results in sets of larger domains and higher $\gamma$ corresponds to sets of smaller domains. We define $\gamma$ and discuss it in more detail later in the text.

  Specifically, we seek to identify a set of non-overlapping domains $D_{\gamma}$ that optimize the following objective:
  %
  \begin{align}
    \label{obj}
    \max \sum_{[a_i,b_i] \in D_{\gamma}} q(a_i,b_i,\gamma),
  \end{align}
  %
  where $q$ is a function that quantifies the quality of a domain $[a_i, b_i]$ at resolution $\gamma$. Since domains are required to contain consecutive fragments of the chromosome, this problem differs from the problem of clustering the graph of 3C interactions induced by $\mathbf{A}$, since such a clustering may place non-contiguous fragments of the chromosome into a single cluster. In fact, this additional requirement allows for an efficient optimal algorithm.
  \end{prob}

  \begin{prob}[Consensus domains across resolutions]
  \label{consensusprob}  Given $\mathbf{A}$ and a
  set of resolutions $\Gamma = \{\gamma_1, \gamma_2, \ldots \}$, identify a set of non-overlapping domains $D_c$ that are most persistent across resolutions in $\Gamma$:
  \begin{align}
  \label{consobj}
  \max \sum_{[a_i,b_i] \in D_c} p(a_i,b_i,\Gamma),
  \end{align}
  where $p(a_i,b_i,\Gamma)$ is the persistence of domain $[a_i, b_i]$ corresponding to how often it appears across resolutions.
  \end{prob}

\section{Algorithms}



  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Domain identification at a particular resolution}
  \label{singleres}
  Since each row and corresponding column in a 3C interaction matrix encodes a genomic position on the chromosome, we can write the solution to objective~(\ref{obj}) as a dynamic program:
  %
  \begin{align}
    \label{pbd}
    \textsf{OPT}_1(l) = \max_{k<l}\{\textsf{OPT}_1(k-1) + \max\{q(k,l,\gamma),0\}\},
  \end{align}
  %
  where $\textsf{OPT}_1(l)$ is the optimal solution for objective~(\ref{obj}) for the sub-matrix defined by the first $l$ positions on the chromosome ($\textsf{OPT}_1(0) = 0$). The choice of $k$ encodes the size of the domain immediately preceding location $l$. We define negative-scoring domains as non-domains and, as such, only domains with $q > 0$ in the max term in~(\ref{pbd}) are retained.

  Our quality function $q$ is:
  %
  \begin{align}
    \label{quality} q(k,l,\gamma) &= s(k,l,\gamma)-\mu_s(l-k),\mbox{ where}\\
    \label{sumtri} s(k,l,\gamma) &= \frac{\sum_{g=k}^l \sum_{h=g+1}^l A_{gh}}{(l-k)^\gamma}
  \end{align}
  %
  is a \emph{scaled density} of the subgraph induced by the interactions $A_{gh}$ between genomic loci $k$ and $l$. Equation~(\ref{quality}) is the zero-centered sum of~(\ref{sumtri}), which is the upper-triangular portion of the submatrix defined by the domain in the interval $[k,l]$ divided by the scaled length $(l-k)^{\gamma}$ of the domain. When $\gamma=1$,  the scaled density is the weighted subgraph density~\cite{goldberg1984finding} for the subgraph induced by the fragments between $k$ and $l$. When $\gamma=2$, the scaled density is half the internal density of a graph cluster~\cite{Schaeffer2007}. For larger values of $\gamma$, the length of a domain in the denominator is amplified, hence, smaller domains would produce larger objective values than bigger domains with similar interaction frequencies. $\mu_s(l-k)$ is the mean value of~(\ref{sumtri}) over all sub-matrices of length $l-k$ along the diagonal of $\mathbf{A}$, and can it be pre-computed for a given $\mathbf{A}$. We disallow  domains where there are fewer than 100 sub-matrices available to compute the mean. By doing this, we are only excluding domains of size larger than $n-100$ fragments, which in practice means that we are disallowing domains that are  hundreds of megabases long.  Values for the numerator in (\ref{sumtri}) are also pre-computed using an efficient algorithm~\cite{Filippova2012}, resulting in an overall run-time of $O(n^2)$ to compute $\textsf{OPT}_1(n)$.

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Consensus set - persistence
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Obtaining a consensus set of persistent domains across resolutions}
  \label{consensusalg}
  For objective (\ref{consobj}), we use the procedure in section~\ref{singleres} to construct a set $\mathcal{D} = \bigcup_{\gamma \in \Gamma} D_{\gamma}$.  $\mathcal{D}$ is a set of overlapping intervals or domains, each with a quality score defined by its persistence $p$ across resolutions. To extract a set of highly persistent, non-overlapping domains from $\mathcal{D}$, we reduce  problem~\ref{consensusprob} to the weighted interval scheduling problem~\cite{Kleinberg2005}, where competing requests to reserve a resource in time are resolved by finding the highest-priority set of non-conflicting requests. To find a consensus set of domains, we map a request associated with an interval of time to a domain and its corresponding interval on the chromosome. The priority of a request maps to a domain's persistence $p$ across length scales.

  The algorithm to solve problem~\ref{consensusprob} is then:
  \begin{align}
  \label{wis}
  \textsf{OPT}_2(j) = \max\{\textsf{OPT}_2(j-1), \textsf{OPT}_2(c(j)) + p(a_j,b_j,\Gamma) \}
  \end{align}
  where $\textsf{OPT}_2(j)$ is the optimal non-overlapping set of domains for the $j$th domain in a list of domains sorted by their endpoints ($\textsf{OPT}_2(0) = 0$), and $c(j)$ is the closest domain before $j$ that does not overlap with $j$.  The first and second terms in~(\ref{wis}) correspond to either choosing or not choosing domain $j$ respectively.
  We pre-compute a domain's persistence $p$ as:
  \begin{align}
  \label{persist}
  p(a_i,b_i,\Gamma) = \sum_{\gamma \in \Gamma} \delta_i \text{ where }
  \delta_i = \begin{cases}
  1 & \text{if } [a_i,b_i] \in D_{\gamma} \\ 0 & \text{otherwise.}
  \end{cases}
  \end{align}
  Equation~(\ref{persist}) is therefore a count of how often domain $i$ appears across all resolutions in $\Gamma$ for domain sets identified by the method in section~\ref{singleres}. It may be desirable to treat multiple highly overlapping, non-equivalent domains as a single domain, however, we conservatively identify exact repetitions of a domain across resolutions since this setting serves as a lower bound on the persistence of the domain. If $m=|\mathcal{D}|$, then pre-computing persistence takes $O(m|\Gamma|)$ time, and $c(j)$ is precomputed after sorting the intervals by their endpoints. The limiting factor when computing $\textsf{OPT}_2(m)$ is time to compute $c(j)$, which is $m\log m$. Thus, the overall algorithm runs in $O(m\log m + (n^2+m)|\Gamma|)$ time taking into account an additional $O(n^2|\Gamma|)$ for computing $\mathcal{D}$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Results: intrinsic and extrinsic validation of domains, comparison
% to Bing Ren's domains
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

  We used chromatin conformation capture data from Dixon et al.~\cite{Dixon2012} for human fibroblast and mouse embryonic cells. The 3C contact matrices were already aggregated at fragment size 40kb and were corrected for experimental bias according to~\cite{Yaffe2011}. We compared our multiscale domains and consensus sets against the domains generated by Dixon et al. for the corresponding cell type and species.
  For human fibroblast cells, we used CTCF binding sites from~\cite{Kim2007}.
  For mouse embryonic cell CTCF binding sites and chromatin modification marks, we used data by Shen et al.~\cite{Shen2012}.

  % List of parameters required by Dixon et al.
  % \begin{itemize}
  % \item 2MB upstream/downstream boundary
  % \item 1-20 mixtures of gaussians
  % \item Median posterior probabilities $\geq 0.99$, at least 80kbp
  % \end{itemize}


  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Dense domains
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Ability to identify densely interacting domains across scales}

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % inter-intra dist plot; disributions of sizes
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{figure}[t]
    \begin{center}
    \subfigure[Domain size vs. frequency]{
      \includegraphics[width=0.45\linewidth]{imr90-inter-intra-all-chromos}
      \label{subfig:size-freq}
    }
    \subfigure[Mean frequency distr.]{
      \includegraphics[width=0.45\linewidth]{mean-freq-distr}
      \label{subfig:freq-distr}
    }
    \caption{~\subref{subfig:size-freq} Our algorithm discovers domains with mean frequency value for inter- and intra-domain interactions (solid lines) at or better than that of Dixon et al. domains (dotted lines). Each solid line represents domains at different resolution $\gamma$ in human fibroblast cells. \subref{subfig:freq-distr}~Multiscale domains identified in human fibroblast cells by our dynamic program tend to have higher mean frequency than those of Dixon et al.
    (distributions are plotted after outliers $> \mu+4\sigma$ were removed).}

    \label{fig:mi_jacc}
    \end{center}
  \end{figure}

  Multiresolution domains successfully capture high frequency interactions and leave interactions of lower mean frequency outside of the domains. We compute the mean interaction frequency for all intra- and inter-domain interactions at various genomic lengths and plot the distribution of means for multiple resolutions (Fig.~\ref{subfig:size-freq}). The mean intra-domain interaction frequency (blue) is consistently higher (up to two times) than the mean frequency for interactions that cross domains (red). Compared to the domains reported by Dixon et al., our domains tend to aggregate interactions of higher mean frequency, especially at larger $\gamma$. The distribution of mean intra-domain frequencies for Dixon et al. is skewed more to the left than that of the multiscale domains (Fig.~\ref{subfig:freq-distr}). This difference can be partially explained by the fact that multiscale domains on average are smaller in size ($\mu=0.2$Mb, $\sigma=1.2$Mb) than domains reported by Dixon et al. ($\mu=1.2$Mb, $\sigma=0.9$Mb).


  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Domain persistence across scales}

  Domain sets across resolutions share significant similarities, even as the distribution of domains and their sizes begin to change (Fig.~\ref{fig:dom_size}). The patterns of similarity are particularly obvious if we plot the domains at various resolutions (Fig.~\ref{subfig:multiresDomains}): many domains identified by our algorithm persist at several resolutions and are aggregated into larger domains at smaller $\gamma$, suggesting a hierarchical domain structure. The stability of these domains across resolutions indicates that the underlying chromosomal structure is dense within these domains and that these domains interact with the rest of the chromosome at a much lower frequency.

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Domain sizes over all resolutions, jaccard/overlap/vi between us and b.r.
  % over all resolutions
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{figure}[t]
    \centering
    \subfigure[domain size \& count vs. $\gamma$]{
        \includegraphics[width=0.46\linewidth]{domain_sizes_all_IMR90.pdf}
        \label{subfig:sizeCount}
    }
    \subfigure[similarity to Dixon et al. domains]{
      \includegraphics[width=0.43\linewidth]{overlap_br_ours_chr1.pdf}
      \label{subfig:dixonSim}
    }
    \caption{~\subref{subfig:sizeCount} The domain sizes increase and the domain count decreases as the resolution parameter drops. Above: plotted are maximum (red), average (blue), and minimum (green) domain size averaged over all chromosomes for the domains on human fibroblasts. The magenta line shows the average domain size for domains reported by Dixon et al. Below: the number of domains increases for higher values of resolution parameter. The magenta line displays domain count for Dixon et al.~\subref{subfig:dixonSim} According to the Jaccard metric, the similarity between multiresolution domains and domains reported by Dixon et al. increases as the resolution parameter goes to zero.}%, however, the variation of information suggests that the two sets of domains are most similar around $\gamma=0.25$.}
    \label{fig:dom_size}
  \end{figure}


  A pairwise comparison of domain configurations displays regions of stability across multiple resolutions (Fig.~\ref{subfig:multiresVI}). We use the variation of information (VI)~\cite{Meila2003}, a metric for comparing two sets of clusters, to compute the distance between two sets of domains. To capture the similarities between two domain sets $D$ and $D'$ and the inter-domain regions induced by the domains, we construct new derivate sets $C$ and $C'$ where $C$ contains all domains $d \in D$ as well as all inter-domain regions ($C'$ is computed similarly). To compute entropy $H(C) = \sum_{c_i  \in C} p_i \log p_i$, we define the probability of seeing each interval in $C$ as $p_i = (b_i - a_i) / L$ where $L$ is the number of nucleotides from the start of the leftmost domain to the end of the rightmost domain in the set $D \cup D'$. When computing the mutual information $I(C, C') = \sum_{c_i \in C} \sum_{c'_j \in C'} p_{ij} \log[ p_{ij} / (p_i p_j) ]$ between two sets of intervals $C$ and $C'$, we define the joint probability $p_{ij}$ to be $| [a_i, b_i] \cap [a_j, b_j] | / L$.
  We then compute variation of information on these two new sets: $VI(C, C') =  H(C) + H(C') - 2I(C, C')$ where $H(\cdot)$ is entropy and $I(\cdot, \cdot)$ is mutual information. Chromosome 1, for example, has three visually pronounced groups of resolutions within which domain sets tend to be more similar than across ($\gamma = $[0.00-0.20], [0.25-0.70], and [0.75-1.00] --- see Fig.~\ref{subfig:multiresVI}).


  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % Domains at different resolutions as a line plot
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{figure}[t]
    \begin{center}
    \subfigure[domains across resolutions]{
      \includegraphics[width=0.55\linewidth]{domains_long_chr1}
      \label{subfig:multiresDomains}
    }
    \subfigure[VI across resolutions]{
      \includegraphics[width=0.366\linewidth]{vi_chr1}
      \label{subfig:multiresVI}
    }
    \end{center}
    \caption{~\subref{subfig:multiresDomains} Domains identified by our algorithm (black) are smaller at higher resolutions and merge to form larger domains at $\gamma$ close to 0. Visual inspection shows qualitative differences between consensus domains (red) and domains reported by Dixon et al. (green). Data shown for the first 4Mb of chromosome 1.~\subref{subfig:multiresVI} Variation of information for domains identified by our algorithm across different resolutions for chromosome 1 in human fibroblast cells.}
    \label{fig:domains_line}
  \end{figure}


  \subsection{Comparison with the previously identified set of domains in Dixon et al.}

  \begin{figure}[t!]
    \begin{center}
      \includegraphics[width=.9\linewidth]{UsVsBingVI}
    \end{center}
    \caption{\textbf{Comparison of Dixon et al.'s domain set} with the multiscale consensus set for chromosomes 1--22 ($x$-axis). We used the variation of information (VI) ($y$-axis) to compute distances between domain sets for the multiscale consensus set vs. Dixon et al (blue dots) and the multiscale consensus vs. randomly shuffled domains (red diamonds).}
    \label{fig:consensus_agreement}
  \end{figure}

  At higher resolutions, domains identified by our algorithm are smaller than those reported by Dixon et al. (Fig.~\ref{subfig:sizeCount}). As the resolution parameter decreases to 0.0, the average size of the domains increases  (see Fig.~\ref{fig:dom_size} for results for chromosome 1 on the IMR90 human fibroblast cells). As domains expand to cover more and more of the chromosome, the similarity to the domains identified by Dixon et al.~\cite{Dixon2012} also increases (Fig.~\ref{subfig:dixonSim}). We calculate the Jaccard similarity between two sets of domains $D$ and $D'$ as $J(D, D') = N_{11} / (N_{11} + N_{01} + N_{10})$ where the quantities $N_{11}$, $N_{01}$, and $N_{10}$ are the number of 3C fragments that are in a domain in both sets $D$ and $D'$, the number of fragments that are in a domain in $D'$, but not in $D$, and the number of fragments that are in a domain in $D$, but not $D'$, respectively (light blue in Fig.~\ref{subfig:dixonSim}). The composition of the domains, however, is different as is captured by the variation of information (red in Fig.~\ref{subfig:dixonSim}). Overall, we identify domains that cover similar regions of the chromosome (Fig.~\ref{fig:mi_jacc}), yet differ in their size distribution and genomic positions.

  We use the algorithm described in section~\ref{consensusalg} to obtain a consensus set of domains $D_c$ persistent across resolutions. We construct the set $\Gamma$ by defining the range of our scale parameter to be $[0, \gamma_\textrm{max}]$ and incrementing $\gamma$ in steps of 0.05. In order to more directly compare with previous results, we set $\gamma_{\max}=0.5$ for human and $0.25$ for mouse since these are the scales at which the maximum domain sizes in Dixon et al.'s sets match the maximum domain sizes in our sets.

  Our consensus domain set agrees with the Dixon et al. domains better than with a randomized set of domains adhering to the same domain and non-domain length distributions (Fig.~\ref{fig:consensus_agreement}). Our primary motivation in comparing to randomized sets of domains is to provide a baseline that we can use to contrast our set of domains with Dixon et al. Comparing to a set of random domains also helps to verify that our observations are due to the observed sequence of domains and not the distribution of domain lengths. To shuffle Dixon's domains, we record the length of every domain and non-domain region, and then shuffle these lengths to obtain a randomized order of domains and non-domains across the chromosome.  The fact that variation of information is lower between consensus domains and domains reported by Dixon et al. demonstrates that, though the approaches find substantially different sets of topological domains, they still agree significantly more than one would expect by chance.

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Enrichment of CTCF and histone modifications near boundaries}
  \label{sec:Enrichment}

  We assess the enrichment of transcription factor CTCF and histone modifications H3K4me3 and H3K27AC within the inter-domain regions induced by the consensus domains. These enrichments provide evidence that the boundary regions between topological domains correlate with genomic regions that act as insulators and barriers, suggesting that the topological domains may play a role in controlling transcription in mammalian genomes~\cite{Dixon2012}.

  Figure~\ref{fig:enrichment} illustrates the enrichment of insulator or barrier-like elements in domain boundaries in both the human fibroblast (IMR90) and mouse embryonic stem cell (mESC) lines.  Specifically, we observe that
  the boundaries between consensus domains are significantly enriched for all of the transcription factors and histone marks we consider.  In certain cases --- specifically in the case of CTCF --- we notice that the CTCF binding signals peak more sharply in the boundaries between the domains we discover than in the boundaries between the domains of Dixon et al.

  \begin{figure}[ht]
  %
  \begin{center}
  \includegraphics[width=0.73\textwidth]{fig6_combined_rotated}
  % \subfigure[IMR90: CTCF]{
  % \includegraphics[width=2.1in]{IMR90_CTCF_Ours}
  % \includegraphics[width=2.1in]{IMR90_CTCF_Bingy}
  % \label{fig:CTCF}
  % }
  % %
  % \subfigure[mESC: CTCF]{
  % \includegraphics[width=2.1in]{mESC_CTCF_Ours}
  % \includegraphics[width=2.1in]{mESC_CTCF_Bingy}
  % \label{fig:mESC_CTCF}
  % }
  % %
  % \subfigure[mESC: H3K4me3]{
  % \includegraphics[width=2.1in]{mESC_H3K4me3_Ours}
  % \includegraphics[width=2.1in]{mESC_H3K4me3_Bingy}
  % \label{fig:H3K4me3}
  % }
  % %
  % \subfigure[mESC: H3K27AC]{
  % \includegraphics[width=2.1in]{mESC_H3K27AC_Ours}
  % \includegraphics[width=2.1in]{mESC_H3K27AC_Bingy}
  % \label{fig:H3K27AC}
  % }
  \caption{\textbf{Enrichment of binding CTCF binding} (a) in IMR90 and (b) in mESC and histone modifications (c), (d) in mESC around domain boundaries for our consensus set of persistent domains (left, blue), and for those identified by Dixon et al. (right, blue).  Green lines represent the presence of CTCF at the midpoint of the topological domains.}
  \label{fig:enrichment}
  \end{center}
  \end{figure}


  \begin{table}[b]
  \centering
  \caption{\textbf{Domain enrichment.} Each table entry is of the form $\frac{e}{t} \approx r$ where $e$ is the number of elements containing $\ge 1$ of CTCF and histone modifications, $t$ is the total number of elements and $r$ is the approximate ratio $e/t$.  Our method produces more domains,
  and hence more boundaries, than that of Dixon et al.~\cite{Dixon2012}.  However, relative to Dixon et al., our domains are depleted for peaks of interest, while our boundaries are significantly enriched
  for such peaks.}
  %Our method tends to produce ($1.6$---$2.3$ times) more domains than that of Dixon et al.~\cite{Dixon2012}.  However, while the domains produced by both methods contain at least peak for the different chromatin factors we consider in roughly the same proportion, the boundaries between our domains contain at least one peak for these factors about twice as frequently as the boundaries between the domains of Dixon et al.}
  \label{tab:differentalEnrichment}
  \scriptsize
  \begin{tabular}{lc@{\hskip 15pt}c@{\hskip 5pt}|@{\hskip 5pt}c@{\hskip 15pt}c@{\hskip 15pt}c}
  \toprule
  %& \multicolumn{3}{r}{Ratio: (current method / \cite{Dixon2012})} \\
  %\cmidrule(r){2-4}
  Signal & Domains (\cite{Dixon2012}) & Domains (Ours) & Boundaries (\cite{Dixon2012}) & Boundaries (Ours) \\
  %       & Domains w/ $\ge 1$ peak        & w/ $\ge 1$ peak & w/ $\ge 1$ peak \\
  \midrule
  CTCF (IMR90)   & $\frac{2050}{2234}\approx0.92$ & $\frac{3092}{5365}\approx0.58$ & $\frac{423}{2136}\approx0.20$ & $\frac{2126}{4861}\approx0.44$ \\[0.5em]
  CTCF (mESC)    & $\frac{2057}{2066}\approx1.00$ & $\frac{2500}{3578}\approx0.70$ & $\frac{654}{2006}\approx0.33$ & $\frac{2258}{3122}\approx0.72$ \\[0.5em]
  H3K4me3 (mESC) & $\frac{2019}{2066}\approx0.98$ & $\frac{2362}{3578}\approx0.66$ & $\frac{600}{2006}\approx0.30$ & $\frac{1738}{3122}\approx0.60$ \\[0.5em]
  H3K27AC (mESC) & $\frac{1922}{2066}\approx0.93$ & $\frac{2254}{3578}\approx0.63$ & $\frac{458}{2006}\approx0.23$ & $\frac{1342}{3122}\approx0.43$ \\
  \bottomrule
  \end{tabular}
  \end{table}
  % \vspace{-25px}

  We also observe that, when compared with the domain boundaries predicted by Dixon et al., our boundaries more often contain insulator or barrier-like elements (see Table~\ref{tab:differentalEnrichment}). Specifically, we normalize for the fact that we identify approximately twice as many domains as Dixon et al., and generally observe a two-fold enrichment in the fraction of boundaries containing
  peaks for CTCF markers. This suggests that structural boundaries identifed by our method are more closely tied to functional sites which serve as barriers to long-range regulation. We also observe a depletion of insulator CTCF elements within our domains when compared to the domains of Dixon et al.  This observation is consistent with the assumption that transcriptional regulation is more active within spatially proximate domains since there are fewer elements blocking regulation within these domains.  Table~\ref{tab:differentalEnrichment} also shows similar patterns for histone modifications which suggests that our domain boundaries are enriched for functional markers of gene regulation.

\section{Discussion and Conclusions}

  In this paper, we introduce an algorithm to identify topological domains in chromatin using interaction matrices from recent high-throughput chromosome conformation capture experiments.  Our algorithm produces domains that display much higher interaction frequencies within the domains than in-between domains (Fig.~\ref{fig:mi_jacc}) and for which the boundaries between these domains exhibit substantial enrichment for several known insulator and barrier-like elements (Fig.~\ref{fig:enrichment}).  To identify these domains, we use a multiscale approach which finds domains at various size scales.  %To obtain a single set of domains from this rich ensemble, e extract a non-overlapping set of consensus domains that are most persistent across multiple length scales.
  We define a consensus set to be a set of domains that persist across multiple resolutions and give an efficient algorithm that finds such a set optimally.

  % -- Practical running time
  The method for discovering topological domains that we have introduced is practical for existing datasets.  Our implementation is able to compute the consensus set of domains for the human fibroblast cell line and extract the consensus set in under 40 minutes when run on a personal computer with 2.3GHz Intel Core i5 processor and 8Gb of RAM.


  Our method is particularly appealing in that it requires only a single user-specified parameter $\gamma_{\text{max}}$. It uses a score function that encodes the quality of putative domains in an intuitive manner based on their local density of interactions.  Variations of the scoring function in~(\ref{quality}), for example, by median centering rather than mean centering, can be explored to test the robustness of the enrichments described here. For our experiments, the parameter $\gamma_{\max}$ was set based on the maximum domain sizes observed in Dixon et. al's experiments so that we could easily compare our domains to theirs.  This parameter can also be set intrinsically from properties of the Hi-C interaction matrices.  For example, we observe similar enrichments in both human and mouse when we set $\gamma_{\max}$ to be the smallest $\gamma \in \Gamma$ such that the median domain size is $>$80kbp (two consecutive Hi-C fragments at a resolution of 40kbp). This is a reasonable assumption since domains consisting of just one or two fragments do not capture higher-order spatial relationships (e.g. triad closure) and interaction frequencies between adjacent fragments are likely large by chance~\cite{LiebAid2009}.  We also compared the fraction of the genome covered by domains identified by Dixon et al. vs. the domains obtained from our method at various resolutions.  Dixon et al.'s domains cover 85\% of the genome while our sets tend to cover less of the genome ($\approx$ 65\% for a resolution which results in the same number of domains as those of Dixon et al.).  The fact that our domain boundaries are more enriched for CTCF sites indicates that our smaller, more dense domains may be more desirable from the perspective of genome function.

  The dense, functionally-enriched domains discovered by our algorithm provide strong evidence that alternative chromatin domains exist and that a single length scale is insufficient to capture the hierarchical and overlapping domain structure visible in heat maps of 3C interaction matrices. Our method explicitly incorporates the desirable properties of domain density and persistence across scales into objectives that maximize each and uncovers a new view of domain organization in mammalian genomes that warrants further investigation.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%
%% Huffmer and Referee -- compression projects
%%
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Compression of sequencing and sequence alignment data}

With increasing amounts of sequencing data being generated, the problem of storing, transmitting, and analyzing such data at scale becomes a bottleneck. Various compression methods design for sequencing data can significantly reduce the size of data, alleviating the storage burden and making transmission times faster. %with some methods enabling faster downstream analyses.

(abstract from Referee)

Continued growth of generated sequencing data demands novel scalable approaches to its storage and transmission. It is also crucial that analyses could be run on the dataset in its compressed format without having to fully reconstruct it. We propose a novel approach to compression of sequence alignment data, a well established data format that is used for a variety of tasks ranging from genome assembly to variant calling. Such alignment files may exceed the size of the original sequence by an order of magnitude, however, \refer, our tool implementing the approach, is able to compress alignment files to $1/10$ of the original SAM file size and is twice as efficient as SAM's binary BAM variant. \refer is fast, highly parallelizable, and outperforms state of the art tools by an average of 8.1\% while enabling a variety of sequence-related tasks that require only a partial decompression. Computations like depth of sequencing that involve seeking through all alignments take from 8 to 44 seconds for \refer as opposed to tens of minutes with \texttt{samtools}. \refer uses a lightweight streaming clustering algorithm to improve quality values compression and encodes sequence information very efficiently, with compression rates as low as 0.06 bits per base. Its modular structure allows one to omit extraneous alignment information from the download reducing sequencing data from many gigabytes to under a hundred megabytes.


Work presented in this chapter was submitted for publication~\cite{Referee_draft} and Referee, the software implementing the ideas, is available for download at \url{https://github.com/Kingsford-Group/referee}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}

  \textit{De novo} compression methods accept raw sequencing reads, FASTA, or FASTQ files as input and focus on ways to compress sequence~\cite{Mince,PathEncode,Rozov2014}, quality values~\cite{SeqSqueeze,KmerQuals,JaninQuals}, or both~\cite{GSQZ,Sahinalp2012}. Tools like SCALCE~\cite{Sahinalp2012} and Mince~\cite{Mince} take the approach of grouping the reads based on their similarity allowing the downstream general-purpose compression tools to build better dictionary encodings. Rozov \etal~\cite{Rozov2014} and recent work by Kingsford and Patro~\cite{PathEncode} use an available reference sequence to inform their compression algorithm without explicitly aligning reads to it. Quip~\cite{Jones2012} takes a step closer to reference-based approach by creating contigs based on the subset of data and encoding the rest of the reads relative to their lightweight assembly. De novo compression tools compress sequence down to 3-10\% of its original size at reasonable speeds~\cite{Deorowicz2013}, however, most tools require that the dataset is completely decompressed before it could be used for analysis.

  A process of mapping reads to a known reference sequence is often a prerequisite for many downstream tasks like assembly~\cite{Assembly}, expression estimation~\cite{Cufflinks,RSEM}, or variant calling~\cite{GATK}, however, alignment files stored in a popular SAM format~\cite{SamTools} occupy orders of magnitude more space than the original sequencing reads themselves. A commonly used approach is to convert SAM files to its block-compressed equivalent, BAM, offers a significant improvement in the used disk space while offering the same set of analyses at a low computational cost. Later approaches improve the compression ratio by sacrificing the ability to run analyses without full decompression~\cite{Goby,Jones2012}, by turning to lossy compression, or by doing without certain sequence data~\cite{SlimGene,CRAM}. More recently, Hach \etal introduced a more efficient block-wise BAM/SAM compressor that allows for block-wise decompression while offering savings of up to 44\% over BAM~\cite{Sahinalp2015}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{De novo sequence compression}

  \subsection{Using standard command-line tools}

  Gzip, bzip, plzip

  Reordering reads

  Formulation of the optimal reordering (recently proven to be NP-hard~\cite{RobPAndCo})

  \subsection{Grammar-based and dictionary-based encoding}

  XXX

  \subsection{Buliding efficient dictionaries}

  XXX

  \subsection{Optimal string parsing given a dictionary with codes}

  XXX

  \subsection{Results}

  XXX

  mini-conclusion - meh

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reference-based sequence and alignment compression}

  \subsection{Improving data homogeneity}

  \refer uses an approach similar to the one used by Goby~\cite{Goby} where homogeneous data streams are compressed together, however, \refer operates in a lossless mode and structures data in a flat, non-hierarchical manner. We organize and transform the data in a way that either makes the input smaller and/or remaps disparate data points into a smaller, more uniform domain. A downstream general-purpose dictionary coder then is able to capture the exposed redundancy and compress the preprocessed streams more efficiently. A similar strategy has been shown to work well for \textit{de novo} sequence compression in SCALCE~\cite{Sahinalp2012} and Mince~\cite{Mince} as well as for alignment compression in Deez~\cite{Sahinalp2015}. We use \texttt{plzip} for all downstream compression tasks since it is fast and highly parallelizable in addition to offering a slight compression improvement on most of our inputs. Below, we discuss specific operations applied to sequencing, numerical, and quality values data.

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Estimating reference-based compression rates}
  \label{sec:seq-comp-methods}

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % XXX CK: Figure 1 and table 1 should be moved to the results (table 1 could be removed). In general, the ``methods'' section should focus more on the algorithm / implementation.

  Suppose a sequencing read $r$ can be mapped to the reference sequence $G$ (i.e. a genome) without errors. Then the only information needed to reconstruct the read is the mapping position within the reference. Given the reference length in bases, $|G|$, we would only need $\log_2 |G|$ bits to encode the mapping position. Given a collection of a $N$ reads of uniform length $|r|$, we can then encode the input $N|r|$ bytes in $N \log_2|G| / 8$ bytes, resulting in a $\log_2|G| / 8|r|$ overall compression ratio. For example, given ten million human RNA-seq 100-bp long reads taking up 953Mb on disk, we could compress these data down to around 35Mb given a 300Mb transcriptome length assuming that every read mapped without errors, on average spending just 0.29 bits to encode a single base, a sevenfold improvement over the classic 2-bit sequence encoding.

  %Longer reads would result in an even greater compression (e.g. 0.03 for 100bp reads).

  This theoretical argument could be extended to account for sequencing errors and variation. A single mismatch, insertion, or a deletion can be recorded by a few bits indicating which operation we must use and its position relative to the beginning of the read. To encode a position, one would spend $\log_2 |r|$ bits of information; similarly, we would spend just 3 bits to encode an operation given the set of 6 edit operations ${A,C,G,T,I,D}$. Altogether, a single variation can be recorded using $3 + \log_2 |r|$ bits of information. Assuming an error rate of at most 3 edits per read, we would spend at most 60Mb to encode edits for the above ten million reads --- a mere 6\% of the size of the original sequence.

  In reality, our estimate of $\log_2 |G|$ bits per read is an upper bound on the amount of information needed to record the mapping positions and \refer achieves better compression rates (especially for deep coverage RNA-seq experiments --- see Table~\ref{tab:seq-compression}). When mapping positions are sorted, recording the intervals between pairs of consecutive positions (\textit{delta encoding}) allows to reconstruct all of the numbers exactly while representing the sequence in a smaller domain --- therefore, requiring fewer bits per single position. In biological data, the sequencing depth may further aid in compression: for multiple reads mapping to the same position it is enough to save the position once and separately maintain the count of reads mapping there.

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Practical sequence compression implementation} %In practice, \refer achieves compression rates that are at or better than the theoretical bound argues above.
  %
  \refer separates sequence and mapping position information from the rest of the SAM data and encodes it in a way that is more efficient than any other method to date. Storing the original read sequence is, in fact, unnecessary when the read alignment and all the differences between the reference sequence and the read are known. The information about the differences is available in SAM format through the reference sequence field, read offsets and a CIGAR and MD strings that encode sequence differences. Since we are considering a case of sorted BAM file, the reference sequence (or, rather, its identifier) needs to only be recorded once at the beginning of the block of alignments mapping to that reference sequence obviating the need to store the reference field altogether. The mapping locations can be viewed as a sequence of non-decreasing highly repetitive integers that naturally lend themselves to an efficient delta and run-length encoding (RLE) scheme. Taken together, a delta transform and RLE minimize the volume of data sent to the downstream compressor thus decreasing memory usage and runtime. Additionally, such an approach to encoding offsets makes \refer robust to increasing sequencing depths and allows one to encode offsets in space on the order of $0.01\%$ ($0.1\%$) of the original SAM file size for RNA-seq data (whole-genome data respectively).

  % efficiency of sequence compression
  % \begin{table}[ht!]
  % 	\caption{\refer uses an efficient encoding for sequencing data, spending as low as 0.03 bits per individual base. $\dag$ -- RNAME, POS, CIGAR, and MD string in a column format, plzipped. Unaligned sequences were not included in the analysis; sizes are reported in Mb.}
  % 	\label{tab:seq_comp}
  % 	\centering
  % 	\begin{tabular}{l c r r r }
  % 	\toprule
  % 	File & Bases & 2bit & SAM$^\dag$ & \refer \\
  % 	\midrule
  % 	% % plzip -- includes clipped regions
  % 	% SRR445718 	& 3,422,790,818 & 855,697,705 	& 140,098,256 & 85,257,945\\
  % 	% % plzip
  % 	% SRR1294122	& 4,343,436,421 & 1,085,859,106 & 143,307,769 &  88,170,914 \\
  % 	% plzip -- includes clipped regions
  % 	SRR445718 	& $3.4 \times 10^9$ & 815.06 & 133.61 & 86.94 \\
  % 	% plzip
  % 	SRR1294122	& $4.3 \times 10^9$ & 1035.56 & 136.67 & 85.22 \\
  % 	\bottomrule
  % 	\end{tabular}
  % \end{table}


  To efficiently encode all differences between the reference sequence and the read, \refer merges edits as represented by CIGAR and MD strings (or obtains mismatch locations by comparing read to the reference genome if MD string is not available) -- this is in contrast to saving edited CIGAR strings as in~\cite{Sahinalp2015}.  When considered on its own, the CIGAR string possesses certain inefficiencies: for example, a single insertion in the middle of a 32bp read would be represented as an 8 byte string \texttt{20M1I11M} where 8 bits would suffice. The same is true for MD strings: matches are explicitly encoded, even in the case when the read and the reference are identical and MD string is redundant. Neither field is needed when sequence matches exactly and a single bit is enough to indicate whether the alignment was exact. Instead of encoding exact matches with CIGAR and/or MD strings, \refer maintains a separate bit vector where $i^{th}$ bit is set to $1$ if $i^{th}$ alignment had any edits and is set to $0$ otherwise. This bit vector itself is amenable to compression: runs of alignments that had edits translate to a series of identical bytes that can be efficiently compressed by the run-length coders.

  For every read that differed from the reference sequence, i.e.\@ that was recorded as a $1$ in the bit vector described above, \refer merges all differences into a single series of edits describing the read completely. Edit positions within the series are ordered and delta encoded relative to the preceding position. The resulting delta encoding is smaller on average than the values in CIGAR or MD strings when considered separately thus minimizing the total entropy of the edits. A list of edits for a single alignment is then appended to the end of an edit stream thus maintaining their original order. The order in which $1$'s were set in the binary vector mentioned earlier corresponds to the order in which edits appear in the edit stream making it easy to match an alignment and its edits when restoring the original sequences. Some biologically meaningful edits, e.g. single nucleotide polymorphisms, would have the same relative encoding from one alignment to another. Storing such edits in a single stream allows the downstream dictionary compressor to pick up on the similarities and to capitalize on such consistent edits.

  However, the compression benefits due to such co-located edits would be minor. To estimate the savings from explicitly identifying consistent edits and encoding them via a modified contig as described in~\cite{Sahinalp2015}, we first identified all bases that had at least one read with an edit in that location. We then reasoned that a base had to have over half of all reads aligning to it agree on a single edit for that edit to be beneficial for compression. For example, if a given position in the reference sequence has a `C' and there are 100 reads aligned to it with 40 reads agreeing with the reference, 30 of the reads having an `A' instead of a `C', and the remaining 30 reads having a `G', modifying the reference to contain an `A' or a `C' would result in 70 of the reads differing from the new reference sequence and only 30 reads matching it exactly. However, if the distribution of the read counts was such that only 10 reads were matching the original reference, 60 reads contained an `A' and 30 reads contained a `G', then modifying the reference to contain `A' would result in 60 reads matching the reference and only 40 reads having an edit. Guided by this reasoning, we observe that across all tested datasets less than 10\% of all loci that have mapping errors display a significant agreement. i.e.\@ had a single edit repeated among $>50$\% of all reads mapped to the same locus.

  % since only a fraction of all edit positions have edits that are consistent across more than $50\%$ of the reads in the pileup.

  \subsection{Unaligned sequence}

  \refer compresses the unaligned sequences separately from the rest of the alignment data and stores unaligned reads along with their quality values and read identifiers in FASTQ format. Since read order does not matter for these reads, the reads are reordered lexicographically in batches as \refer accumulates them while compressing alignments. Before reordering the reads, each sequence is compared to its reverse complement. \refer chooses to keep the original sequence or its reverse complement based on which minimizer was lexicographically smaller. Intuitively, this operation reduced the space of substrings observed in the unaligned reads and aids in compression. A more sophisticated compression tool like Mince~\cite{Mince} can further reduce compressed size of the unaligned reads.

  % XXX: will it make more sense to store qual. for unaligned sequences with other qualities in the clusters? then unaligned sequences would be on their own and might compress better. unaligned qualities tend to be rather bad, so might compress better by landing in the appropriate cluster.


  % XXX: compute compression rates for the unaligned sequence; how much does Mince improve on it


  % Numerical fields (flags, map, plen, etc): encoding into unique dictionaries; delta on the last 2 columns.

  % \subsection*{Compressing read identifiers}

  % XXX Split and encode prefixes separately.



  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Encoding numerical fields}

  SAM format contains a number of numerical fields that encode details such as whether or not read was reverse complements or the quality of the alignment assigned by the mapping tool. Even though the domain for these fields can be rather large (for example, \texttt{PNEXT} field can assume values in $[0, 2^{31}-1]$), in practice, there often are just a few frequent values that these fields assume. To exploit this fact, we make a mapping from the original domain to a new integer domain $[0, f - 1]$ where $f$ is the number of unique values a field assumes in a given input file. On average, this increases homogeneity between the numerical fields within a single alignment and across many consecutive alignments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Quality values compression}

  We use three observations to compress quality vectors efficiently: the fact that quality values often get worse towards the end of the read~\cite{SeqSqueeze}; that a large percent of all quality vectors are mostly consistent (i.e.\@ a single quality value is overrepresented); and that many vectors share a sufficiently long subsequence. Indeed, most quality vectors have higher values in the beginning of the vector while values in its suffix may be consistently low due to the low level physics of the sequencing technology. We also observe that --- at least for Illumina quality vectors --- the prefix may also contain a sequence of low values that tend to be similar across many of the vectors.  Additionally, we observe that for many datasets, several quality values are significantly more frequent than the others, for example, for the \textit{P. aeruginosa} dataset, the top 6 quality values account for almost 80\% of all characters observed in quality vectors (top 10 values for \texttt{SRR445718}). The distribution of these values within vectors follows a certain trend as well: for 64\% of the quality vectors in \textit{P. aeruginosa}, occurrences of a single value add up to more than 50\% of the vector's length. It is not surprising, therefore, that many vectors have similar profiles and can be encoded relative to each other in an efficient way.

  % XXX -- cite occurrences of each quality value (Illumina \& Solexa).

  % Second observation. Third observation.

  To efficiently identify groups of quality vectors that share a significant amount of information, we devised a streaming clustering algorithm that separates the incoming vectors into distinct classes that are then compressed separately. For every incoming quality vector, we first compute its weighted mode $q^{*}$ and the mode's frequency $f_{q^{*}}$. To compute the mode, we treat the quality vector $v_i$ as a collection of numerical values and define weighted mode as $\argmax_q n(q) w_q$ where $n(q)$ is the number of $q$'s occurrences in vector $v_i$ and weight $w_q$ is the ordinal value for $q$ transposed so that all quality values fall within the $[0;q_{max}]$ range. The weighted mode resolves ambiguities in favor of ``better'' quality values when several values occur with near equal frequency. If the mode is not frequent enough ($\le 25\%$ of the vector's values), the variation in the vector makes it less likely to compress well and we assign $v_i$ to the general pile.

  For a vector that has a sufficiently frequent mode, we proceed to trim its prefix and the suffix to account for flanking low quality values. We retain both substrings to be able to fully reconstruct the vector, but only use the remaining core string when assigning $v_i$ to a cluster. The characters in $v_i$ are considered low quality until we observe the mode $q^*$ twice in a row. When such low quality prefixes and suffixes are separated from the rest of the quality vector and stored separately, each collection --- the prefixes, the core substrings, and the suffixes -- compresses better.

  We then compare $v_i$'s' core substring to centroids of the existing clusters. We assign $v_i$ to the first cluster that falls within a given radius, or create a new cluster $C$ if there are no sufficiently close clusters and set the vector $v_i$ to be $C$'s centroid. We use $D2$ distance~\cite{D2Dist} to compute the similarity between the core of vector $v_i$ and cluster's centroid. Using $D2$ metric to compare two quality vectors allows \refer to capture subsequence similarity without enforcing a specific order between matching parts of the vector.

  To avoid forming too many clusters with a long tailed distribution of sizes, we interrupt the algorithm after scanning through the first $200000$ quality vectors (where the size of this set is an input parameter and can be adjusted for different size datasets) and only retain the clusters that contain at least $5\%$ of the vectors seen so far. Clusters that were formed during this bootstrap stage are used to separate all of the following vectors. Finally, we record the order in which quality vectors arrive into a separate data stream ensuring that we can match quality vectors to their corresponding alignments during decompression.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

  We introduce \refer that makes global sequence analyses possible without ever fully reconstructing the alignments and further improves SAM compression rates by leveraging separability of data within alignments. \refer performs common tasks at speeds orders of magnitude faster than those for similar tasks within \texttt{samtools} package~\cite{SamTools} and compresses data in times comparable to existing alternatives. \refer operates by grouping related data and converting it into its more compact representation, then passing it down to a general-purpose dictionary coder like \texttt{plzip} for further compression. \refer can compress sequence data down to 0.06 bits per base (a 66x improvement over 2-bit encoding for the same data), its total compressed sizes can be as small as $1/10$ of the original SAM file or up to $50\%$ smaller than an equivalent BAM file. On average, \refer achieves an 8.1\% improvement over the next best approach~\cite{Sahinalp2015}. \refer reduces the size occupied by quality values by identifying highly similar quality vectors in a streaming fashion and encoding them separately from the rest of the qualities. \refer can compress a 13.5Gb SAM file to 1.5Gb in about 10 minutes and while using less than 2Gb of memory. It is highly parallelizable and can be made significantly faster by allowing it to use more threads and memory. When operating on sequencing data exclusively, \refer can process the same 13.5Gb file in under three minutes requiring $<$100Mb of disk space to losslessly represent aligned sequences, a two- to fourfold improvement over competing approaches. Global statistics like depth of coverage can be computed using these compressed sequence data alone and take 44 seconds for a 72Gb SAM file as opposed to 48 minutes when performed by \texttt{samtools}.


  % test data for referee

  We test \refer on a human and bacterial RNA-seq and human whole genome sequencing (WGS) data with varying number of alignments, depth of coverage, error rates, and read lengths (Table~\ref{tab:datasets}). We generated alignments for \texttt{SRR445718} and \texttt{SRR1294122} using STAR~\cite{DobinSTAR}, a fast multi-threaded mapper, with different error rate cutoffs (see later sections).
  \texttt{K562\_cytosol\_LID8465\_TopHat\_v2}, \texttt{NA12878\_S1} datasets are available from EMBL-EBI's ArrayExpress data repository, accession numbers \texttt{ERX283488} and \texttt{ERX237515} correspondingly;
  \textit{P. aeruginosa} was available for download alongside with DeeZ~\cite{Sahinalp2015}. All alignment files were sorted and converted into SAM format using \texttt{samtools}~\cite{SamTools}. Experiments were performed on a Dell PowerEdge T620 machine with 256Mb of RAM.

  \begin{table}[ht!]
    \caption{\textbf{Test set of SAM files.} Experiments were performed on human and bacterial RNA-seq as well as human whole genome sequencing data (WGS) for reads of different lengths. All datasets, except K562, had a significant number of unaligned reads, varying in depth of coverage and error rates. Datasets mostly comprised of Illumina (I) sequences with one exception (Solexa, S).}
    \label{tab:datasets}
    \centering
    \begin{tabular}{l l r r r}
    \toprule
    File 			& Type 		& |r| & Alignments & Unalign. \\
    \midrule
    % SRR445718 		& RNA-seq 	& 100 & 31892550 & 5677563 \\
    SRR445718, I		& RNA-seq & 100 & 35677496 & 5677563 \\
    SRR1294122, I 		& RNA-seq	& 101 & 47933574 & 1859223 \\
    P. aeruginosa, S 	& RNA-seq	& 51  & 83412201 & 5560731 \\
    K562, I 			& RNA-seq	& 76  & 246476391 & 0 \\
    NA12878\_S1, I 	& WGS 	& 101 & 1015694132 & 202874775 \\
    \bottomrule
    \end{tabular}
  \end{table}

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{De novo compression performance}

  XXX --- need to write this whole thing

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Reference-based compression performance}


  % XXX talking about sequence -- possibly move to sequence section
  % \refer performance on RNA-seq may be partially explained by its improved sequence compression: \refer compresses RNA-seq sequence data up to 3.5 times better than Deez and up to 8 times better than Quip (details in Table~\ref{tab:seq-compression}).


  %
  % results in the table generated based on run-seq-improvements.sh script
  % use python/parse_seq_logs.py to generate the table
  \begin{table*}[ht!]
    \caption{\textbf{\refer performance.}\refer can compress sequence very efficiently down to 0.03 bits per base (bpb). Sequence sizes are shown in Megabytes. Error rate was calculated as an average number of edits (clipping, indels, splices, and mismatches) over all aligned bases. Sequencing depth was calculated as the average number of aligned reads over the covered bases.}
    \label{tab:seq-compression}
    \centering
    \begin{tabular}{l r r r r c r}
    \toprule
    % May 4
    % File & Total bases & \refer (bpb) & Deez (bpb) & Quip (bpb) & Error rate & Depth \\
    % \midrule
    % SRR445718 &  3603436792 & \textbf{86.94 (0.22)} & 298.4 (0.69) & 145.34 (0.34) & 2.72 & 32.67 \\
    % SRR1294122 & 4889234340 & \textbf{85.22 (0.16)} & 268.47 (0.46) & 150.54 (0.26) & 1.96 & 21.51 \\
    % P. aeruginosa & 4337434556 & \textbf{41.66 (0.08)} & 59.2 (0.11) & 359.75 (0.71) & 0.53 & 687.19 \\
    % K562 &  18978688729 & \textbf{57.98 (0.03)} & 258.78 (0.11) & 417.98 (0.19) & 0.92 & 73.53 \\
    % NA12878\_S1 & 99817837824 & \textbf{2730.54 (0.23)} & 4229.62 (0.35) & 5845.28 (0.48) & 1.43 & 36.11 \\
    % May 7th
    File & Total bases &  Referee & Quip & Deez  & Error rate & Depth \\
    \midrule
    SRR445718 & 3567749600 & 86.63 (0.20) & 145.34 (0.34) & 298.40 (0.70) & 2.85 & 32.67 \\
    SRR1294122 & 4841290974 & 84.33 (0.15) & 150.54 (0.26) & 268.47 (0.47) & 1.95 & 21.51 \\
    P. aeruginosa & 4254022251 & 41.61 (0.08) & 359.75 (0.71) & 59.20 (0.12) & 1.04 & 687.19 \\
    K562 & 18732205716 & 135.99 (0.06) & 417.98 (0.19) & 258.78 (0.12) & 1.20 & 73.53 \\
    NA12878\_S1 & 101569413200 & 2940.07 (0.24) & 5845.28 (0.48) & 4229.62 (0.35) & 1.50 & 36.11 \\
    \bottomrule
    \end{tabular}
  \end{table*}
  %

  % figure w/ depth and error rates over different files
  \begin{figure}[ht!]
    \centering
    % \includegraphics[width=\linewidth]{seq_vs_depth_error}
    \includegraphics[width=0.6\linewidth]{referee_fig1_complete}
    \caption{\textbf{\refer outperformed both Quip and Deez in sequence compression}: \refer handles increasing error rates and sequencing depths better than its competitors. Deez's rates deteriorate with increasing error rate and Quip's rates tend to be significantly affected by the sequencing depth. Each line in the plots represents a single file; files correspond to those in Table~\ref{tab:seq-compression}.}
    \label{fig:referee:depth-error}
  \end{figure}

  \refer achieves significantly better compression rates for sequencing data within alignments over other competing methods (see Table~\ref{tab:seq-compression}), especially on datasets at high depth of coverage and low error rates. To compare \refer performance to other tools on only the aligned sequence, we replaced read identifiers, quality values, and non-essential fields with default values. We further modified Deez to produce additional debugging information on the size of each stream and ran Quip in a verbose mode. Since both tools could retain some information from other SAM file columns or build indices to allow for fast data access, we only report the sizes for compressed streams that were directly relatable to sequence and not the total size of the file on disk (\texttt{seq} for Quip and the sum of \texttt{seq} and \texttt{edits} for Deez, see Table~\ref{tab:seq-compression}). For \refer, we report a sum of stream sizes for alignment offsets, clipped prefixes and suffixes, and edits. \refer surpasses both Deez and Quip for every dataset leaving a considerable margin.

  % To demonstrate the efficiency of our sequence encoding scheme, we performed an experiment where we compared SAM and \refer ways of encoding sequence (see Table~\ref{tab:seq_comp}). For two RNA-seq runs, we generated alignments with optional MD string and extracted reference names, offsets, cigar and MD strings for each file --- the four columns that completely represent the read sequence (ignoring the clipped regions). Compressing these four columns resulted in improvement of 83\% and 75\% correspondingly over the 2-bit encoding of the read sequences with \refer's format reducing the compressed size further almost twofold.


  The \refer compression rates of bits spent to encode a single base vary from 0.06 to 0.24 --- up to 66 times better than a straightforward 2-bit sequence encoding and up to 8 times better than the two competing methods. In section~\ref{sec:seq-comp-methods}, we discussed how sequence compression quality can be related to read length and should be more effective on longer reads, however, our results show that other factors, like error rate and depth of coverage, can significantly affect the predicted performance (Table~\ref{tab:seq-compression}). We observe that \refer and Deez scale up well with increasing depth of coverage (Figure 1(a)) while Quip's rates drastically go up for the deep sequencing dataset \textit{P. aeruginosa}. \refer achieves best performance for the two datasets with the highest coverage, \texttt{K562} and \textit{P. aeruginosa}, which can be explained by the fact that these also happen to be the two datasets with the lowest error rates. Indeed, compression rates for \refer's and Quip start to decrease slowly as error rates increase --- these two tools are more robust to changes in error rates than Deez for which compression rates quickly deteriorates at higher error rates ((Figure~\ref{fig:referee:depth-error}b).
  %
  % When error rate is at its lowest, bit/base ratio is about half that of the ratio at the highest error rate for the RNA-seq data (Figure~\ref{fig:seq-errors}). For example, the two datasets with the best bit per base ratio are also the ones with the shortest read length (\textit{P. aeruginosa} and K562 RNA-seq data), however, \refer is able to capitalize on their sequencing depth and low error rate.


  % compression ratios (bpb) when varying the error rate
  % figure generated by script seq_rates_over_error.py, takes in analysis/seq_bit_per_base_over_error.txt
  % That can be regenrated by referee_error_rates.py and parse_compr_rates_error_rate.py;
  % \begin{figure}[ht]
  % 	\centering
  % 	\includegraphics[width=\linewidth]{seq_rates_over_error}
  % 	\caption{\refer consistently outperforms its competitors in sequence compression, even as error rate increases. Blue: experiments for SRR445718 alignments, green: experiments for SRR1294122 alignments with various allowed error rates. Circles represent \refer, stars --- Deez, and pentagons --- Quip.}
  % 	\label{fig:seq-errors}
  % \end{figure}


  \refer also compresses sequence more efficiently than CRAM~\cite{CRAM}, the first work to consider reference-based sequence compression, for the same depths, error rates, and read lengths. CRAM estimates the bpb ratio to be between 0.175 and 0.20 for unpaired 100bp reads at 1\% error rate and sequencing depth of 25. However, \refer achieves a ratio of 0.14bpb at a higher error rate and similar coverage. As error rates decrease, \refer compression rates improve and are comparable to the rates for 200 and 400 base long reads reported by CRAM (see Figure 2 in~\cite{CRAM}).

  % XXX \refer offers sequence compression rates that are comprable to those reported by CRAM Tools~\cite{CRAMTools} at higher error rates. XXX -- need to run CRAM?



  % In general, we can always expect an improvement in total compression rate over aligned and unaligned sequences to follow a trend $r_a \alpha + r_u (1-\alpha)$ where $\alpha$ is the percent of the reads that successfully aligned and $r_a, r_u$ are compression rates for the aligned and unaligned sequence correspondgly. Whatever the $\alpha$, the savings $(r_u - r_a)\alpha$ may result in large savings in disk space and minutes or hours of the download times.

  % XXX --seq option -- do not encode anything else, just offs, edits, clips, separate timing

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Compression performance on alignment data}

  \refer outperformed its competitors on all but one test cases, saving hundreds of megabytes in each case. Compared to the original plain text SAM format, \refer offers seven- to tenfold improvement in storage; or up to twofold improvement over its binary variant BAM.
  Our results show that \refer can effectively compress alignments from a wide range of depths of coverage: \refer achieves equally good performance on bacterial \textit{P. aeruginosa} sequenced at an average depth of 687 and human RNA-seq \texttt{SRR445718} dataset with average depth of coverage of 32.
  % XXX discuss error rate performance

  \refer performed significantly better than Deez on human RNA-seq data where alignments contained a significant number of splicing events and achieved a modest improvement for the bacterial RNA-seq alignments. Quip's performance paralleled that of \refer on two of the smaller human RNA-seq datasets, but Quip spent an additional gigabyte encoding alignments for the large \texttt{K562} RNA-seq dataset. \refer had an additional 272Mb over Quip for the human whole genome sequencing dataset, however, it required 600Mb less than Deez to encode the same information.

  Quality values contributed significantly to the size of the compressed data adding up to us much as 80\% of the total compressed size (Figure~\ref{fig:fields}). At the same time, sequencing data took no more than 5.6\% across all datasets with read identifiers and unaligned reads contributing most significantly to the overall size after qualities.

  % (XXX -- update results after applying cigar to the qual cores and with Mincing for unaligned reads)

  % XXX We do not compare to CRAM Tools and Scramble since they result in loss of inofrmation while \refer uses a fully lossless approach.

  % forcing to appear earlier!
  % run compute_sizes.sh on ocean:/mnt/scratch1/dfilippo/aligned to get the sizes
  \begin{table*}[ht!]
    \caption{\textbf{Comparison between BAM/SAM compression methods and \refer on human RNA-seq, bacterial and human WGS datasets.} The second number for \refer indicates \% improvement over the next best approach. The sizes are reported in megabytes.}
    \label{tab:compression}
    \centering
    \begin{tabular}{l r r r r r}
    \toprule % Deez Refere Bam Quip
    File    		& Referee (\%) & Quip 			& Deez 		& BAM  & SAM \\
    \midrule
    SRR445718       & \textbf{1242.67} (8.30\%) & 1355.5 & 1607.11 & 2282.45 & 9544.98 \\
    SRR1294122      & \textbf{1545.15} (10.7\%) & 1730.15 & 2022.63 & 3107.67 & 13650.17 \\
    P. aeruginosa   & \textbf{1866.87} (2.80\%) & 2180.95 & 1921.06 & 3339.54 & 19008.37 \\
    K562    & \textbf{7147.04} (10.6\%) & 8274.43 & 7997.7 & 13119.44 & 72398.19 \\
    NA12878\_S1     & 62176.53 (-0.4\%) & \textbf{61904.65}  & 62774.92 & 108289.6 & 437589.43 \\
    \bottomrule
    \end{tabular}
  \end{table*}


  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Quality values compression performance}

  % \# of clusters, cluster sizes, compression rates per cluster vs the general pile.
  % + How many items were in some cluster vs. generic pile?
  % - How well did clusters compress?
  % - Specificl clusters: best quality values, worst quality values.
  % - Separating prefixes \& suffixes -- clustering improves; compression improves.

  Although high variability of quality values makes them notoriously hard to compress, \refer's clustering scheme results in 13 to 17\% reduction in compressed size for quality values over \texttt{plzip} without a significant increase in runtime or memory used. Across the tested datasets, \refer performs similarly or better than arithmetic coding (AC) schemes employed by Deez and Quip (see Table~\ref{tab:qual-compression}) while making partial decompression feasible.
  For quality vectors that get placed into clusters, the average compression rate lies between 6.2 and 10.9. Combined with a considerably lower compression ratio for the quality vectors placed in the generic pile, the overall compression rate for qualities varies between 3.33 and 4.22 on the tested datasets.

  The number of quality clusters did not exceed four (including the generic pile) with the number of vectors assigned to non-generic clusters ranging from 31 to 67\% over all datasets. For most datasets, two clusters formed consistently: a cluster of vectors consisting of mostly the highest quality value and a smaller cluster of vectors consisting almost entirely of the lowest quality value. For vectors in clusters, collections of core substrings compressed down to $1/3 - 1/4$ of the original size. In general, quality vectors compressed better when separated into prefixes, cores, and suffixes as opposed to vectors that were not split, e.g. clusters for \texttt{SRR1294122} before separation would compress at 5.6, 5.7, and 4.5 ratios, but with prefixes and suffixes compressed separately the compression rate over the three clusters increases to 6.3, 6.6, and 4.8 correspondingly.

  % to generate table:
  \begin{table}[ht!]
    \caption{\textbf{\refer streaming clustering approach improves quality values compression} across most of the tested datasets (sizes reported in Megabytes).}
    \label{tab:qual-compression}
    \centering
    \begin{tabular}{l r r r}
    \toprule
    File & \refer & Deez & Quip \\
    \midrule
    SRR445718 	& \textbf{976.76} & 1055.70 & 984.54 \\
    SRR1294122 	& \textbf{1092.92} & 1328.17 & 1226.99 \\
    P. aeruginosa & 1160.75 & 1084.87 & \textbf{994.1} \\
    K562 		& \textbf{5358.94} & 5830.98 & 5715.71 \\
    NA12878\_1 	& \textbf{32560.26} & 38413.76 & 38097.35 \\
    \bottomrule
    \end{tabular}
  \end{table}

  % \refer's streaming approach improves on the arithmetic coding quality compression rates reported by Deez and Quip (see Table~\ref{tab:qual-compression}), however, block-wise encoding makes random access feasible.  To obtain compressed size for quality values achieved by Deez and Quip, we used the auxilary output produced by Deez and ``-v'' option for Quip. Deez was run with default options (lossless compression).



  % than collections of mixed quality vectors with compressed clusters occupying up to 37 times less space than the uncompressed quality vectors.

  % XXX Separating the prefixes and suffixes from the core substring results in further compression since prefixes and suffixes within a cluster tend to largely repeat from vector to vector.

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Operations on separate streams and random access}

  % XXX discuss stream separability; partial decompression/random access. operations can be done on offsets alone; offests and edits. Expect sequence to be of most interest.

  Separability of data streams in \refer allows for field-specific analyses that are order of magnitudes faster than comparable tasks done with, e.g. \texttt{samtools}~\cite{SamTools}. A common task such as computing a dataset's sequencing depth involves going through all alignments recording number of reads aligned to any covered base. \texttt{samtools depth} takes about 19 minutes to compute depth for a modestly sized \texttt{SRR445718} dataset and over 48 minutes for \texttt{K562} while \refer accomplishes that same task in 8 and 44 seconds correspondingly. The difference is explained by the fact that \texttt{samtools} has to scan through all of alignment data, including data unrelated to depth calculation, while \refer accomplishes the same task by  having to go through compressed offsets (about 5\% of the compressed data).

  Likewise, \refer format allows to quickly reconstruct aligned reads originally embedded in SAM file (an equivalent of \texttt{samtools faidx} command) without ever reconstructing the alignments fully. Users can specify a genomic region of interest and \refer can reconstruct alignments from that genomic region without decompressing the whole dataset. The index that \refer constructs to enable partial decompression is lightweights: for example, to index all data blocks for \texttt{NA12878}, \refer uses 1.2Mb which can be further compressed down to 233Kb -- a size negligible when compared to the size of the data itself.

  % XXX index for NA128 -- 1.2 Mb



  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \subsection{Speed and memory demands}

  \refer is fast and only requires on the order of 1.5Gb of RAM to run making it usable on even smaller machines (see Table~\ref{tab:runtimes}). \refer streams through the alignments in a sequential fashion only keeping intermediate information in memory before it is passed on to \texttt{lzlib}. \refer launches several threads that each invoke \texttt{lzlib} operations on a single block of data at a time making use of modern multithreading architectures. \refer will run faster if given more processing power, however, \refer can reach a saturation point where data blocks prepared by \refer are compressed at a faster rate than the new blocks are being produced.

  % XXX does it scale up linearly w/ \# CPUs, memory?

  % XXX fill out the table -- timing information, memory (Figure~\ref{tab:runtimes})

  % generated by script python/parse_timings.py
  % relies on the usr/bin/time -v and logs for each file, tool
  \begin{table}[ht]
    \caption{\textbf{\refer is comparable or faster to the other competing tools.} We report wall running times and \% of CPU used by each tool. Both Deez and \refer were run with 10 threads, Quip was run in its default mode since it does not offer a multithreaded functionality.}
    \label{tab:runtimes}
    \centering
    % \begin{tabular}{l r r r r}
    % \toprule
    % File & \refer & Deez & Quip -r & sam2bam \\
    % \midrule
    % SRR445718 		& 9m 	&	16.3m 	& - & -\\
    % SRR1294122 		& 7m 	& 	7.7m	& 5m32s & - \\
    % P. aeruginosa 	& 11m13s&	7.1m 	& 7m07s & - \\
    % K562 			& -		& 	- 		& 27m32s & - \\
    % NA12878\_S1 	& 5h18m & 5h49m 	& - & - \\
    \begin{tabular}{l r r r r }
    \toprule
    File & Referee & Deez & Quip -r \\
    \midrule
    SRR445718 		& 7:05 (1017\%) & 10:34 (190\%) & 6:59 (139\%) \\
    SRR1294122 		& 10:26 (1016\%) & 11:07 (204\%) & 7:48 (149\%) \\
    P. aeruginosa 	& 18:41 (608\%) & 7:34 (163\%) & 9:18 (153\%) \\
    K562 			& 54:09 (620\%) & 50:07 (180\%) & 36:06 (157\%) \\
    NA12878\_S1 	& 5:18:12 (828\%) & 3:00:08 (190\%) & 3:13:31 (164\%) \\
    \bottomrule
    \end{tabular}
  \end{table}

  \refer could be run in a lossy manner in which only the essential information about sequence and alignment is recorded and information like read identifiers, flags, quality values, and optional SAM fields are not stored. Users can enable this mode by providing an optional ``\texttt{--seqOnly}'' parameter: \refer would encode offsets and edits for every alignment it incurs along with the unaligned reads. Alternatively, users can further restrict \refer to only encode primary alignments by using a ``\texttt{--discardSecondary}'' option. Since \refer has to look over less data, sequence only mode is much faster than the lossless operation: for example, the sequence in either of the three smallest RNA-seq datasets can be compressed in under 3 minutes.

  % XXX a paragraph that discusses speed to decompress, including random access

  Given the fact that modern mapping tools can generate alignments in the matter of minutes even for large files, the ``\texttt{--seqOnly}'' options make \refer a reasonable choice when the goal is to compress sequencing data. Indeed, an  aligner like STAR~\cite{DobinSTAR} can process 11Gb human RNA-seq dataset (e.g.\@ \texttt{SRR1294122}) in under five minutes generating alignments for $94.31\%$ of all reads. \refer takes under three minutes to compress the resulting SAM file with ``\texttt{--seqOnly --discardSecondary}'' options encoding the aligned reads in 2.1\% of their original size and the unaligned reads in 25\% of their original size. Overall, this combined scheme encodes all of the 39.7 million of original reads in 3.3\% of their original size requiring only eight minutes to do so. Considering \refer's consistently good performance at different error rates (Figure 1), we conclude that for the purposes of sequence compression, it is reasonable to trade off the quality of alignments for the number of aligned reads.




%\section{Results}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion and Conclusions}

We present a novel approach for efficient compression and storage of the  alignment information that enables certain computational analyses without ever having to decompress the data fully. \refer operates by separating related streams of data and encoding them individually achieving better compression rates for each stream. \refer is competitive in speed and performance to other state of the art tools that employ more complex compression schemes~\cite{Sahinalp2015,Jones2012}. We show that \refer can compress alignment data down to $1/10^{th}$ of its original SAM file size. When focusing on alignments that retain only sequence data (i.e. stripped of extraneous information like read identifiers), \refer can compress it in a lossless manner 2 to 9 times better than the competing methods (see Table~\ref{tab:seq-compression}). \refer's efficient encoding of mapping locations and sequence variation makes it robust to increasing error rates (Figure~\ref{fig:referee:depth-error}). Coupled with the fact that modern alignment tools can successfully map millions of sequencing reads in minutes, \refer becomes an appealing choice in sequence compression, especially for the rapidly growing body of RNA-seq and WGS datasets for the model organisms for which well-annotated reference genomes are available.


Many analyses relying on the alignment information do not require original read identifiers or unaligned reads focusing instead on the sequence that aligned successfully. In such cases, being able to download only the data one needs may significantly reduce the download size. \refer separates data streams into distinct compressed files and only requires a subset of them to reconstruct the original SAM/BAM data. Read identifies, quality values, optional SAM fields and all of the unaligned reads can be skipped; that means a reduction of download size by 84-93\% (see Figure~\ref{fig:fields}).

% Modular design: requires only essential fields: offsets, mapping flags, edits+clipped regions. Can skip: read ids, qualities, opt. fields, unaligned reads.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.6\linewidth]{Referee-size-breakdown-May4}
  \caption{\textbf{Compressing streams separately allows to download them independently.} Quality values and read identifiers contribute significantly to the size of the compressed file, however, \refer does not rely on these data to reconstruct sequence alignments. Omitting the non-essential alignment data can reduce the size of the download by up to 93\%.}
  \label{fig:fields}
\end{figure}

% XXX: merging sequence?


\refer is written in C++ and is available on GitHub (\url{https://github.com/Kingsford-Group/referee}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Overall conclusions
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Discussion and Conclusions}

XXX

% \appendix
% \include{appendix}

\backmatter

%\renewcommand{\baselinestretch}{1.0}\normalsize

% By default \bibsection is \chapter*, but we really want this to show
% up in the table of contents and pdf bookmarks.
\renewcommand{\bibsection}{\chapter{\bibname}}
%\newcommand{\bibpreamble}{This text goes between the ``Bibliography''
%  header and the actual list of references}
\bibliographystyle{plainnat}
\bibliography{mypapers,references,jazzmap,graph_drawing,dynnetvis,coral_bmc,coredomain,compression}

\end{document}
